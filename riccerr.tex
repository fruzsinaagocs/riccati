\documentclass[10pt]{article}
\oddsidemargin = 0.2in
\topmargin = -0.5in
\textwidth 6in
\textheight 8.5in

\usepackage{graphicx,bm,hyperref,amssymb,amsmath,amsthm}

\usepackage{showlabels}

% -------------------------------------- macros --------------------------
% general ...
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\bmp}[1]{\begin{minipage}{#1}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\pig}[2]{\bmp{#1}\includegraphics[width=#1]{#2}\emp} % mp-fig, nogap
\newcommand{\bp}{\begin{proof}}
\newcommand{\ep}{\end{proof}}
\newcommand{\ie}{{\it i.e.\ }}
\newcommand{\eg}{{\it e.g.\ }}
\newcommand{\etal}{{\it et al.\ }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdc}[3]{\left. \frac{\partial #1}{\partial #2}\right|_{#3}}
\newcommand{\infint}{\int_{-\infty}^{\infty} \!\!}      % infinite integral
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}^2}
\newcommand{\ve}[4]{\left[\begin{array}{r}#1\\#2\\#3\\#4\end{array}\right]}  % 4-col-vec
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\bigO}{{\mathcal O}}
\newcommand{\qqquad}{\qquad\qquad}
\newcommand{\qqqquad}{\qqquad\qqquad}
\newcommand{\eps}{\epsilon}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\vol}{vol}
\newtheorem{thm}{Theorem}
\newtheorem{cnj}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}
% this work...
\newcommand{\om}{\omega}
\newcommand{\te}{\tilde\eta}



\begin{document}

\title{Analysis of an iteration for non-oscillatory solutions to the Riccati form of the phase function for 2nd-order linear ODEs}

\author{Fruszina Agocs and Alex H. Barnett}
\date{\today}
\maketitle

\begin{abstract}
We gather basic definitions and error analysis.
\end{abstract}

\section{Introduction}

We are interested in efficient numerical solution of
the oscillatory linear 2nd-order ODE
\be
u''(t) +\gamma(t) u'(t) + \om^2(t)u(t) \;=\; 0
~,
\qquad t \in (0,1)~,
\label{ode}
\ee
with initial conditions
\bea
u(0) &=& u_0 \\
\label{ic0}
u'(0) &=& u_1~.
\label{ic1}
\eea
%Here the local frequency function $\om(t)$ is purely real or purely imaginary.
In regions where $\om \gg 1$
then the solution $u$ is oscillatory, meaning that on some local
scale it is approximately $a e^{i\om_0t} + b e^{-i\om_0t}$ where $\om_0$ is the local value of $\om(t)$.
Conventional ODE integrators then require discretization with several
time-points per local period $2\pi/\om$, which can be prohibitively slow.
This has led to the development of more efficient solvers for this
high-frequency case.
For now we consider only regions where $\om$ is real and positive;
one may also consider $q(t):=\om^2(t)$ real and allow evanescent
$q<0$ regions, where the solution $u$ has rapid growth and decay.
For now we ignore the real-valued damping function $\gamma(t)$,
setting $\gamma \equiv =0$. *** to include.

Background. ***
WKB blah.
QML.
oscode.
Recently Bremer following Bremer--Rokhlin has proposed an efficient
method
which requires $\bigO(1)$ effort per $t$ evaluation for such an
oscillatory solution, by solving for a nonoscillatory
phase function for Kummer's equation.
The existence of such a nonoscillatory phase function
(meaning that its Fourier transform is exponentially small in $\om$ ***
look up) is proven by Bremer--Rokhlin.
Despite this, finding initial conditions on the phase function leading to
a nonoscillatory solution is challenging.
Those authors proposed a
``windowing'' method to numerically find such a global phase function
on an interval:
the idea is to use a $C^\infty$ partition of unity near the upper end of the interval ($t=1$) to smoothly ``blend'' $q$ to
a constant function, then integrate backwards from the constant region
down to the lower end ($t=0$)
finally integrating $t\in[0,1]$ forward using the original $q$.
This exploits the facts that i) a nonoscillatory phase function for
the constant case $q(t) = \om_0^2$ is simple to write down, and
ii) the smooth window allows
the nonlinear Kummer oscillator to make a smooth (in physics
terminology ``adiabatic'') transition which excites
only exponentially small oscillation amplitude.
ETC.

Here we present an alternative approach using
an asymptotic expansion for a nonoscillatory
Riccati phase function, meaning simply that the
phase function is the logarithmic derivative $x(t) = (\log u(t))'$.
Our expansion is simpler than the traditional WKB expansion,
although of similar accuracy.
One novelty is that we
implement the expansion iteratively
using high-order numerical differentiation, rather
than algebraically (as WKB is usually applied).
The iteration does not introduce oscillatory contributions,
hence finds a nonoscillatory approximate phase function without
reference to initial conditions.
Another crucial difference from Bremer is that $x$ is solved for
locally on an interval with adaptively-chosen length, rather than
globally.
Solutions to \eqref{ode}--\eqref{ic1} are then constructed by
gluing intervals using Cauchy data.
Another advantage is we include a damping term $\gamma(t)$,
allowing direct
solution of a variety of 2nd-order linear ODEs.

Review recent WKB-based approaches using WKB as basis on intervals ***.
*** solution orders in $h$ and $\om$.

Like Bremer, our solver is arbitrarily high-order.
One advantage over Bremer is that our method generates an explicit asymptotic
expansion. A disadvantage is that its error is limited by the number
of iterations, and by $\omega$ itself; however,
we will show that close to machine accuracy is easily reached in
intervals that contain many oscillations.

*** better list of adv vs prior work.
(Arb high-order compared to oscode $\bigO(h)$ and the $h^2$ guys.
Local rather than QML or Bremer global.)



\section{The method}

\subsection{Phase function solution on a single interval}

We will use prime to denote $d/dt$.
We consider an interval $t\in[a,b]$ lying in $[0,1]$.
Writing $u = e^z$ where $z'(t) = x(t)$ is the phase function,
then any nonvanishing $u(t)$ satisfies \eqref{ode} if and only if
its complex phase function $x(t)$ satisfies the nonlinear Riccati equation
\be
x' + x^2 + \om(t)^2 \;=\; 0~.
\label{ricc}
\ee
Almost all solutions to \eqref{ricc} are oscillatory,
as the presence of $\om(t)$ would suggest.
This is illustrated by the case of constant $\om(t) = \om_0^2$
which has the family of analytic solutions
$x(t) = \om_0 \tan(\alpha - \om_0t)$ parameterized by $\alpha\in\C$,
where $\re \alpha$ is interpreted as a phase shift and $\im \alpha$
controls the amplitude. Their oscillation frequency is $2\om_0$.
However, the limit $\im \alpha \to \pm \infty$ produces
the only two nonoscillatory solutions $x(t) = \pm i\om_0$.
The picture is similar
for general analytic functions $\om(t)$:
it has recently been proved that there exist nonoscillatory phase functions
(in a precise sense for $\om\gg 1$ involving exponential decay of the Fourier
transform) in that case \cite{Heit15,Brem16}.
In fact this result was proven
for the related real-valued Kummer's equation
\be
\frac{3 \alpha'^2}{4\alpha^2} - \frac{\alpha''}{\alpha} - \alpha^2 +
\om(t)^2 \; = \; 0~,
\label{kummer}
\ee
corresponding to ODE solutions $u(t) = \alpha(t)^{-1/2} e^{\pm i\int \alpha(t) dt}$,
but the existence of a nonoscillatory Kummer's solution implies the same for
a Riccati solution \footnote{We thank Jim Bremer for explaining this argument.}.
The latter is easy to show, since if $\alpha(t)$ satisfies \eqref{kummer},
then defining $\beta = -\alpha'/2\alpha$ allows \eqref{kummer}
to be written
$\beta' - \alpha^2 + \beta^2 + \om(t)^2 = 0$, and these last two equations
are the real and imaginary parts of \eqref{ricc}
for $x = i\alpha + \beta$.

We now turn to our numerical method to find nonoscillatory
solutions to \eqref{ricc}.
When $\om\gg 1$, there are two nonoscillatory solutions
which take the approximate form
$x_{\pm}(t) \approx \pm i\om(t)$, leading to a conjugate pair of basis
counter-rotating functions $u_{\pm}(t) := e^{\int x_\pm(t) dt}$.
Given any trial solution $x$ to \eqref{ricc}, its \textit{residual
  function} is defined as the left-hand side of \eqref{ricc}, namely
\be
R[x](t) := x' + x^2 + \om(t)^2~.
\label{R}
\ee
Our proposal takes the following \textit{functional iteration}
to generate a sequence of functions $x_0, x_1, \dots, x_k$
on $t\in(a,b)$,
using the above positive (say) approximation as a starting point:
\bea
x_0(t) &=& +i\om(t)
\label{init}
\\
x_{j+1}(t) &=& x_j(t) - \frac{R[x_j](t)}{2 x_j(t)}
~,\qquad j=0,1,\dots,k-1~.
\label{iter}
\eea
%with the functional $R$ defined as in \eqref{R}.
This may be justified heuristically as an approximate Newton
iteration to reduce the residual function
(noting that similar exact Newton iterations have been
exploited for analysis \cite{Heit15}).
Namely, since
$$
R[x_j + \delta] = x_j' + \delta' + x_j^2 + 2 x_j \delta + \delta^2 + \om(t)^2\\
= R[x_j] + \delta' + 2x_j\delta + \bigO(\delta^2)~,
$$
by linearizing we get that $\delta$ solves the linear 1st-order ODE
$\delta' + 2x_j(t) \delta  = -R[x_j](t)$,
which has an analytic solution.
Solving this exact Newton update symbolically is the basis of the QML iteration
(*** CITE + check).
However, this ODE is again generally oscillatory,
with unknown initial condition needed for a nonoscillatory solution $\delta$.
Yet if $\delta$ is nonoscillatory the first term $\delta'$ is
a factor $x_j = \bigO(\om)$ smaller than the second term, thus
one might hope that by dropping it a useful reduction in residual might
still result.
This leads to the algebraic formula $2x_j(t) \delta(t) = -R[x_j](t)$
which is solved pointwise for each $t$, and
writing $x_{j+1} = x_j + \delta$ gives \eqref{iter}.
This is a type of defect correction scheme (CITE book).
The result is nonoscillatory by construction, without explicit reference
to initial conditions.

The early iterates of \eqref{init}--\eqref{iter} illustrate
algebraically
the type of residual reduction that occurs:
\bea
x_0(t) &=&  i\om(t)~, \hspace{3.0in} R[x_0](t) \; = \; i\om'(t) = \bigO(\om)
 \nonumber
\\
x_1(t) &=& i\om(t) - \frac{\om'(t)}{2\om(t)}~,
\hspace{2.5in} R[x_1](t) \; = \; -\frac{\om''}{2\om} + \frac{3\om'^2}{4\om^2}
= \bigO(1)
 \nonumber
\\
x_2(t) &=& i\om(t) - \frac{\om'(t)}{2\om(t)} + \delta_1(t),
\mbox{ where }
\delta_1(t):=\frac{\om''/2\om - 3\om'^2/4\om^2}{2i\om - \om'/\om}
,
\quad R[x_2](t) \; = \; \delta_1' + \delta_1^2 = \bigO(\om^{-1})
\nonumber
\eea
To measure the size of term we assume that $\om'$, $\om''$, etc, are
of the same order as $\om$, because $\om(t)$ is smooth,
and also assume that $\om$ has a lower bound of the same order.
Thus we see that each iteration the residual drops by a factor $\bigO(\om)$.
However, each iteration also results in terms with
one higher time-derivative of $\om$.
Despite the rapid growth in complexity, one can check that this
pattern continues
by defining the $j$th correction function $\delta_j$,
so that $x_{j+1} = x_j + \delta_j$, then using \eqref{R} to write
\be
R[x_{j+1}] \;=\;
R[x_j] + 2x_j \delta_j + \delta_j^2 + \delta_j'
\;=\;
\delta_j^2 + \delta_j'
~,
\label{Rdelta}
\ee
where the first two terms cancelled by design due to \eqref{iter}.
The resulting evolution of the residual is then
summarized as follows, which is proved simply by
substituting $\delta_j := -R[x_j]/2x_j$ into \eqref{Rdelta} and using
the quotient rule.
\begin{pro}\label{p:Riter}
  Let $x_j \in C^2([a,b])$ be a function on an interval $[a,b]\subset \R$
  % we may as well be pedantic about conditions
  such that $x_j(t) \neq 0, \; \forall t\in [a,b]$,
  let $\om\in C^1([a,b])$,
  % *** add in gamma too
  and let $x_{j+1}$ be given by the single iteration \eqref{iter}.
  Then their associated residual functions \eqref{R} on $[a,b]$
  are related pointwise by
  \be
  R[x_{j+1}] = \frac{1}{2x_j}\biggl(
  \frac{x_j'}{x_j} R[x_j]
  - R[x_j]'
    \biggr) + \biggl(\frac{R[x_j]}{2x_j}\biggr)^2~.
  \label{Riter}
  \ee
\end{pro}
Thus, with the initialization \eqref{init},
assuming that the lower bound on each $x_j$ is no smaller than $\bigO(\om)$,
and that the quadratic term in \eqref{Riter} is small,
one might hope that
the residual shrinks like $R[x_j] = \bigO(\om^{1-j})$ for each $j=0,1,\dots$.
Unfortunately this will turn out not to be true, due to the increasing
order of derivatives of $\om(t)$, which will lead to an
asymptotic (but not convergent) series.
However, we show that the iteration is numerically useful,
and analyse its residual rigorously in Theorem~\ref{t:R}.


\subsection{Direct spectral solution on nonoscillatory intervals}

*** describe whatever spectral method

\subsection{Adaptive selection of interval size and type}

*** to do. No numerical results here.


% EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
\section{Error analysis}

From now on we abbreviate the residual function by $R_j = R_j(t) := R[x_j](t)$.
Proposition~\ref{p:Riter} described the evolution of $R_j$
under the proposed iteration, which is local in $t$.
For smooth $x_j$ of size $\bigO(\om)$, and smooth $R_j$,
it suggests geometric reduction in $R_j$ by a factor $\bigO(\om)$ per iteration;
however, this is misleading in general because of
growth of high-order $t$-derivatives.
The series will turn out to be merely asymptotic in the large parameter $\om$.
We now make this argument rigorous, for $\om$ analytic
and sufficiently large throughout some neighborhood of $t$.

%One might hope that, given $x_j'$ and $x_j$ of order $\om$,
%and a lower bound on $x_j$ of the same order,
%then $R_j$ is reduced at each point by a factor $\bigO(\om)$.
%then the last term is negligible
%then if $R_j$ and $R_j'$ are sufficiently small


\begin{thm}\label{t:R}
  Fix $t\in\R$, and let the frequency function $\om$ be analytic
  in the closed ball $B_\rho(t) := \{z\in\C : |z-t| \le \rho\}$,
  for some $\rho>0$, with bounds
  \bea
  \eta_1 &\le& |\om(z)| \;\le\; \eta_2~, \qquad \forall z\in B_\rho(t)~,
  \label{ommag} \\
  |\om'(z)| &\le& \eta_3 \;\le\; \frac{\eta_1^2}{18}~,
  \qquad \forall z\in B_\rho(t)~,
  \label{omder}
  \eea
  that is, its derivative should be sufficiently small.
  Let $k\in\{0,1,\dots\}$ be sufficiently small such that
  \be
  r := \frac{1}{2\te_1 \rho}
  \biggl(1 + \frac{\te_2}{\te_1}\biggr) k + \frac{\eta_3}{4\te_1^2}
  \; \le \; \frac{4}{5}
  \label{r}
  \ee
  where $\te_1:=\eta_1 - 3\eta_3/\eta_1$ and $\te_2:=\eta_2 + 3\eta_3/\eta_1$.
  Then after any number $j\le k$ of iterations of \eqref{init}--\eqref{iter},
  the function $x_j$ has Riccati residual \eqref{R} bounded at the point $t$ by
  \be
  |R_j(t)| \;\le\; \eta_3 r^j~.
  \label{Rjbnd}
  \ee
\end{thm}
This shows, for $\om$ of sufficiently small derivative
relative to its magnitude,
temporary geometric convergence up to $k$ iterations,
but at a rate $r$ that deteriorates with $k$.
And for any $k$ to exist satisfying the condition on $r$, $\om$ must have
a sufficiently large lower bound $\eta_1$, i.e., $t$
must be in a sufficiently oscillatory region for the original ODE.

\begin{rmk}\label{r:slight}
  By \eqref{omder}, $\te_1 \ge 5\eta_1/6$ and $\te_2 \le \eta_2 + \eta_1/6$,
  thus $[\te_1,\te_2]$ is only a slight
  expansion of the interval $[\eta_1,\eta_2]$ containing the range of
  $|\om|$ in the ball.
  Applying this we see that the last term in \eqref{r}
  is no more than $1/50$, thus can cause only slight deterioration in the rate.
\end{rmk}

\begin{rmk}
  In the limit when $\om$ tends to be constant in the ball ($\eta_3$ small),
  then $r$ tends to $k/\om \rho$, where we can interpret $\om\rho$ as
  the number of oscillation periods across the ball radius divided by $2\pi$.
  Then, for example, with a ball of radius 50 periods, $r$ satisfies \eqref{r}
  for $k\le 25$.
\end{rmk}

\begin{proof}
  Define the concentric nested set of closed balls $B_j := B_{\rho_j}(t)$,
  with radii $\rho_j := (1-j/k)\rho$, $j=0,1,\dots,k$. Note that
  $B_0$ is the original ball in the statement of the theorem, while
  $B_k = \{t\}$ is the single point of interest.
  For any function $f$ analytic in $B_j$ we abbreviate the $\infty$-norm by
  $\|f\|_j := \max_{z \in B_j}|f(z)|$.
  We will need a bound for $f'$ in $B_{j+1}$ in terms of $\|f\|_j$ by
  applying Cauchy's theorem for derivatives
  \cite{steinshakarchi},
  \be
  f'(z) = \frac{1}{2\pi i} \int_{|\zeta-t|=\rho_j} \frac{f(\zeta)\, d\zeta}{(\zeta-z)^2}
  ~, \qquad z \in B_{j+1}~.
  \label{cauder}
  \ee
  Bounding the integrand, then using the cosine rule we get
  $$
  |f'(z)| \;\le \;
  \frac{\|f\|_j}{2\pi} \int_{|\zeta-t| = \rho_j} \frac{|d\zeta|}{|\zeta-z|^2}
  =
  \frac{\|f\|_j}{2\pi} \int_0^{2\pi} \frac{\rho_j\, d\theta}{|z|^2 + \rho_j^2 - 2|z|\rho_j \cos \theta}~.
  $$
  We now use $\int_{0}^{2\pi} d\theta /(a + b \cos \theta)^2 = 2\pi/\sqrt{a^2-b^2}$
  for $b<a$ \cite[Eq.~3.613.1]{GR8}, with
  $a = |z|^2 + \rho_j^2$ and $b = -2|z|\rho_j$, so that
  $\sqrt{a^2-b^2} = \rho_j^2-|z|^2$.
  Noting that the case $|z| = \rho_{j+1}$ bounds the others,
  and using $\rho_j=(1-j/k)\rho$, we compute, for any $0\le j < k$,
  \be
  \|f'\|_{j+1} \le
  \frac{\|f\|_j}{2\pi} \frac{2\pi\rho_j}{\rho_j^2-\rho_{j+1}^2}
  =
  \frac{\|f\|_j}{2 \rho} \frac{k(k-j)}{2k-2j-1}
  =
  \frac{\|f\|_j}{2 \rho} k \left[ \frac{1}{2} + \frac{1}{2(2k-2j-1)}\right]
  \le
  \frac{k}{\rho}\|f\|_j~.
  %~, \; 0\le j< k
  \label{derbnd}
  \ee
  Note that this bound is a factor $\bigO(k)$ better than naively
  lower bounding the denominator in \eqref{cauder}.
  
  %The 
  We now use induction in iteration number $j$.
%  and consider the iteration acting on the functions
  We take as the induction hypothesis that $x_\ell$ (and thus $R_\ell$)
  is analytic in $B_j$, for all $0\le \ell \le j$, with
  \bea
  \te_1 \;\le\; |x_\ell| &\le& \te_2
  \qquad \mbox{ in } B_j~, \quad \mbox{ for all } \ell = 0,1,\dots,j~,
  \label{hypx}
  \\
  |R_\ell| &\le& \eta_3 r^\ell \quad \mbox{ in } B_j~, \quad \mbox{ for all } \ell = 0,1,\dots,j~.
  \label{hypR}
  \eea
  Assuming for now the hypothesis for $j$, we apply simple bounds to the
  residual iteration \eqref{Riter},
  lower-bounding denominator magnitudes and upper-bounding numerators
  via \eqref{hypx},
  and applying \eqref{derbnd} to the two derivative terms, to get
  $$
  \|R_{j+1}\|_{j+1}
  \;\le\;
  \frac{1}{2\te_1}\biggl(
  \frac{1}{\te_1} \frac{k}{\rho}\te_2 + \frac{k}{\rho}
  \biggr)\|R_j\|_j
  +\biggl(\frac{\|R_j\|_j}{2\te_1}\biggr)^2
  \;\le\;
  r \cdot \|R_j\|_j
  \;\le\;
  \eta_3 r^{j+1}~,
  $$
where in the middle step we used the crude bound $\|R_j\| \le \eta_3$
from \eqref{hypR}, then the definition of $r$ in \eqref{r}.
%This handles the case $\ell=j+1$.
The lower cases $\ell\le j$ follow trivially from the hypothesis since the balls
are nested.
Thus \eqref{hypR} is proven for $j+1$.

It remains to verify that \eqref{hypx} also holds for $j+1$.
By the functional iteration \eqref{init}--\eqref{iter},
$$
x_{j+1}(z) = i\om(z) - \sum_{\ell=0}^{j} \frac{R_\ell(z)}{2 x_\ell(z)}~,
\qquad z\in B_{j+1}~.
$$
Using the hypothesis, the sum is pointwise bounded in magnitude
in $B_{j+1}$ by
$$
\left|\sum_{\ell=0}^{j} \frac{R_\ell}{2 x_\ell} \right|
\;\le\; \frac{\eta_3}{2\te_1} \cdot \sum_{\ell=0}^j r^{\ell}
%\;\le\; \frac{\eta_3}{2\te_1} \frac{1}{1-r}
\;\le\; \frac{\eta_3}{2\te_1} \cdot 5
\;\le\; \frac{3\eta_3}{\eta_1}
$$
where the upper bound in \eqref{r} was used to bound the geometric series,
and $\te_1 \ge 5\eta_1/6$.
Combining this with $|\om| \le \eta_2$ gives
$\|x_{j+1}\|_{j+1} \le \eta_2 + 3\eta_3/\eta_1 = \te_2$ which verifies
the upper bound \eqref{hypx} for $j+1$.
Instead combining with $|\om| \ge \eta_1$ gives
$|x_{j+1}| \ge \eta_1 - 3\eta_3/\eta_1 = \te_1$ in $B_{j+1}$,
which verifies the lower bound \eqref{hypx}.
Again, the hypothesis is inherited for $\ell\le j$ by the nesting of the balls.

Finally, the base case $j=0$ for induction
satisfies \eqref{hypx}--\eqref{hypR}
by the conditions of the theorem,
since $x_0(t) = i\om(t)$, noting $[\eta_1,\eta_2] \subset [\te_1,\te_2]$,
while $R_0(t) = i\om'(t)$ is bounded in magnitude by $\eta_3$.
\end{proof}

In the above theorem, the choice of $4/5$ in \eqref{r} was merely a convenient one, chosen so that the bounds $[\te_1,\te_2]$ on $|x_j|$
involving the geometric sum were
not much wider than the bounds $[\eta_1,\eta_2]$ on $|\om|$.
%It is possible to write simpler but less tight formulae for $r$.


\subsection{Practical aspects of residual reduction}

The fact that, as $k$ grows,
the rate $r$ in Theorem~\ref{t:R} deteriorates
we believe to be an inevitable consequence of
the series being asymptotic in $1/\om$ but not convergent.
We support this in numerical results in ***.

However, given a function $\om(t)$ uniformly large enough
in a ball,
by stopping at a roughly optimal $k$ (an idea
called ``superasymptotics'' \cite{berrysuper,boydsuper})
one can achieve
exponential convergence with respect to the size of the frequency $\om$,
as follows.

\begin{cor}[Superasymptotic approximation]\label{super}
  Suppose $\om$ satisfies the conditions \eqref{ommag}--\eqref{omder}
  of Theorem~\ref{t:R}
  about a point $t\in\R$,
  and let $\alpha := (1+\te_2/\te_1)/2\te_1\rho$ be the
  first term in the rate \eqref{r} arising from these bounds.
  Then there is a $k\in\{0,1,\dots\}$ such that
  $$
  |R_k(t)| \; \le \; e \eta_3 e^{-1/3\alpha}~.
  $$
\end{cor}
\begin{proof}
  Set $k$ to be the integer in the interval $[1/3\alpha-1, \, 1/3\alpha)$.
    Then since the final term in \eqref{r} is bounded by $1/50$
    by Remark~\ref{r:slight}, and the
    first term $k\alpha \le 1/3$, we get $r\le e^{-1}$, which
    is also $\le 4/5$ so that \eqref{Rjbnd} holds.
    Choosing $j=k$, and inserting a lower bound on $k$,
    $|R_k(t)| \le \eta_3 r^k \le \eta_3 e^{-(1/3\alpha-1)}$, which
    finishes the proof.
  \end{proof}
Recalling that $\alpha = \bigO(1/\om)$, this
shows that an $\om$-dependent
number of iterations can give, in exact arithmetic, an
exponentially convergent pointwise residual,
$$
|R(t)| \;=\; \bigO(e^{-c\om})
$$
for some constant $c>0$.
In the limit where
$\om$ tends to constant in the ball of radius $\rho$ about $t$, then
$\alpha$ tends to $1/\om\rho$, so the above
constant is roughly $c\approx \rho/3$.
Equivalently, roughly one decimal digit is achieved per
$1.1$ periods of oscillation across the ball radius.
% log(10)*3/(2*pi) = 1.099

However, this optimal number of iterations grows as $k = \bigO(\om)$,
so for large $\om$ is impractical (and unnecessary).
In practice we use an adaptive stopping criterion for $k$ based
on user-defined tolerance.
For large $\om$ this will have \textit{nearly} geometric convergence
for the first few iterations, sufficient to reach
machine accuracy efficiently.





\subsection{TODO}

*** Add in $\gamma$ throughout above!

*** compare at least in a paragraph to Bremer's iteration
\footnote{J. Bremer, private communication.}

*** relate the ball in the theorem to the interval size that
can be well-approximated by $p$th order Cheby,
using Bernstein ellipse of Tref.


*** Connect $R$ to residual error in sat the ODE (easy),
then from this to solution error bounds, maybe?
This is similar to local discretization err
in a more typical numerical solver.
(check LeVeque, Tref, Hairer, etc).




% BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
\bibliographystyle{abbrv}
\bibliography{refs}
\end{document}

