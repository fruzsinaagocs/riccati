\documentclass[10pt]{article}
\oddsidemargin = 0.2in
\topmargin = -0.5in
\textwidth 6in
\textheight 8.5in

\usepackage{graphicx,bm,hyperref,amssymb,amsmath,amsthm}

% -------------------------------------- macros --------------------------
% general ...
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\bmp}[1]{\begin{minipage}{#1}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\pig}[2]{\bmp{#1}\includegraphics[width=#1]{#2}\emp} % mp-fig, nogap
\newcommand{\bp}{\begin{proof}}
\newcommand{\ep}{\end{proof}}
\newcommand{\ie}{{\it i.e.\ }}
\newcommand{\eg}{{\it e.g.\ }}
\newcommand{\etal}{{\it et al.\ }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdc}[3]{\left. \frac{\partial #1}{\partial #2}\right|_{#3}}
\newcommand{\infint}{\int_{-\infty}^{\infty} \!\!}      % infinite integral
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}^2}
\newcommand{\ve}[4]{\left[\begin{array}{r}#1\\#2\\#3\\#4\end{array}\right]}  % 4-col-vec
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\bigO}{{\mathcal O}}
\newcommand{\qqquad}{\qquad\qquad}
\newcommand{\qqqquad}{\qqquad\qqquad}
\newcommand{\eps}{\epsilon}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\vol}{vol}
\newtheorem{thm}{Theorem}
\newtheorem{cnj}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}
% this work...
\newcommand{\om}{\omega}



\begin{document}

\title{Analysis of an iteration for non-oscillatory solutions to the Riccati form of the phase function for 2nd-order linear ODEs}

\author{Fruszina Agocs and Alex H. Barnett}
\date{\today}
\maketitle

\begin{abstract}
We gather basic definitions and error analysis.
\end{abstract}

\section{Introduction}

We are interested in efficient numerical solution of
the oscillatory linear 2nd-order ODE
\be
u''(t) +\gamma(t) u'(t) + \om^2(t)u(t) \;=\; 0
~,
\qquad t \in (0,1)~,
\label{ode}
\ee
with initial conditions
\bea
u(0) &=& u_0 \\
\label{ic0}
u'(0) &=& u_1~.
\label{ic1}
\eea
%Here the local frequency function $\om(t)$ is purely real or purely imaginary.
In regions where $\om \gg 1$
then the solution $u$ is oscillatory, meaning that on some local
scale it is approximately $a e^{i\om_0t} + b e^{-i\om_0t}$ where $\om_0$ is the local value of $\om(t)$.
Conventional ODE integrators then require discretization with several
time-points per local period $2\pi/\om$, which can be prohibitively slow.
This has led to the development of more efficient solvers for this
high-frequency case.
For now we consider only regions where $\om$ is real and positive;
one may also consider $q(t):=\om^2(t)$ real and allow evanescent
$q<0$ regions, where the solution $u$ has rapid growth and decay.
For now we ignore the real-valued damping function $\gamma(t)$,
setting $\gamma \equiv =0$. *** to include.

Background. ***
WKB blah.
QML.
oscode.
Recently Bremer following Bremer--Rokhlin has proposed an efficient
method
which requires $\bigO(1)$ effort per $t$ evaluation for such an
oscillatory solution, by solving for a nonoscillatory
phase function for Kummer's equation.
The existence of such a nonoscillatory phase function
(meaning that its Fourier transform is exponentially small in $\om$ ***
look up) is proven by Bremer--Rokhlin.
Despite this, finding initial conditions on the phase function leading to
a nonoscillatory solution is challenging.
Those authors proposed a
``windowing'' method to numerically find such a global phase function
on an interval:
the idea is to use a $C^\infty$ partition of unity near the upper end of the interval ($t=1$) to smoothly ``blend'' $q$ to
a constant function, then integrate backwards from the constant region
down to the lower end ($t=0$)
finally integrating $t\in[0,1]$ forward using the original $q$.
This exploits the facts that i) a nonoscillatory phase function for
the constant case $q(t) = \om_0^2$ is simple to write down, and
ii) the smooth window allows
the nonlinear Kummer oscillator to make a smooth (in physics
terminology ``adiabatic'') transition which excites
only exponentially small oscillation amplitude.
ETC.

Here we present an alternative approach using
an asymptotic expansion for a nonoscillatory
Riccati phase function, meaning simply that the
phase function is the logarithmic derivative $x(t) = (\log u(t))'$.
Our expansion is simpler than the traditional WKB expansion,
although of similar accuracy.
One novelty is that we
implement the expansion iteratively
using high-order numerical differentiation, rather
than algebraically (as WKB is usually applied).
A crucial difference from Bremer is that $x$ is solved for
locally on an interval with adaptively-chosen length, rather than
globally.
Solutions to \eqref{ode}--\eqref{ic1} are then constructed by
gluing intervals using Cauchy data.
Another advantage is we include a damping term $\gamma(t)$,
allowing direct
solution of a variety of 2nd-order linear ODEs.

Review recent WKB-based approaches using WKB as basis on intervals ***.
*** solution orders in $h$ and $\om$.

Like Bremer, our solver is arbitrarily high-order.
One advantage over Bremer is that our method generates an explicit asymptotic
expansion. A disadvantage is that its error is limited by the number
of iterations, and by $\omega$ itself; however,
we will show that close to machine accuracy is easily reached in
intervals that contain many oscillations.

*** better list of adv vs prior work.
(Arb high-order compared to oscode $\bigO(h)$ and the $h^2$ guys.
Local rather than QML or Bremer global.)



\section{The method}

\subsection{Phase function solution on a single interval}

We will use prime to denote $d/dt$.
We consider an interval $t\in[a,b]$ lying in $[0,1]$.
Writing $u = e^z$ where $z'(t) = x(t)$ is the phase function,
then any nonvanishing $u(t)$ satisfies \eqref{ode} if and only if
its complex phase function $x(t)$ satisfies the nonlinear Riccati equation
\be
x' + x^2 + \om(t)^2 \;=\; 0~.
\label{ricc}
\ee
Almost all solutions to \eqref{ricc} are oscillatory,
as the presence of $\om(t)$ would suggest.
This is illustrated by the case of constant $\om(t) = \om_0^2$
which has the family of analytic solutions
$x(t) = \om_0 \tan(\alpha - \om_0t)$ parameterized by $\alpha\in\C$,
where $\re \alpha$ is interpreted as a phase shift and $\im \alpha$
controls the amplitude. Their oscillation frequency is $2\om_0$.
However, the limit $\im \alpha \to \pm \infty$ produces
the only two nonoscillatory solutions $x(t) = \pm i\om_0$.
The picture is similar
for general analytic functions $\om(t)$:
it has recently been proved that there exist nonoscillatory phase functions
(in a precise sense for $\om\gg 1$ involving exponential decay of the Fourier
transform) in that case \cite{Heit15,Brem16}.
In fact this result was proven
for the related real-valued Kummer's equation
\be
\frac{3 \alpha'^2}{4\alpha^2} - \frac{\alpha''}{\alpha} - \alpha^2 +
\om(t)^2 \; = \; 0~,
\label{kummer}
\ee
corresponding to ODE solutions $u(t) = \alpha(t)^{-1/2} e^{\pm i\int \alpha(t) dt}$,
but the existence of a nonoscillatory Kummer's solution implies the same for
a Riccati solution \footnote{We thank Jim Bremer for explaining this argument.}.
The latter is easy to show, since if $\alpha(t)$ satisfies \eqref{kummer},
then defining $\beta = -\alpha'/2\alpha$ allows \eqref{kummer}
to be written
$\beta' - \alpha^2 + \beta^2 + \om(t)^2 = 0$, and these last two equations
are the real and imaginary parts of \eqref{ricc}
for $x = i\alpha + \beta$.

We now turn to our numerical method to find nonoscillatory
solutions to \eqref{ricc}.
When $\om\gg 1$, there are two nonoscillatory solutions
which take the approximate form
$x_{\pm}(t) \approx \pm i\om(t)$, leading to a conjugate pair of basis
counter-rotating functions $u_{\pm}(t) := e^{\int x_\pm(t) dt}$.
Given any trial solution $x$ to \eqref{ricc}, its \textit{residual
  function} is defined as the left-hand side of \eqref{ricc}, namely
\be
R[x](t) := x' + x^2 + \om(t)^2~.
\label{R}
\ee
Our proposal takes the following \textit{functional iteration}
to generate a sequence of functions $x_0, x_1, \dots, x_k$
on $t\in(a,b)$,
using the above positive (say) approximation as a starting point:
\bea
x_0(t) &=& +i\om(t)
\label{init}
\\
x_{j+1}(t) &=& x_j(t) - \frac{R[x_j](t)}{2 x_j(t)}
~,\qquad j=0,1,\dots,k-1~.
\label{iter}
\eea
%with the functional $R$ defined as in \eqref{R}.
This may be justified heuristically as an approximate Newton
iteration to reduce the residual function
(noting that similar exact Newton iterations have been
exploited for analysis \cite{Heit15}).
Namely, since
$$
R[x_j + \delta] = x_j' + \delta' + x_j^2 + 2 x_j \delta + \delta^2 + \om(t)^2\\
= R[x_j] + \delta' + 2x_j\delta + \bigO(\delta^2)~,
$$
by linearizing we get that $\delta$ solves the linear 1st-order ODE
$\delta' + 2x_j(t) \delta  = -R[x_j](t)$,
which has an analytic solution.
Solving this exact Newton update symbolically is the basis of the QML iteration
(*** CITE + check).
However, this ODE is again generally oscillatory,
with unknown initial condition needed for a nonoscillatory solution $\delta$.
Yet if $\delta$ is nonoscillatory the first term $\delta'$ is
a factor $x_j = \bigO(\om)$ smaller than the second term, thus
one might hope that by dropping it a useful reduction in residual might
still result.
This leads to the algebraic formula $2x_j(t) \delta(t) = -R[x_j](t)$
which is solved pointwise for each $t$, and
writing $x_{j+1} = x_j + \delta$ gives \eqref{iter}.
This is a type of defect correction scheme (CITE book).

The early iterates of \eqref{init}--\eqref{iter} illustrate
algebraically
the type of residual reduction that occurs:
\bea
x_0(t) &=&  i\om(t)~, \hspace{3.0in} R[x_0](t) \; = \; i\om'(t) = \bigO(\om)
 \nonumber
\\
x_1(t) &=& i\om(t) - \frac{\om'(t)}{2\om(t)}~,
\hspace{2.5in} R[x_1](t) \; = \; -\frac{\om''}{2\om} + \frac{3\om'^2}{4\om^2}
= \bigO(1)
 \nonumber
\\
x_2(t) &=& i\om(t) - \frac{\om'(t)}{2\om(t)} + \delta_1(t),
\mbox{ where }
\delta_1(t):=\frac{\om''/2\om - 3\om'^2/4\om^2}{2i\om - \om'/\om}
,
\quad R[x_2](t) \; = \; \delta_1' + \delta_1^2 = \bigO(\om^{-1})
\nonumber
\eea
Each iteration the residual drops by a factor $\bigO(\om)$,
where we have used that $\om'$, $\om''$, etc, are
of the same order as $\om$ because $\om(t)$ is smooth,
and assumed that $\om$ is bounded from below also by $\bigO(\om)$.
However, each iteration also results in terms with
one higher time-derivative of $\om$.
Despite the rapid growth in complexity, one can check that this
pattern continues
by defining the $j$th correction $\delta_j$,
so that $x_{j+1} = x_j + \delta_j$, then using \eqref{R} to write
\be
R[x_{j+1}] \;=\;
R[x_j] + 2x_j \delta_j + \delta_j^2 + \delta_j'
\;=\;
\delta_j^2 + \delta_j'
~,
\label{Rdelta}
\ee
where the first two terms cancelled by design due to \eqref{iter}.
Then recalling that $\delta_j := -R[x_j]/2x_j$ proves the following.
\begin{pro}\label{p:Riter}
  Let $x_j$ be an arbitrary smooth function,
  and let $x_{j+1}$ be given by the single iteration \eqref{iter}.
  Then their associated residual functions are related by
  \be
  R[x_{j+1}] = \frac{1}{2x_j}\biggl(
  R[x_j]' - \frac{x_j'}{x_j} R[x_j]
    \biggr) + \biggl(\frac{R[x_j]}{2x_j}\biggr)^2~.
  \label{Riter}
  \ee
\end{pro}
Thus with the initialization \eqref{init},
for each $j=0,1,\dots$,
the residual $R[x_j] = \bigO(\om^{1-j})$ and involves up to $(j+1)$ derivatives
of $\om(t)$.
For $\om$ a general analytic function, eventually the
growth in derivatives overcome the powers,
leading to divergence: the series is asymptotic
but not convergent.

\subsection{Direct spectral solution on nonoscillatory intervals}

\subsection{Adaptive selection of interval size and type}


\section{Error analysis}

Recall Prop.~\ref{p:Riter}.
We use the abbreviation $R_j = R_j(t) := R[x_j](t)$.
Note that since $x_j'$ and $x_j$ are of order $\om$,
that if $R_j$ and $R_j'$ are sufficiently small,
the last term is negligible, and $R_j$ is reduced at each point $t$
by roughly $x_j(t)$.

*** compare at least in a paragraph to Bremer's iteration
\footnote{J. Bremer, private communication.}





% BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
\bibliographystyle{abbrv}
\bibliography{refs}
\end{document}

