\documentclass[10pt]{article}
\oddsidemargin = 0.2in
\topmargin = -0.5in
\textwidth 6in
\textheight 8.5in

\usepackage{graphicx,bm,amssymb,amsmath,amsthm} % Figures and maths
\usepackage{xcolor}
\usepackage[colorlinks]{hyperref} % Hyperlinks
\usepackage[capitalise, nameinlink]{cleveref} % Better referencing
\crefname{equation}{}{} % Cref shows eqs as (n) rather than Eq. (n)
\usepackage{array}
\usepackage{showlabels} % Shows eq, fig, etc labels
\usepackage{siunitx} % SI units
\usepackage[caption=false]{subfig} % For subfloats
\usepackage{bold-extra} % bold for texttt
\usepackage{bm} % bold for maths 
\usepackage{tabularx} % for nice tables
\usepackage[export]{adjustbox} % for adjusting the alignment of subfigures
% Tikz figures
\usepackage{tikz}
\usepackage[most]{tcolorbox}
\usetikzlibrary{positioning}



% SI unit definitions
\DeclareSIUnit\parsec{pc}

% -------------------------------------- macros --------------------------
% general ...
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\bmp}[1]{\begin{minipage}{#1}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\pig}[2]{\bmp{#1}\includegraphics[width=#1]{#2}\emp} % mp-fig, nogap
\newcommand{\bp}{\begin{proof}}
\newcommand{\ep}{\end{proof}}
\newcommand{\ie}{{\it i.e.\ }}
\newcommand{\eg}{{\it e.g.\ }}
\newcommand{\etal}{{\it et al.\ }}
\newcommand{\etc}{{\it etc.\ }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdc}[3]{\left. \frac{\partial #1}{\partial #2}\right|_{#3}}
\newcommand{\infint}{\int_{-\infty}^{\infty} \!\!}      % infinite integral
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}^2}
\renewcommand{\d}{\mathrm{d}} % Upright differential
\newcommand{\ve}[4]{\left[\begin{array}{r}#1\\#2\\#3\\#4\end{array}\right]}  % 4-col-vec
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\bigO}{{\mathcal O}}
\newcommand{\qqquad}{\qquad\qquad}
\newcommand{\qqqquad}{\qqquad\qqquad}
\newcommand{\eps}{\epsilon}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\vol}{vol}
\newtheorem{thm}{Theorem}
\newtheorem{cnj}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{defn}[thm]{Definition}
% this work...
\newcommand{\om}{\omega}
\newcommand{\g}{\gamma}
\newcommand{\te}{\tilde\eta}
\crefalias{prop}{Proposition}
% Review
\newcommand{\AB}[1]{{\color{orange}#1}}
\newcommand{\Fruzsi}[1]{{\color{blue}#1}}

\begin{document}

\title{An adaptive spectral method for oscillatory second-order linear ODEs with frequency-independent cost }

\author{Fruzsina J.\ Agocs and Alex H.\ Barnett}
\maketitle

\begin{abstract}

% Main points to communicate:
% * Presenting a method for second order linear ODEs whose solution may be highly oscillatory
% * Spectral, adaptive -- can deal with oscillatory or slowly varying solutions
% * Based on constructing a nonoscillatory phase function (which obeys the Riccati equation)
% via defect correction which yields an asymptotic series,
% geometric/exponential convergece in residual for first few iters
% * Chebyshev spectral method is alternative solver
% * We demonstrate performance with numerical examples, results

% 168 words!

We introduce an efficient numerical method for second order linear ODEs whose
solution may vary between highly oscillatory and slowly changing over the solution interval.
%
In oscillatory regions the solution is generated via a nonoscillatory phase
function that obeys the nonlinear Riccati equation. We introduce a
defect-correction iteration that gives an asymptotic series for the phase
function; this iteration is spectrally approximated on a Chebyshev grid with a small number of nodes. 
%
For analytic coefficients we prove that each iteration, up to a certain
maximum, reduces the residual by a factor of order of the local frequency. 
%
When the solution changes slowly, the algorithm automatically switches to a
spectral method based on an adaptively varied number of Chebyshev nodes.
%
In numerical experiments we find that out method outperforms other state-of-the-art
oscillatory solvers, most significantly at low-to-intermediate
frequencies and at low tolerances, where our method performs up to $10^6$
times fewer function evaluations.  Even at high frequencies, our method is
on average $10$ times faster than other specialised solvers.
\end{abstract}

\section{Introduction}

The efficient numerical solution of highly oscillatory ordinary differential
equations (ODEs) has long been a challenge.
A wide range of specialised methods exists for dealing with ODEs
with various structures;
see \cite{petzold1997,engquist2009} for thorough reviews.
In this work we handle a commonly occurring
form, the variable-coefficient, linear, second
order ODE with
% \Fruzsi{analytic} % numerical method could handle merely smooth.
smooth coefficients.
This includes simple % you didn't mean "single", right?
harmonic oscillators with time-dependent coefficients,
thus is pivotal in many computational physics problems.
% note I cut "simple-looking". not needed
One application in which an extremely fast numerical solution is needed
is Bayesian parameter estimation in cosmology.
Specifically, each spatial Fourier mode of the density pertubations
obeys an ODE in time of this form,
with solutions that may change repeatedly between smooth and
highly oscillatory characters \cite{agocs2020dense}.
Of order $10^3$ modes are needed, and this must be repeated for
up to $10^6$ trial parameter choices; thus the ODE solver
may be called a billion times, and an efficient frequency-independent
solver is crucial.
Besides inflationary cosmology \cite{Hergt2022,martin2003,winitzki2005}, this class of ODE also appears in
Hamiltonian dynamics \cite{Pritula2018,fiore2022}, particle accelerators and plasma physics
\cite{courant1958,davidson2001,hazeltine2003,lewis1968}, electric circuits
\cite{likharev2022}, satellite systems \cite{saxena2020}, acoustics and gravity waves \cite{filippi1998,einaudi1970}, and quantum mechanics \cite{griffiths2018,adhikari1988,arnold2011wkb,cea1982}.
In many of these fields our proposed method could supply a simple and efficient
numerical replacement for
asymptotic analytic methods which at high order can become cumbersome to apply.
%  https://www.math.leidenuniv.nl/~vivi/stud_sem2011/BosleyKevorkian92.pdf  latex fail -> coupled oscillators

% Define the problem completely before you start getting into details about complex omega:
The ODE under study has the form
\be
u''(t) + 2\g(t) u'(t) + \om^2(t)u(t) = 0, \qquad t \in (t_0,t_1) \subset \R,
\label{ode}
\ee
where $\g$ is a given smooth (by which we mean $C^\infty$) damping function,
and $\om$ is a given smooth, but possibly very large, real-valued
local frequency function\footnote{If coefficients are merely piecewise smooth, a solution may simply be patched together at the breakpoints, thus we do not discuss this situation further.}.
We solve the initial value problem (IVP)
\begin{align} % \ba, \ea doesn't work here, LaTeX complains
    u(t_0) &= u_0, \label{ic0} \\
    u'(t_0) &= u'_0, \label{ic1}
\end{align}
although we note that the methods that we present could easily be applied to
two-point boundary value problems.
%Here the local frequency function $\om(t)$ is purely real or purely imaginary.
% AB: so is om(t) complex or real?
% FA: technically it could be complex. The issue is when we try to use a spectral method over complex coefficients.
% AB: Hmmm, really, why does that go wrong?
% FA: I would expect it to pick up the exponentially growing mode -- but not so sure now.
The restriction to real $\om$ is not crucial,
but avoids complications with exponentially large dynamic ranges
(e.g., deeply evanescent regions)
that we leave for future study.
%For now, we only consider the case when $\om$ is purely real, but note that our
%asymptotic approximation for the Riccati phase function described in
%\cref{phasefun} is applicable to complex $\om$.
% consider cut or footnote. Notice it forced you to explain stuff we're going to explain shortly anyway, so it's confusing.
%
%Care needs to be taken in
%regions of $\Im(\om) \neq 0$ if any other method is used, however, since due to
%roundoff error some of the exponentially growing solution will inevitably be
%picked up and will dominate over the evanescent solution\footnote{In quantum
%mechanical applications where having a purely imaginary $\om$ is ubiquitous, a
%workaround is to integrate the ODE from outside the edges of the potential well
%towards the inside.}. We defer allowing for complex $\om$ to future work.

In regions where $\om \gg 1$,
the solution $u$ is oscillatory:
loosely speaking,
it has local approximate form $a e^{i\om_0t} + b e^{-i\om_0t}$, where $\om_0$
is a local value of $\om(t)$.
Conventional ODE integrators then require discretization with several
grid points per local period $2\pi/\om_0$, forcing the overall
cost to be $\bigO(\om_0)$, which can be prohibitively slow.
This has led to the development of solvers
that exploit new representations or asymptotic expansions of the solution, allowing many periods between discretization nodes.
% AB: note the following waypoint to the reader re the big section about to happen:
We now review such prior work,
while noting that, to our knowledge, our proposal is the first
tackle the problem adaptively with spectral accuracy
when the solution has both highly oscillatory and smooth regimes.

%more efficient solvers for this
%high-frequency case that exploit new representations of the solution to allow m%any periods between discretization nodes. *** REFS.
%However, as we will review later, none of these methods so
%far has been robust or general enough to

% Asymptotic methods: WKB, oscode, QLM
% Not asymptotics: Bremer
%One class of efficient numerical solvers for highly oscillatory ODEs of the
%above form makes use of asymptotic expansions.  <- this feels redundant.

% WKB
The most common asymptotic expansion
is the Wentzel--Kramers--Brillouin (WKB), a
perturbation method applied to linear differential equations that contain some
small parameter $\varepsilon$ in the highest derivative
\cite{benderorszag,logan}.
This approximation arose in quantum mechanics where it is still widely
applied for approximate analytic solutions of the Schr\"{o}dinger equation.
Writing $\om(t) = \om_0\Omega(t)$
where $\Omega(t)$ is of unit size, and the asymptotic parameter
is $\om_0 \gg 1$,    % AB: note how I cut down here
% AB: oops not \ll
% AB: note you don't set gam=0 until later, so I fixed that
\cref{ode} becomes
\be\label{odewkb}
u'' + 2\gamma u' + \om_0^2 \Omega^2 u = 0.
\ee
Substituting $u(t) = e^{\om_0 z(t)}$ and defining $z'(t) = x(t)$
% AB note how I cue the reader these are functions of t
gives the Riccati equation (a nonlinear 1st-order ODE) for $x$, 
\be\label{riccwkb}
% AB *** please debug if om_0^2 here ???
% FA: No bugs here, note the exponent in the substitution contains om_0, which kills one factor of om_0.
x' + 2\gamma x + \om_0 x^2 + \om_0 \Omega^2 = 0.
\ee
Making a regular perturbation expansion in $1/\om_0$,
\be\label{wkbexpansion}
x(t) = \sum_{j=0}^{N} \om_0^{-j} x_j(t)~,
% AB note how I put consts before funcs (traditional) and cue the reader re funcs of t, not merely numbers.
\ee
and matching powers of $\om_0$ in \cref{odewkb} gives the recursion relation
for the functions $x_j(t)$
(for simplicity we give only the $\gamma \equiv 0$ case),
\begin{equation}\label{wkbrecur}
  x_0 = \pm i\Omega, \quad x_j = -\frac{1}{2x_0}\left( \dot{x}_{j-1} + \sum_{k = 1}^{j-1}x_k x_{j-k} \right), \quad \text{for } j = 1, 2, \ldots.
  % *** AB: note I killed N since it is never referred to nor given later.
\end{equation}
Although originally an analytic tool,
WKB has recently been exploited in numerical solvers.
These algorithms achieve
$\bigO(1)$ (i.e., $\om_0$-independent) runtime in regions where the asymptotic
series is a sufficiently good approximation.
% note the pivot, since it's a surprise   -> AB, yes, nice.
WKB forms the basis of the method described in
\cite{agocs2020efficient,agocs2020dense}, and associated open-source software
package \texttt{oscode} \cite{agocs2020joss}. This uses the expansion
\cref{wkbexpansion} up to and including $x_3$ to advance the numerical solution
for $u(t)$ in a time-stepping context, but has the ability to switch to a 4,5th
order Runge--Kutta pair if the asymptotic expansion isn't accurate enough. In \cite{arnold2011wkb,korner2022wkb} the ``WKB marching method'' uses the same
expansion truncated after $x_2$ to transform \cref{odewkb} into a smoother
problem in the highly oscillatory regions, otherwise switching to a 4,5th order
Runge--Kutta method.
%The spectral method is based
%on $n+1$ Chebyshev nodes, where $n$ is chosen adaptively, resulting in an
%% CHECK NUMERICALLY!  *** is that still a relevant comment
%$\bigO(h^{n})$ local error, where $h$ is the stepsize. This is in contrast to
%\texttt{oscode} and the WKB-marching method, which both use a fixed (4,5th)
%order Runge--Kutta scheme as their alternative (non-oscillatory)
%solver and produce a
%$\bigO(h^6)$ error.
%
\texttt{oscode}'s WKB-based method produces an $\bigO(h)$
error when $h \ll 1$, although in regions of high-frequency oscillations, $h$
tends to be very large in comparison to the solution's timescale. The
WKB-marching method's oscillatory solver improves on this and has $\bigO(h^2)$ local error,
resulting in better performance at lower tolerances.
A consequence of these two methods having low $h$-order and fixed asymptotic
order is that they are forced to take very small stepsizes at low tolerances,
%which in turn causes them to switch over to their respective Runge--Kutta solvers 
even if the solution is oscillatory. They are therefore only efficient if no more than about 6 digits of accuracy
are required, and can suffer at low-to-intermediate frequencies where more asymptotic terms would be required. Our method, on the other hand, is both spectrally accurate and 
adaptive in the number of terms in its asymptotic expansion.

%The local error of our proposed
%method's Riccati steps depends on the stepsize $h$ in two
%ways: first, the asymptotic expansion's validity is determined by the magnitude
%and rate-of-change of $\om$ and $\g$ over the course of the step (formalised in
%\cref{errorana}); and second, the iterates $x_i$ are computed via numerical
%differentiation on a Chebyshev grid with an adaptive number of nodes, which has
%$\bigO(h^{n+1-k})$ accuracy if the number of iterates included is $k$. In this
%sense, both of the methods making up our solver are arbitrarily high order.
%\AB{Valiant, but I think too much detail here. Just say we are spectral. Move detail to discussion}

% QLM
Instead of finding an approximation via a regular perturbative expansion, the
``quasilinearisation method''
% note M stands for method, so approach was bit confusing
(QLM) \cite{bellman1970} approximates the solution
of \cref{riccwkb} with a series of iterates $x_i(t)$ by converting it to a
differential equation recurrence
% AB: note word order
% *** your labeling was wrong: (8) was just the x_0 which isn't an ODE
% Thus I combined as you did for WKB.
\be
x_0 = \pm i\om,
\quad
x_j' - x_{j-1}^2 + 2x_j x_{j-1} + \om^2 = 0, \quad \text{for } j = 1, 2, \ldots
% *** ditto N as above
\label{qlmode}
\ee
and either solving them analytically, \eg in \cite{mandelzweig2004,krivec2006}
or numerically \cite{krivec2008,krivec2014}. The semi-analytic approach
requires increasing amounts of algebra as the iteration number grows and can
become cumbersome. Integrating each ODE in \cref{qlmode} numerically does not prove to be a better strategy since each such ODE is itself oscillatory.
% AB clarified

% Bremer
% Accuracy?
% arbitrarily high order
Recently Bremer \cite{bremer2018} following \cite{heitman2015,bremer2016} has proposed an efficient
method for solving \cref{ode}, in the special case $\g = 0$, in the high frequency regime. 
%\AB{Is this distinct from the WKB methods just described?
%  Ie, don't they also run in $\bigO(1)$ time?
%  In which case I suggest we state that above, and not make it a new property here.
%  To me the Bremer innovation is high-order approximation of phase func
%  on intervals,
%  plus convergence, plus code.}
The method aims to find a nonoscillatory phase function $\alpha(t)$, defined
via Kummer's equation,
\be
\frac{3 \alpha'^2}{4\alpha^2} - \frac{\alpha''}{\alpha} - \alpha^2 +
\om(t)^2 = 0,
\label{kummer}
\ee
a nonlinear and generally oscillatory ODE, and will therefore be
referred to as the phase function method hereafter.
%\AB{I think you should move eq {kummer} here otherwise reader doesn't know what it is}.
The nonoscillatory phase function $\alpha$ can be used to compute an approximation
of the solution of \cref{riccwkb} accurate to within
$\bigO((\mu\om_0)^{-1}\exp(-\mu\om_0))$ as $\om_0 \to \infty$, where $\mu$ is a constant that depends on $\Omega(t)$ but not $\om_0$.
The proposed algorithm itself generates $\tilde{\alpha}$, an approximation
of $\alpha$, such that the logarithm form of Kummer's
equation is satisfied to within $\bigO(-\tfrac{1}{2}\mu\om_0)$. These
generated phase functions are nonoscillatory in the sense that they can be
accurately represented using a series expansion (\eg via Chebyshev polynomials)
with $\bigO(1)$ terms, independently of $\om_0$.
The basis of the method are the results of \cite{heitman2015,bremer2016}
that prove the existence of such nonoscillatory phase functions if $\Omega(t)$ is
nonoscillatory, in the sense that the Fourier transform of $\log \Omega$ is smooth
and rapidly decaying.
Finding initial conditions on the phase function leading to
a nonoscillatory solution is, however, challenging.
The authors of \cite{bremer2018} proposed a ``windowing'' method to numerically find such a global phase function
on an interval:
the idea is to first transform the interval $t \in [a,b]$ to $[0, 1]$, then use a $C^\infty$ partition of unity near $t=1$ to smoothly ``blend'' $\Omega$ to
a constant function, integrate backwards from the constant region
down to $t = 0$, and finally integrating $t \in [0,1]$ forward using the original $\Omega$.
This exploits the facts that a nonoscillatory phase function for
the constant case $\Omega(t) = 1$ is simple to write down; and
that the smooth window allows
the nonlinear Kummer oscillator to make a smooth 
%(in physics terminology ``adiabatic'') 
transition which excites only an exponentially small oscillation amplitude.
\AB{something about windowing involves parameter choices.
  Nevertheless Bremer's is the most efficient
  and accurate prior method known to the authors.}
\Fruzsi{Need clarification about this comment.}

Here we present an approach using an asymptotic expansion for a nonoscillatory Riccati phase function,
meaning simply that the
phase function is the logarithmic derivative $x(t) = (\log u(t))'$ from \cref{riccwkb}.
%
The series representing the phase function is constructed via a defect
correction scheme, see \cite{bohmer1984} and references therein. It is,
however, asymptotic not convergent, thus only a limited number of iterates may
be used before it becomes unstable, which we address numerically.  
%
Our algorithm is capable of handling a non-zero damping term $\g \neq 0$,
allowing for the direct solution of a wider range of second order, linear ODEs
than the WKB-marching and the phase function method. 
%
The expansion we use does not introduce oscillatory contributions,
hence finds a nonoscillatory approximate phase function without
reference to initial conditions, in contrast to the phase function method. 
%
Another crucial difference from the latter is that $x(t)$ is solved for
locally on an interval with an adaptively-chosen length, rather than globally.
Solutions to \cref{ode,ic0,ic1} are then constructed by time-stepping over the
integration range and matching Cauchy boundary data over adjacent intervals.
%
Our expansion is simpler than the traditional WKB expansion in that the
associated recursion relation $x_i(x_{i-1}, x_{i-2}, \ldots, x_0)$ only
involves the previous term $x_{i-1}$ as opposed to all past terms in
\cref{wkbrecur}. 
%
The simplicity of the recursion relation makes it
computationally simpler for the method to be adaptive in the number of terms 
used in the asymptotic expansion.
%
The expansion presented here has similar accuracy, and
exhibits geometric convergence for the first few iterations, which we show
formally in \cref{errorana}. The iterations are computed using high-order
numerical differentiation, rather than algebraically as the WKB expansion is
usually applied.
%
The phase function method, as with WKB or QLM methods,
is naturally only applicable in the high frequency regime.
Thus, in the present work, we borrow ideas from \cite{agocs2020efficient} to
create a fully adaptive algorithm to switch
solution methods between oscillatory and non-oscillatory intervals,
and to choose lengths of these intervals to achieve a user-requested
tolerance efficiently.
This is achieved by switching between the Riccati asymptotic expansion and a conventional
spectral method on the fly as the solver
steps through the full user-requested interval $[t_0, t_1]$.

%

%Structure
This paper is structured as follows. \cref{methods} describes the various
numerical methods used in the proposed algorithm including the asymptotic
expansion for regions of oscillations, the spectral method based on Chebyshev
nodes, adaptive control of stepsize, and switching between the two alternative
sub-methods. \cref{errorana} discusses the error properties of the method based
on the asymptotic expansion of the Riccati equation and its consequences for
implementation. We show results from numerical experiments in
\cref{numresults}, which involves a comparison of the contemporary ODE solver
mentioned above. \cref{conclusions} concludes this paper with a summary and
suggestions for future work.

%One advantage over Bremer is that our method generates an explicit asymptotic
%expansion. A disadvantage is that its error is limited by the number
%of iterations, and by $\om$ itself; however,
%we will show that close to machine accuracy is easily reached in
%intervals that contain many oscillations.

\section{Methods \label{methods}}

\subsection{Riccati phase function solution on a
  single interval \label{phasefun}}

%\AB{The following will be cut down quite a bit by moving ricc, kummer,
%  eqns etc to intro as discussed there, and referring back. No eqn need appear
%twice, and repetition is generally avoided}

We will use prime to denote differentiation with respect to time $t$.
We consider an interval $t\in[a,b]$ lying in $[0,1]$.  
Writing $u = e^z$ where $z'(t) = x(t)$ is the phase function,
then any nonvanishing $u(t)$ satisfies \cref{ode} if and only if
its complex phase function $x(t)$ satisfies the nonlinear Riccati equation
\be
x' + x^2 + 2\g(t)x + \om(t)^2 = 0.
\label{ricc}
\ee
%For the sake of simplicity, let us set $\g(t) = 0$ for the time being.
Almost all solutions to \cref{ricc} are oscillatory,
as the presence of $\om(t)$ would suggest.
This is illustrated by the case of constant $\om(t) = \om_0$
and $\g(t)\equiv 0$,
which has only the family of analytic solutions
$x(t) = \om_0 \tan(\alpha - \om_0t)$ parameterized by $\alpha\in\C$,
where $\re \alpha$ is interpreted as a phase shift and $\im \alpha$
controls the amplitude. Their oscillation frequency is $2\om_0$.
%\AB{note the $\om_0$ then reinforces using this instead of $\lambda$
%  in the intro for Bremer.}
However, the limit $\im \alpha \to \pm \infty$ produces
the only two nonoscillatory solutions $x(t) = \pm i\om_0$.
The picture is similar
for general analytic functions $\om(t)$:
it has recently been proved that there exist nonoscillatory phase functions
(in a precise sense for $\om\gg 1$ involving exponential decay of the Fourier
transform) in that case \cite{heitman2015,bremer2016}.
In fact this result was proven
for the related real-valued Kummer's equation \cref{kummer},
%\AB{now we move Kummer back to intro and refer to that}
%\be
%\frac{3 \alpha'^2}{4\alpha^2} - \frac{\alpha''}{\alpha} - \alpha^2 +
%\om(t)^2 = 0,
%\label{kummer}
%\ee
corresponding to ODE solutions $u(t) = \alpha(t)^{-1/2} e^{\pm i\int \alpha(t) \d t}$,
but the existence of a nonoscillatory Kummer's solution implies the same for
a Riccati solution.
The latter is easy to show, since if $\alpha(t)$ satisfies \cref{kummer},
then defining $\beta = -\alpha'/2\alpha$ allows \cref{kummer}
to be written
$\beta' - \alpha^2 + \beta^2 + \om(t)^2 = 0$, and these last two equations
are the real and imaginary parts of \cref{ricc}
for $x = i\alpha + \beta$.
\footnote{We thank Jim Bremer for explaining this argument.}

We now present a new numerical method to find nonoscillatory
solutions to the nonlinear Riccati equation \cref{ricc}.
%(in its most general form, \ie without imposing $\g(t) = 0$).
When $\om\gg 1, \g$, there are two nonoscillatory solutions
which take the approximate form
$x_{\pm}(t) \approx \pm i\om(t)$, leading to a conjugate pair of basis
counter-rotating functions $u_{\pm}(t) := e^{\int x_\pm(t) \d t}$.
Given any trial solution $x$ to \cref{ricc}, its \textit{residual
  function} is defined as the left-hand side of \cref{ricc}, namely
\be
\label{R}
R[x](t) := x' + x^2 + 2\g(t)x + \om(t)^2.
\ee
Our proposal takes the following \textit{functional iteration}
to generate a sequence of functions $x_0, x_1, \dots, x_k$
on $t\in(a,b)$,
using the above positive (say) approximation as a starting point:
\begin{align}
x_0(t) &= +i\om(t)
\label{init}
\\
    x_{j+1}(t) &= x_j(t) - \frac{R[x_j](t)}{2 \left( x_j(t) + \g(t) \right)}, \qquad j=0,1,\dots,k-1.
\label{iter}
\end{align}

%with the functional $R$ defined as in \cref{R}.
This may be justified heuristically as an approximate Newton
iteration to reduce the residual function
(noting that similar exact Newton iterations have been
exploited for analysis \cite{heitman2015}).
Namely, since
\begin{align}
    R[x_j + \delta] &= x_j' + \delta' + x_j^2 + 2 x_j \delta + \delta^2 
    + 2\g x_j + 2\g\delta + \om(t)^2 \nonumber \\
    &= R[x_j] + \delta' + 2x_j\delta + 2\g\delta + \bigO(\delta^2), \nonumber
\end{align}
by linearizing we get that $\delta$ solves the linear 1st-order ODE
$\delta' + 2x_j(t) \delta + 2\g \delta  = -R[x_j](t)$,
which has an analytic solution.
Solving this exact Newton update symbolically is the basis of the quasilinearization method (QLM) \cite{bellman1970}.
However, this ODE is again generally oscillatory,
with unknown initial condition needed for a nonoscillatory solution $\delta$.
Yet if $\delta$ is nonoscillatory the first term $\delta'$ is
a factor $x_j = \bigO(\om)$ smaller than the second term, thus
one might hope that by dropping it a useful reduction in residual might
still result.
%\footnote{\Fruzsi{[Strictly speaking, we should then be dropping the $\g$ term as well, since it's also an order of magnitude smaller...]}}. 
This leads to the algebraic formula $2\left(x_j(t) + \g(t)\right) \delta(t) = -R[x_j](t)$
which is solved pointwise for each $t$, and
writing $x_{j+1} = x_j + \delta$ gives \cref{iter}.
This is a type of defect correction scheme \cite{bohmer1984}.
The result is nonoscillatory by construction, without explicit reference
to initial conditions.

The early iterates of \cref{init,iter} illustrate
algebraically
the type of residual reduction that occurs. We take $\g(t) = 0$ below for the
sake of simplicity, but the results hold for the case of $\g(t) \neq 0$. The first few iterates and residuals are
\begin{align}
    x_0 &=  i\om, &&R[x_0] = i\om' = \bigO(\om), \nonumber \\
x_1 &= i\om - \frac{\om'}{2\om}, 
    &&R[x_1] = -\frac{\om''}{2\om} + \frac{3\om'^2}{4\om^2} = \bigO(1), \nonumber \\
x_2 &= i\om - \frac{\om'}{2\om} + \delta_1, \mbox{ where }
    \delta_1 \equiv \frac{\frac{\om''}{2\om} - \frac{3\om'^2}{4\om^2}}{2i\om - \frac{\om'}{\om}}, \quad
    &&R[x_2] = \delta_1' + \delta_1^2 = \bigO(\om^{-1}). \nonumber
\end{align}
To measure the size of term we assume that $\om'$, $\om''$, \etc, are
of the same order as $\om$, because $\om(t)$ is smooth,
and also assume that $\om$ has a lower bound of the same order. If we were to include $\g$, we would assume $\g = \bigO(1) $ in terms of $\om$, and that its derivatives are of the same order as $\g$ itself by the same reasoning as with $\om$. 
Thus we see that each iteration the residual drops by a factor $\bigO(\om)$.
However, each iteration also results in terms with
one higher time-derivative of $\om$ (and $\g$).
Despite the rapid growth in complexity, one can check that this
pattern continues
by defining the $j$th correction function $\delta_j$,
so that $x_{j+1} = x_j + \delta_j$, then using \cref{R} to write
\be
R[x_{j+1}] = R[x_j] + 2\left(x_j + \g \right)\delta_j  + \delta_j^2 + \delta_j' = \delta_j^2 + \delta_j',
\label{Rdelta}
\ee
where the first two terms cancel by design due to \cref{iter}.
The resulting evolution of the residual is then
summarized as follows, which is proved simply by
substituting $\delta_j \equiv -R[x_j]/2(x_j + \g)$ into \cref{Rdelta} and using
the quotient rule.
\begin{pro}\label{PRiter}
  Let $x_j \in C^2([a,b])$ be a function on an interval $[a,b]\subset \R$
  % we may as well be pedantic about conditions
  such that $x_j(t) \neq 0 \; \forall t\in [a,b]$,
  let $\om\in C^1([a,b])$,
  $\g\in C^1([a,b])$,
  and let $x_{j+1}$ be given by the single iteration \cref{iter}.
  Then their associated residual functions \cref{R} on $[a,b]$
  are related pointwise by
  \be
  \label{Riter}
    R[x_{j+1}] = \frac{1}{2(x_j + \g)}\left( \frac{(x_j + \g)'}{x_j + \g} R[x_j] - R[x_j]' \right) 
    + \left(\frac{R[x_j]}{2(x_j + \g)}\right)^2. 
  \ee
\end{pro}
Thus, with the initialization \cref{init},
assuming that the lower bound on each $x_j$ is no smaller than $\bigO(\om)$,
and that the quadratic term in \cref{Riter} is small,
one might hope that
the residual shrinks like $R[x_j] = \bigO(\om^{1-j})$ for each $j=0,1,\dots$.
Unfortunately this will turn out not to be true, due to the increasing
order of derivatives of $\om(t)$, which will lead to an
asymptotic (but not convergent) series.
However, the iteration is numerically useful since when the right conditions
are met the residual convergences geometrically for the first few iterations with a
rate that gets better with larger $\om$. We show this formally in \cref{TR}.

%\AB{there is currently a problem of ordering in that Fig 1 can't be understood until the Cheby discr method is explained. (Or is Fig 1 not done via Cheby - surely must be). In which case Cheby must come before the demo. I made a crude attempt at this.}
The numerical evaluation of the asymptotic solution $x(t)$, and the solution of
the original ODE $u(t)$ requires repeated differentiation (as part of each
correction term) and quadrature (since $u(t) = \exp \int^t x(\sigma)
\mathrm{d}\sigma$). 
Since our solver is a stepper, we are not attempting to find a global solution
$u(t)$ for the entire integration range, and only need to compute the
asymptotic solution over a timestep, $t \in [t_i, t_i+h]$ at a time. At each 
timestep, we choose to perform differentiation and quadrature by discretizing
time on a Chebyshev grid of $n + 1$ nodes.  
Over the standard interval $t \in [-1, 1]$, the $n+1$ Chebyshev nodes are given by 
\be\label{chebnodes}
\tilde{t}_n := \{\tilde{t}_j\}, \quad \tilde{t}_j = \cos\left( j\pi/n\right), \quad j = 0, 1, \ldots, n,
\ee
where the parameter $n$ can be set by the user with a default value of $n = 16$, and is fixed throughout integration. 
One may then define an $(n+1) \times (n+1)$ differentiation matrix $\tilde{D}_n$ which, given a vector of function
values $\tilde{v}_n$ defined on the Chebyshev nodes $\tilde{t}_j$, returns $\tilde{w}_n$, the function's
derivative evaluated at the nodes,
\be\label{diffmat}
\tilde{w}_n = \tilde{D}_n\tilde{v}_n, \quad \tilde{f}_n := f(\tilde{t}_n),
\ee
The timestep $t \in [t_i, t_i+h]$ can always be scaled up or down to the standard interval via
\be\label{scaledt}
t_n := \{ t_j \}, \quad t_j = t_i + \frac{h}{2} + \frac{h}{2}\tilde{t}_j, \quad j = 0, 1, \ldots, n. 
\ee
Under the scaling, the differentiation matrix transforms as
\begin{align}
    w_n &= D_nv_n, \quad f_n := f(t_n) \label{scaledDdef}\\
    D_n &= \frac{2}{h}\tilde{D}_n. \label{scaledD}
\end{align}
We then use $D_n$ to compute antiderivatives (\eg $v_n$ given $w_n$), by
solving the
%matrix equation  AB: "matrix eqn" implies the unknonw is a matrix :)
linear system \cref{scaledDdef}. 

%In \cref{phasefun}
%\AB{better to refer forward to a Remark since Sec 2.1 is long. But maybe the comment doesn't add anything here - it's too in the weeds. Save the entire comparison of residuals until our method is described.}
%we show that this is equivalent to
%starting from the zeroth order solution $x_0 = \pm i\om$ and generating
%successive iterates $x_i(t) := x_{i-1}(t) + \delta(t)$ by setting the residual they leave on the
%right-hand-side when substituted into \cref{riccwkb} to zero, to first order in
%$\delta$. 



\subsection{Numerical demonstration of Riccati defect correction iteration}
%\AB{swtiching from presenting the method to testing it makes a needed subsec break here}

Here we demonstrate the temporary convergence of the residual by example: in the left panel of \cref{convergence-plot}, we apply
the iteration from \cref{init,iter} to the equation 
\begin{equation}\label{bursteq}
    u'' + \frac{m^2 - 1}{(1+t^2)^2}u = 0,  
\end{equation}
over the timestep $t \in [0, 2]$ starting with initial conditions $u(0) = 1/m$ and
$u'(0) = i$. For a given $m$, we show the maximum residual over the length of
the step, $\max_{t \in [t_j, t_{j+1}]}R[x_k]$, as a function
of the iteration number $k$. We vary $m$ and observe that the maximum residual
indeed drops geometrically for the first few iterations, $R[x_k] \propto
\om_{\text{max}}^{-k}$, where $\om_{\text{max}} = \sqrt{1 + m^2}$ is the
maximum value of $\om$ over the timestep. At small $\om_{\text{max}}$, the
geometric progression stops and the series starts diverging after $5$
iterations and only about $4$ digits of accuracy can be achieved, clearly
showing the asymptotic nature of the expansion. At large $\om$, however, the
convergence is exteremely rapid and machine precision is reached within about
$5$ iterations. In \cref{pracres} we show that even though the number of iterations
required to reach minimal residual grows with $\om$ as $\bigO(\om)$, in
practice a larger $\om$ required \emph{fewer} iterations to reach a given
accuracy threshold $\varepsilon$, approximately $k_{\varepsilon} \approx
\bigO(1/\log\om)$.  

\begin{figure*}[tb]
    \flushleft
    \subfloat{\includegraphics{plots/residual-k-corr.pdf}}
    %\label{residual-reduction}
    \subfloat{\includegraphics{plots/convergence.pdf}}
    \caption{\label{convergence-plot} Error analysis and convergence of our
    method. The left panel shows the maximum residual of the Riccati equation
    over a single step of the numerical method as a function of the number of
    terms added to the asymptotic expansion, which is used to propagate the
    numerical solution in the highly oscillatory regions of the ODE. The
    differently coloured curves correspond to different peak frequencies over
    the same time interval. A geometric reduction in the residual for the first
    few iterations, forecast qualitatively in \cref{phasefun} and shown formally in
    \cref{TR}, is observed. Following this geometric reduction the
    asymptotic nature of the approximation is manifest as an increase in the
    residual, especially visible at low $\omega_{\text{max}}$. The right panel
    is a convergence plot showing the achieved relative error against the
    relative tolerance set to the solver. The coloured curves correspond to
    different upper ends of the integration region. These should be compared to
    the grey dashed line showing an achieved error matching the tolerance.
    In regions where the condition number $\kappa$ of the problem may limit the achievable
    accuracy (see \cref{conditionnodef}), we plot the maximum achievable
    accuracy as a coloured dashed line to compare the achieved error with. }
   % Need to say what ODEs were used to create these  
\end{figure*}



\subsection{Matching Riccati intervals}
%\AB{short but probblay seperate subsec like this, since confusing before
%  the demo of Fig.1}

We ensure continuity of the solution and its derivative across timestep
boundaries by treating $u(t_i)$ and $u'(t_i)$ as initial conditions in the step
from $t_i$ to  $t_i+h$. Noting that the asymptotic iteration \cref{iter} can be
initialised with $x_0(t) = +i\om(t)$ as is done in \cref{init}, or with $x_0(t)
= -i\om(t)$, the resulting asymptotic solutions, $x_{\pm}(t)$, approximate the
nonlinear Riccati equation, but the associated 
\be
u_{\pm} = e^{\int_{t_i}^{t_i+h}x_{\pm}(\sigma)\mathrm{d}\sigma}
\ee
are two, linearly independent approximate solutions of the linear ODE \cref{ode}.
They may therefore be linearly combined to match any set of initial conditions,
\begin{align}
    u(t) &= A_{+}u_{+}(t) + A_{-}u_{-}(t), \qquad t \in [t_i, t_i + h],\\
    u'(t) &= A_{+}u'_{+}(t) + A_{-}u'_{-}(t), \qquad t \in [t_i, t_i + h],
\end{align}
with 
\be
\begin{bmatrix}
    1 & 1 \\
    x_{+}(t_i) & x_{-}(t_i)
\end{bmatrix}
\begin{bmatrix}
    A_{+} \\
    A_{-}
\end{bmatrix}
= 
\begin{bmatrix}
    u(t_i) \\
    u'(t_i)
\end{bmatrix}, 
\ee
where we used that $u_{\pm}(t_i) = 1$ and $u'_{\pm}(t_i) = x_{\pm}(t_i)$ by construction.


% +/- solutions complex conjugates




\subsection{Direct spectral solution on nonoscillatory intervals \label{chebysteps}}

In case the solution is nonoscillatory or the asymptotic expansion fails to
converge before the residual reaches a user-specified tolerance $\varepsilon$, we
employ a spectral method based on Chebyshev nodes instead. The motivation
behind this choice is that by adjusting the number of nodes, an arbitrarily
high order can be achieved. Although spectral methods are usually used for
solving boundary value problems whereby the nodes are laid out over the entire
integration range, there is no reason one could not apply them over one timestep at a
time in a time-stepper setting. Since the present algorithm was constructed to
switch between two methods -- one based on the asymptotic expansion of the
Riccati equation and a spectral method -- on the fly and the points of
switching are not predetermined, we opt to break the integration range up into
smaller timesteps, applying the spectral method over a single timestep at a
time.

Using the differentiation matrix from \cref{scaledD}, \cref{ode} may then be discretized over the interval $t \in [t_i, t_i+h]$ as
\begin{align}\label{discreteode}
    F_n u_n = \left(D_n^2 + 2\text{diag}(\g_n)D_n + \text{diag}(\om_n) \right)u_n = 0, 
\end{align}
where the subscript $n$ denotes a vector of function values at the $n+1$ (scaled) Chebyshev nodes $t_n$.
%\be\label{scaledt}
%t_n = \{ t_j \}, \quad t_j = t_i + \frac{h}{2} + \frac{h}{2}\tilde{t}_j, \quad j = 0, 1, \ldots, n. 
%\ee
A spectral method based on Chebyshev nodes finds $u_n$ by solving the system of
equations defined by \cref{discreteode}, subject to auxiliary conditions that
ensure the uniqueness of the solution. In our case these conditions encode
continuity across the lower timestep-boundary, \ie are the initial conditions
\be\label{discreteic}
(u_n)_n = u(t_i), \quad \left(D_nu_n\right)_0 = u'(t_i),
\ee
where a lower index outside brackets denotes a given vector element.
The initial conditions may be encoded as two rows appended to $F_n$,
\be\label{discreteodeic}
\renewcommand*{\arraystretch}{1.25}
\begin{bmatrix}
    (F_n)_{00} & \ldots & (F_n)_{0n} \\
    \vdots & \ddots & \vdots \\
    (F_n)_{n0} & \ldots & (F_n)_{nn} \\
    (D_n)_{00} & \ldots & (D_n)_{0n} \\
    1 & \ldots & 0
\end{bmatrix}
u_n =  
\begin{bmatrix}
0 \\
\vdots \\
0 \\
u'(t_i) \\
u(t_i)
\end{bmatrix},
\ee
where lower indices outside a bracket specify a given matrix element.
After solving the $(n+3) \times (n+1)$ system in \cref{discreteodeic}, the value of the solution
$u(t)$ at the end of the interval can be read off as the last element of the
solution vector $u_n$, and its derivative as the last element of $D_nu_n$.
These then serve as the initial conditions for the subsequent time-step.
Note that the system has now become rectangular but also overdetermined. The
matrix on the right-hand-side of \cref{discreteodeic} may be replaced with a
square one by removing the last two rows and incorporating the initial
conditions directly, \eg the last row can be removed by solving only for
$(u_n)_1, \ldots, (u_n)_n$ and setting $(u_n)_0 = u(t_i)$.

The error on steps taken with the spectral method above is estimated by
doubling $n$, attempting the step with the same stepsize
$h$ again, and comparing results. If the error falls below the user-specified
threshold $\varepsilon$, the spectral method is considered to have converged and the
step is accepted. If the error does not reach $\varepsilon$ by $n=n_{\text{max}}$, $h$
is halved and the iteration in $n$ begins again starting with
$n=n_{\text{min}}$. The parameters $n_{\text{min, max}}$ can be chosen by the
user, but their default values are set to $n_{\text{min}} = 16$,
$n_{\text{max}} = 64$. Computation time spent in failed steps is lost, making
it imperative that the initial stepsize estimate $h$ is chosen carefully. We
describe the procedure for choosing $h$ for both spectral and asymptotic steps
in detail in the next section.

% Error analysis? Just briefly about order of error...
% Argue why Chebyshev nodes are a good choice here, citing Tref


\subsection{Adaptive selection of interval size and type}

\subsubsection{Initial stepsize estimates}

At each timestep $t_i$, a decision needs to be made about whether to attempt a
step using the spectral method or the asymptotic exansion. The two types of
steps could be attempted simultaneously and the decision could be made based on
their estimated error (as done in \cite{agocs2020efficient}), but in order to minimise
computation time, we estimate whether the asymptotic expansion would
converge quickly enough first. Inspecting the first few iterates of the asymptotic
method from \cref{init,iter}, and noting that the convergence rate is
determined by the size of the correction term added in each iteration, a rough
upper bound on the first correction term is $\om'/\om$, giving the
approximate timescale 
\be\label{hoscini}
h_{\text{osc}} = \frac{\om(t_i)}{\om'(t_i)}.
\ee
\cref{TR} to follow justifies this initial stepsize-estimate by showing that
the ratio of bounds on $\om'$ and $\om$ appears in the rate of
convergence of successive Riccati residuals. 

A similar estimate for the spectral method should depend on two things: how
oscillatory the solution is, and what timescales the terms in \cref{ode} change
on, \ie their smoothness. If the solution is highly oscillatory, we wish to
avoid taking a step with a Chebyshev spectral method. A measure of the rate of
oscillations is simply the frequency, giving the timescale
\be\label{hsloini}
h_{\text{slo}} = \frac{1}{\om(t_i)}.
\ee

These initial stepsize estimates are too crude to attempt a step with in
practice, since over $h_{\text{osc}}$ or  $h_{\text{slo}}$, $\om$, $\om'$, and $\g$ may change significantly. 
Nonetheless they provide a useful starting point for the algorithm that computes
finer estimates.

\subsubsection{Refining the stepsize estimates}

The local error of of the asymptotic solution described in \cref{phasefun} is not
inherently dependent on the stepsize $h$ in the same way a \eg Runge--Kutta
method's is. The convergence rate of the residual of asymptotic steps depends
on the bounds on $\om$, $\om'$, and $\g$ over the course of the step as we show
in \cref{TR}, which indirectly introduces stepsize-dependence, as does the fact
that the derivatives and integral appearing in the step are computed
numerically on an $(n+1)$-point Chebyshev grid (with $n \approx 16$). Ultimately, it is how well $\om$ and $\g$ can be represented by $(n+1)$th
degree Chebyshev polynomials that determines the error in asymptotic steps, which
we therefore test directly to gain a more accurate estimate for the stepsize
starting from $h_{\text{osc}}$ as an initial estimate. Given $\om$ and $\g$ evaluated on $t_n$, 
which we denote $\om_n$ and $\g_n$, respectively, we define an error
\be
    \Delta_n[f] := \max_{j = 0, 1, \ldots, n} \left| \frac{f(t^{\star}_j) -
    f^{\star}(t^{\star}_j)}{f(t^{\star}_j)} \right|,
\ee
where $f$ is $\om$ or $\g$, 
\be
t^{\star}_n := \{t^{\star}_j\}, \quad t^{\star}_j = t_i + \frac{h}{2} + \frac{h}{2}\frac{j + \frac{1}{2}}{n\pi}, \quad j = 0, 1, \ldots, n,
\ee
and the $f^{\star}(t^{\star}_j)$ are interpolated using the $f(t_n)$ values,
\be
f^{\star}(t^{\star}_n) = L_n f(t_n).
\ee
The interpolation matrix $L_n$ is computed using a Vandermonde matrix \cite{atap}. The entries of $L_n$ are invariant under the simultaneous rescaling of
$t_n$ and $t^{\star}_n$ to lie within the standard interval $[-1, 1]$,
therefore $L_n$ need only to be computed once.
Given $\Delta_n[\om]$ and $\Delta_n[\g]$, we accept or update the stepsize as follows,
\begin{align}
    \Delta :=& \max \left(\Delta_n[\om], \; \Delta_n[\g]\right), \\
    h :=& \begin{cases}
        h &\text{if } \Delta \leq \varepsilon_h, \\
        \min \left( 0.7h,\; 0.9 h \left( \frac{\varepsilon_h}{\Delta} \right)^{\frac{1}{n+1}} \right) &\text{otherwise}.
    \end{cases}
\end{align}
In the above, the $1/(n+1)$ in the exponent is justified by noting that the
error in $n+1$-point Chebyshev interpolation is proportional to $h^(n+2)$, therefore if
$\Delta$ exceeds $\varepsilon_h$, using an exponent slightly larger than $1/(n+2)$ takes
the local error back down to a value smaller than $\varepsilon_h$, which is further
ensured by multiplying by the safety factor 0.9. We decrease the step to $0.7h$
if it yields a smaller stepsize to ensure quicker convergence if $\Delta$ is
only slightly larger than $\varepsilon_h$. The factors 0.7 and 0.9 were set based on
empirical observations. 

The local error of Chebyshev spectral steps depends on both the stepsize and
the number of nodes. Since within a step we iterate over both of these until
convergence, we only need to ensure that over our proposed stepsize the
timescale over which $u$ changes ($\approx 1/\om$) does not increase too much. Starting from $h =
h_{\text{slo}}$, we refine the stepsize proposal according to
\begin{align}
    h :=& \begin{cases}
        h/2 &\text{if } \min\limits_{j = 0, 1, \ldots, n}\frac{1}{\om(t^{\star}_j)} < \sigma h \\
        h &\text{otherwise},
    \end{cases}
\end{align}
with $\sigma = 0.8$, chosen experimentally, and $n$ being a hyperparameter with a default value of $n = 16$.

\subsubsection{Choosing the steptype}

With a refined $h_{\text{osc}}$ and $h_{\text{slo}}$ in hand, we can make an informed decision about which type of step to take.
\begin{pro}\label{steptypechoose}
    Let $h_{\text{osc}}$ and $h_{\text{slo}}$ be the refined stepsize proposals
    for a step to be taken from $t = t_i$ using the asymptotic expansion and the Chebyshev
    spectral method, respectively. The algorithm chooses to attempt an
    asymptotic step of size $h_{\text{osc}}$ if and only if
$$ h_{\text{osc}} > 5h_{\text{slo}} \quad \text{and} \quad \omega(t_i) h_{\text{osc}} > 2\pi, $$
    otherwise it will attempt a spectral step of size $h_{\text{slo}}$.
\end{pro}
The reason behind requiring the proposed stepsize for an asymptotic step not to
simply be larger than that of a Chebyshev spectral step is that in regions
where the two stepsizes compete, the asymptotic method rarely converges before
the residual reaches $\varepsilon$, either because $\om$ is not large enough, or
either of $\g$ and $\om'$ is too large. The prefactor of $5$ was set based on numerical experiments. 

If an asymptotic step was decided to be attempted, the algorithm will start
iterating over $x_j$ according to \cref{iter}, monitoring the residual
$R[x_j](t_n)$ throughout. If $\max R[x_j](t_n) < \varepsilon$ \emph{or} $\max
R[x_j](t_n) \geq \max R[x_{j-1}](t_n)$, the iteration is stopped. In the former
case, the proposed solution $u(t_i+h)$ is accepted and the independent variable
is incremented, $t := t_i + h$. Otherwise, the step is re-attempted with size
$h = h_{\text{slo}}$ and using the spectral method. 

If either of the conditions in \cref{steptypechoose} were not met, a spectral
step is attempted with $h = h_{\text{slo}}$. The local error of the step is
then estimated and the number of nodes, as well as the stepsize, is
adapted until a local error value of at most $\varepsilon$ is reached, as described in
\cref{chebysteps}.

A flowchart showing how estimating the stepsizes and choosing the steptype ties
together is shown in the left panel of \cref{balls-flowchart}.


% EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
\section{Error analysis for the Riccati iteration\label{errorana}}

From now on we abbreviate the residual function by $R_j = R_j(t) := R[x_j](t)$.
\cref{PRiter} described the evolution of $R_j$
under the proposed iteration, which is local in $t$.
For smooth $x_j$ of size $\bigO(\om)$, and smooth $R_j$,
it suggests geometric reduction in $R_j$ by a factor $\bigO(\om)$ per iteration;
however, this is misleading in general because of
growth of high-order $t$-derivatives.
The series will turn out to be merely asymptotic in the large parameter $\om$.
We now make this argument rigorous, for $\om$ analytic
and sufficiently large throughout some neighborhood of $t$.

%One might hope that, given $x_j'$ and $x_j$ of order $\om$,
%and a lower bound on $x_j$ of the same order,
%then $R_j$ is reduced at each point by a factor $\bigO(\om)$.
%then the last term is negligible
%then if $R_j$ and $R_j'$ are sufficiently small


\begin{thm}\label{TR}
  Fix $t\in\R$, and let the frequency function $\om$ be analytic
  in the closed ball $B_\rho(t) := \{z\in\C : |z-t| \le \rho\}$,
  for some $\rho>0$, with bounds
    \begin{alignat}{4}
        \eta_1 &\leq |\om&&(z)| &&\leq \eta_2, \qquad &&\forall z\in B_\rho(t), \label{ommag} \\
        |\om'(z)| &\leq &&\eta_3 &&\leq \frac{\eta_1^2}{17}, &&\forall z\in B_\rho(t), \label{omder} \\
        |\g(z)| &\leq &&\eta_4 &&\leq \frac{\eta_1^2}{34\eta_2}, \qquad &&\forall z\in B_\rho(t), \label{gammaupper}
  \end{alignat}
  that is, its derivative should be sufficiently small.
  Let $k\in\{0,1,\dots\}$ be sufficiently small such that
  \be
    r := \frac{1}{2\te_1 \rho} \left(1 + \frac{\te_2}{\te_1}\right) k + \frac{\te_3}{4\te_1^2} \leq \frac{3}{4},
  \label{r}
  \ee
  where
  % *** note attempt at new consts:
  \begin{align}
    \te_3 &:= \eta_3 + 2\eta_2\eta_4, \label{eta3}
    \\
    \te_1 &:= \eta_1 - \eta_4 - \frac{17 \te_3}{4 \eta_1},  \label{eta1}
    \\ 
    \te_2 &:= \eta_2 + \eta_4 + \frac{17 \te_3}{4 \eta_1}. \label{eta2}
  \end{align}
  Then after any number $j\le k$ of iterations of \cref{init}--\cref{iter},
  the function $x_j$ has Riccati residual \cref{R} bounded at the point $t$ by
  \be
  |R_j(t)| \le \te_3 r^j.
  \label{Rjbnd}
  \ee
\end{thm}
%
\begin{figure*}[tb]
    \centering
    \subfloat{\begin{adjustbox}{width=\width, valign=c}\input{plots/flowchart.tikz}\end{adjustbox}}
    \hfill
    \subfloat{\includegraphics[width=0.5\textwidth, valign=c]{plots/complexsetup.pdf}}
    \caption{\label{balls-flowchart} Left: Visualization of the nested set of
    closed balls in the complex plane for the proof of \cref{TR}, centered on
    $t = t_j$. Right: Flowchart showing stepsize refinement and the logic
    behind choosing the steptype in our proposed algorithm for a step from
    $t_j$ of size $h$.}
\end{figure*}
%
This shows, for $\om$ of sufficiently small derivative
relative to its magnitude,
temporary geometric convergence up to $k$ iterations,
but at a rate $r$ that deteriorates with $k$.
And for any $k$ to exist satisfying the condition on $r$, $\om$ must have
a sufficiently large lower bound $\eta_1$, i.e., $t$
must be in a sufficiently oscillatory region for the original ODE.

\begin{rmk}\label{slight}
    By \cref{omder}, $\te_1 \ge 8\eta_1/17$ and $\te_2 \le \eta_2 + 9\eta_1/17$,
  thus $[\te_1,\te_2]$ is only a slight
  expansion of the interval $[\eta_1,\eta_2]$ containing the range of
    $|\om|$ in the ball.
  Applying this we see that the last term in \cref{r}
    is no more than $17/128 \leq 0.14$, thus can cause only slight deterioration in the rate.
\end{rmk}

\begin{rmk}
    In the limit when $\om$ tends to be constant in the ball ($\eta_3$ small) and the damping term is small ($\eta_4$ small),
  then $r$ tends to $k/\om \rho$, where we can interpret $\om\rho$ as $2\pi$
    time the number of oscillation periods across the ball radius.
    Then, for example, with a ball of radius of only 5 periods, $r$ satisfies \cref{r}
    for $k\le 25$.
\end{rmk}

\begin{proof}
  Define the concentric nested set of closed balls $B_j := B_{\rho_j}(t)$,
  with radii $\rho_j := (1-j/k)\rho$, $j=0,1,\dots,k$. Note that
  $B_0$ is the original ball in the statement of the theorem, while
    $B_k = \{t\}$ is the single point of interest. This setup is shown in the
    right panel of \cref{balls-flowchart}.
  For any function $f$ analytic in $B_j$ we abbreviate the $\infty$-norm by
  $\|f\|_j := \max_{z \in B_j}|f(z)|$.
  We will need a bound for $f'$ in $B_{j+1}$ in terms of $\|f\|_j$ by
  applying Cauchy's theorem for derivatives
  \cite{steinshakarchi},
  \be
  f'(z) = \frac{1}{2\pi i} \int_{|\zeta-t|=\rho_j} \frac{f(\zeta)\, d\zeta}{(\zeta-z)^2}
  ~, \qquad z \in B_{j+1}~.
  \label{cauder}
  \ee
  Bounding the integrand, then using the cosine rule we get
  $$
  |f'(z)| \;\le \;
  \frac{\|f\|_j}{2\pi} \int_{|\zeta-t| = \rho_j} \frac{|d\zeta|}{|\zeta-z|^2}
  =
  \frac{\|f\|_j}{2\pi} \int_0^{2\pi} \frac{\rho_j\, d\theta}{|z|^2 + \rho_j^2 - 2|z|\rho_j \cos \theta}~.
  $$
  We now use $\int_{0}^{2\pi} d\theta /(a + b \cos \theta) = 2\pi/\sqrt{a^2-b^2}$
  for $b<a$ \cite[Eq.~3.613.1]{GR8}, with
  $a = |z|^2 + \rho_j^2$ and $b = -2|z|\rho_j$, so that
  $\sqrt{a^2-b^2} = \rho_j^2-|z|^2$.
  Noting that the case $|z| = \rho_{j+1}$ bounds the others,
  and using $\rho_j=(1-j/k)\rho$, we compute, for any $0\le j < k$,
  \be
  \|f'\|_{j+1} \le
  \frac{\|f\|_j}{2\pi} \frac{2\pi\rho_j}{\rho_j^2-\rho_{j+1}^2}
  =
  \frac{\|f\|_j}{2 \rho} \frac{k(k-j)}{2k-2j-1}
  =
  \frac{\|f\|_j}{2 \rho} k \left[ \frac{1}{2} + \frac{1}{2(2k-2j-1)}\right]
  \le
  \frac{k}{\rho}\|f\|_j~.
  %~, \; 0\le j< k
  \label{derbnd}
  \ee
  Note that this bound is a factor $\bigO(k)$ better than naively
  lower bounding the denominator in \cref{cauder}.
  
  %The 
  We now use induction in iteration number $j$.
%  and consider the iteration acting on the functions
    We take as the induction hypothesis that $x_\ell$ (and thus $R_\ell$)
  is analytic in $B_j$, for all $0\le \ell \le j$, with
  \begin{align}
      \te_1 \leq |x_\ell + \g| &\leq \te_2
  \qquad \mbox{ in } B_j, \quad \mbox{ for all } \ell = 0,1,\dots,j,
  \label{hypx} \\
      |R_\ell| &\leq \te_3 r^\ell \quad \mbox{ in } B_j, \quad \mbox{ for all } \ell = 0,1,\dots,j,
  \label{hypR}
  \end{align}
  where $r$ is defined by \cref{r}.
  Assuming for now the hypothesis for $j$, we apply simple bounds to the
  residual iteration \cref{Riter},
  lower-bounding denominator magnitudes and upper-bounding numerators
  via \cref{hypx},
  and applying \cref{derbnd} to the two derivative terms, to get
  $$
  \|R_{j+1}\|_{j+1}
  \leq
  \frac{1}{2\te_1}\left(
  \frac{1}{\te_1} \frac{k}{\rho}\te_2 + \frac{k}{\rho}
  \right)\|R_j\|_j
  +\biggl(\frac{\|R_j\|_j}{2\te_1}\biggr)^2
  \leq
  r \cdot \|R_j\|_j
  \leq
  \te_3 r^{j+1},
  $$
where in the middle step we used the crude bound $\|R_j\| \le \te_3$
following from \cref{hypR} and that $r\le1$, and then the definition of $r$ in \cref{r}.
%This handles the case $\ell=j+1$.
The lower cases $\ell\le j$ follow trivially from the hypothesis since the balls
are nested.
Thus \cref{hypR} is proven for $j+1$.

It remains to verify that \cref{hypx} also holds for $j+1$.
By the functional iteration \cref{init}--\cref{iter},
$$
x_{j+1}(z) + \g(z) = i\om(z) + \g(z) - \sum_{\ell=0}^{j} \frac{R_\ell(z)}{2 \left(x_\ell(z) + \g(z)\right)},
\qquad z\in B_{j+1}.
$$
Using the hypothesis, the sum is pointwise bounded in magnitude
in $B_{j+1}$ by
$$
\left|\sum_{\ell=0}^{j} \frac{R_\ell}{2\left( x_\ell + \g \right)} \right|
\leq \frac{\te_3}{2\te_1} \cdot \sum_{\ell=0}^j r^{\ell}
%\;\le\; \frac{\eta_3}{2\te_1} \frac{1}{1-r}
\leq \frac{\te_3}{2\te_1} \cdot 4,
%\leq \frac{95\eta_3}{198\eta_1}
$$

where the upper bound in \cref{r} was used to bound the geometric series.
Using \cref{eta1,eta3} for $\te_1$ and $\te_3$ we write this upper bound as
$$
\left|\sum_{\ell=0}^{j} \frac{R_\ell}{2\left( x_\ell + \g \right)} \right|
\leq
2\frac{\eta_3 + 2\eta_2\eta_4}{\eta_1 - \eta_4 - \frac{17\eta_3}{4\eta_1} - \frac{17\eta_2\eta_4}{2\eta_1}},
$$
then extend it by lower-bounding the denominator via \cref{omder} and the crude bound ${\eta_4 \leq \eta_1/34}$ from \cref{gammaupper}. This gives
$$
\left|\sum_{\ell=0}^{j} \frac{R_\ell}{2\left( x_\ell + \g \right)} \right| 
\leq
\frac{17\eta_3}{4\eta_1} + \frac{17\eta_2\eta_4}{2\eta_1},
$$
which, when combined with $|\om| \leq \eta_2$ and $|\g| \leq \eta_4$ yields
$$\|x_{j+1} + \g\|_{j+1} \le \eta_2 + \eta_4 + \frac{17\eta_3}{4\eta_1} + \frac{17\eta_2\eta_4}{2\eta_1} = \te_2,$$
verifying the upper bound \cref{hypx} for $j+1$.
Instead combining with $|\om| \ge \eta_1$ gives
$$ |x_{j+1}| \ge \eta_1 - \eta_4 - \frac{17\eta_3}{4\eta_1} - \frac{17\eta_2\eta_4}{2\eta_1} = \te_1 $$
in $B_{j+1}$, which verifies the lower bound \cref{hypx}.
Again, the hypothesis is inherited for $\ell\le j$ by the nesting of the balls.

Finally, the base case $j=0$ for induction
satisfies \cref{hypx}--\cref{hypR}
by the conditions of the theorem,
since $x_0(t) = i\om(t)$, noting $[\eta_1,\eta_2] \subset [\te_1,\te_2]$,
while $R_0(t) = i\left(\om'(t) + 2\g\om \right)$ is bounded in magnitude by $\te_3$.
\end{proof}

In the above theorem, the choice of $4/5$ in \cref{r} was merely a convenient one, chosen so that the bounds $[\te_1,\te_2]$ on $|x_j|$
involving the geometric sum were
not much wider than the bounds $[\eta_1,\eta_2]$ on $|\om|$.
%It is possible to write simpler but less tight formulae for $r$.


\subsection{Practical aspects of residual reduction \label{pracres}}

The fact that, as $k$ grows,
the rate $r$ in \cref{TR} deteriorates
we believe to be an inevitable consequence of
the series being asymptotic in $1/\om$ but not convergent.
%We support this in numerical results in ***.

However, given a function $\om(t)$ uniformly large enough
in a ball,
by stopping at a roughly optimal $k$ (an idea
called ``superasymptotics'' \cite{berrysuper,boydsuper})
one can achieve
exponential convergence with respect to the size of the frequency $\om$,
as follows.

\begin{cor}[Superasymptotic approximation]\label{super}
  Suppose $\om$ and $\g$ satisfy the conditions \cref{ommag,omder,gammaupper}
  of \cref{TR}
  about a point $t\in\R$,
  and let $\alpha := (1+\te_2/\te_1)/2\te_1\rho$ be the
  first term in the rate \cref{r} arising from these bounds.
  Then there is a $k\in\{0,1,\dots\}$ such that
  $$
  |R_k(t)| \; \le \; e \te_3 e^{-1/5\alpha}~.
  $$
\end{cor}
\begin{proof}
  Set $k$ to be the integer in the interval $[1/5\alpha-1, \, 1/5\alpha)$.
    Then since the final term in \cref{r} is bounded by $17/128 \leq 0.14$
    by \cref{slight}, and the
    first term $k\alpha \le 1/5$, we get $r\le e^{-1}$, which
    is also $\le 3/4$ so that \cref{Rjbnd} holds.
    Choosing $j=k$, and inserting a lower bound on $k$,
    $|R_k(t)| \le \te_3 r^k \le \te_3 e^{-(1/5\alpha-1)}$, which
    finishes the proof.
  \end{proof}
Recalling that $\alpha = \bigO(1/\om)$, this
shows that an $\om$-dependent
number of iterations can give, in exact arithmetic, an
exponentially convergent pointwise residual,
$$
|R(t)| \;=\; \bigO(e^{-c\om})
$$
for some constant $c>0$.
In the limit where
$\om$ tends to constant in the ball of radius $\rho$ about $t$, then
$\alpha$ tends to $1/\om\rho$, so the above
constant is roughly $c\approx \rho/3$.
Equivalently, roughly one decimal digit is achieved per
$1.1$ periods of oscillation across the ball radius.
% log(10)*3/(2*pi) = 1.099

However, this optimal number of iterations grows as $k = \bigO(\om)$,
so for large $\om$ is impractical (and unnecessary).
In practice we use an adaptive stopping criterion for $k$ based
on user-defined tolerance.
By returning to \cref{TR} and dropping the small second term
in \cref{r} we get, in the limit of constant $\om$ in the ball,
$$
|R_k(t)| \; \approx \; \te_3 \left( \frac{k}{\rho\om}\right)^k~.
$$
This shows \textit{nearly} geometric convergence in $k$,
sufficient to reach machine accuracy efficiently with a few iterations
when $\om$ is large.
For large $\rho\om$ one may approximately invert this to predict the iteration number $k$ sufficient for $|R_k| \approx \varepsilon$, to get
$$
k_\varepsilon \approx \frac{\log \varepsilon^{-1}}{\log \rho\om}~.
$$
%*** compare to numerics.

Finally, note that a stopping criterion for $k$ based on the residual $R[x_k]$ of the Riccati equation is viable because the residual is directly related to that of the original ODE.
\begin{cor}[Relationship of residuals]\label{residualu}
    Let $R[x_k](t)$ be the residual of the Riccati equation as defined in
    \cref{R} after $k$ iterations of \cref{init,iter}. Then if
    $\tilde{R}[u_k](t)$ is the associated residual of the original ODE
    \cref{ode}, defined as
    \be\label{Rode}
    \tilde{R}[u_k](t) := u''_k + 2\gamma(t)u'_k + \omega^2(t),
    \ee
    and $u_k = e^{\int^t x_k(\sigma)\mathrm{d}\sigma}$, then
    \be
    \tilde{R}[u_k] = u_k R[x_k].
    \ee
\end{cor}
\begin{proof}
    The proof follows straightforwardly from substitution of $u_k(x_k)$ into \cref{Rode}.
\end{proof}


\section{Numerical results \label{numresults}}

Before discussing the results of numerical experiments, we define a few useful quantities. 

We measure the achieved accuracy of a method by comparing the numerical
solution to the analytic (or a reliable reference) solution.
\begin{defn}[Absolute and relative error]\label{deltau}
    If an ODE of the form \cref{ode} has analytic (or otherwise reference) solution $u(t)$ and 
    numerical solution $\tilde{u}(t)$, then its absolute error at a given time $t$ is
    \be
    \Delta u := \left| \tilde{u}(t) - u(t)\right|,
    \ee
    and its relative error is defined as
    \be
    \left| \Delta u/u \right| := \left| \frac{\tilde{u}(t) - u(t)}{u(t)}\right|.
    \ee
\end{defn}

The achieved accuracy of a method is limited by the condition number of the
problem $\kappa$. The condition number measures the sensitivity of a given system to
outside perturbations, which may manifest as uncertainty in any of the inputs.
The condition number has been defined for functions of a single or multiple
variables, for matrices or linear equations written in matrix form (see, \eg
\cite{rice1966,trefethenlinalg}), and several other problems. We approximate
the condition number of an oscillatory ODE as the cumulative number of periods of
oscillation the solution of the ODE has traversed.
\begin{defn}[Condition number of an oscillatory ODE]\label{conditionnodef}
    Since the condition number $\kappa$ of an oscillatory function $f(t) =
    c_0\exp(i\int^t \om(\sigma)\mathrm{d}\sigma)$, where $c_0$ is a constant,
    is
    \begin{equation}\label{conditionno}
        \kappa_f(t) = \left| \frac{tf'(t)}{f(t)} \right| = i\om(t)t,
    \end{equation}
    let the condition number of an ODE of the form \cref{ode} be defined as the total number of oscillations the solution $u$ has passed through over the solution interval, 
    \begin{equation}
        \kappa := \frac{1}{2\pi}\Im\left(\log\frac{u(t_1)}{|u(t_1)|}  - \log\frac{u(t_0)}{|u(t_0)|}\right).
    \end{equation}
    In practice, we will often approximate $\kappa$ by using the numerical
    solution $\tilde{u}$ instead of the analytic solution $u$, since the latter may not
    exist or be available.
\end{defn}

\begin{cor}[Maximum achievable accuracy]\label{mineps}
In the case of an ODE being solved numerically, the smallest possible
perturbation to the input data of the system is the (finite) precision with
which numbers are represented, which is $\varepsilon_{\mathrm{mach}}$, machine
precision. The best achievable accuracy with any given numerical method on an
IVP with condition number $\kappa$ is 
\be
\varepsilon \geq \kappa \cdot \varepsilon_{\mathrm{mach}}.
\ee
\end{cor}

\subsection{The Airy equation}

A simple but effective test for highly oscillatory solvers is the Airy ODE,
\be
u'' + tu = 0, \quad t \in [1, 10^8], 
\ee
with initial conditions
\be
u(1) = \text{Ai}(-1) + i\text{Bi}(-1), \quad u'(1) = - \text{Ai}'(-1) -i\text{Bi}'(-1).
\ee
where Ai, Bi are the Airy functions \cite[Chapter~9]{DLMFairy}
Its unique solution is
\be
u(t) = Ai(-t) + iBi(-t).
\ee
%\AB{Good aim to explain why a good test here. I think we need to refine the
%  points though}
It serves as a good test case because not only is it hard
%\AB{surely any osc soln is equally bad for std methods?}
 for standard numerical methods due to the frequency $\om(t)$ growing with time, but should prove to
be easy for specialised oscillatory methods since the change in $\om(t)$ per wavelength becomes arbitrarily small, making an asymptotic approximation (such as WKB) 
%-- the zeroth order solution --
an increasingly good local appoximation.
%\AB{We're dancing between a precise and merely intuitive claim here.
%  Maybe rather than state a formula that is not really right since $\om(t)$
%  changes, why not just state in words that the relative
%  change in $\omega(t)$ per wavelength becomes arbitrarily small as $t$ grows?
%  ``Zero order'' sounds precise but I don't think is since what's the expansion, WKB?}
With this test
case one can check whether the method adapts its stepsize correctly and
achieves the maximum possible accuracy, defined in \cref{mineps}.
%\AB{that last point applies to any osc ode, right?}


%\AB{new para since presenting results now}
\cref{airy-results} shows the
internal steps our numerical solver takes while solving the Airy equation,
colour-coded by step type, as well as the progression of stepsizes, numerical
error, and number of periods of oscillations traversed in a single step. The
stepsize $h$ is expected to reflect the timescale over which $\om$ changes,
therefore $h \propto \om/\om' \propto t$, which is observed. The numerical
error traces the theoretical minimum, $\kappa \cdot \varepsilon_{\text{mach}}$, to
within a digit. The number of oscillations traversed per step can be derived
from $h(t)$: $n_{\text{osc}} \propto \om h \propto t^{3/2}$, which again is
clearly seen in \cref{airy-results}.

An essential standard measure of robustness for any numerical method is convergence, a plot of accuracy achieved against accuracy demanded. The right panel of \cref{convergence-plot} demonstrates convergence of our method on the Airy equation for different solution interval lengths, achieved by varying $t_1$.

\begin{figure}[tb]
    \centering
    \includegraphics{plots/airy-numsol.pdf}
    \caption{\label{airy-results} Numerical solution of the Airy equation. The
    top left panel shows the real part of the analytic solution in black, on
    top of which the individual timesteps of the solver are plotted, coloured
    by their step type. The top right panel plots the stepsize as a function of
    time, which exhibits linear growth. The bottom left shows the relative
    error achieved by the solver in black, with two dashed lines to guide the
    eye: the tolerance requirement we set to the solver in grey, and the
    maximum achievable accuracy (calculated as the product of the condition
    number~$\kappa$, defined in \cref{conditionnodef}, and machine precision~$\varepsilon_{\text{mach}}$) in blue. On the
    bottom right, we show the number of wavelengths of oscillation traversed in
    a single step, as a function of time. 
%    This is expected to be roughly proportional to the frequency times timescale over which $\om$ changes, which is $\om^2/\om' = t^{3/2}$, plotted in dashed blue. 
    %, $u'' + tu = 0$, on $t \in [1, 10^8]$, with initial conditions $u(1) = \text{Ai}(-1) + i\text{Bi}(-1)$, $u'(1) = - \text{Ai}'(-1) -i\text{Bi}'(-1)$.   
    }
\end{figure}


\subsection{Comparison with standard and state-of-the-art oscillatory solvers \label{solvercomp}}

As reviewed in the introduction, a number of numerical methods have been
created specifically for highly oscillatory ODEs. We compare the performance of
the present solver with theirs in this section. For reference, a short
description of the available numerical solvers is found below, along with the
names by which they will be referred to hereafter. The method we propose will
be referred to as RDC, short for Riccati defect correction.
%\AB{maybe we should give ``this work'' a name: Riccati defect correciton (RDC) or something}
%
\begin{description}
\item[The %\AB{Kummer?}
  phase function method]{is an arbitrarily high-order solver
        described by Bremer \cite{bremer2018}, specifically created for highly
        oscillatory problems, operating near machine precision. Its main
        drawbacks are the lack of generalisability to include a friction term
        and to operate at low frequencies or non-oscillatory regions of the
        solution. Bremer provides a \texttt{Fortran 90} implementation.
        % note the "method" is not specific to F90 !
        }
\item[oscode]   % sorry gives tex errs for me, will work on
  %\texttt{\textbf{oscode}}
{is a low-order numerical solver
        \cite{agocs2020efficient,agocs2020dense} that can accommodate a
        friction term and nonoscillatory regions in the solution. It requires
        only requires the user to specify $\om$ and $\g$, which can be given as
        either closed-form functions or as timeseries. The solver has both a
        \texttt{Python} and \texttt{C++} interface, but for the experiments below it is called in
        \texttt{Python}.}
    \item[The WKB marching method]{\cite{arnold2011wkb,korner2022wkb} uses the
        mechanism described in \cite{agocs2020efficient} to switch between an a
        WKB-inspired method in regions of rapid oscillation and Runge--Kutta
        method. Its convergence order in the oscillatory regions is one higher
        than that of \texttt{oscode}, but does not (easily) generalise to
        include a $\g$ term. It further requires the user to specify the
        derivatives of $\om$ by hand, up to $\om^{(5)}$. This recently
        developed method is implemented in \texttt{MATLAB}.
        %\AB{Emphasize it's recent?}
    }
\end{description}
%
Since solvers not created specifically for oscillatory problems typically have
a runtime proportional to $\om$\footnote{This would mean unacceptably long
runtimes for the problems presented in this section.} and the phase function
method is the only existing 
arbitrarily high-order specialised method with an open-source implementation
available\footnote{From
\url{https://github.com/JamesCBremerJr/Phase-functions}.}, we shall use its
output as the reference solution for the highly oscillatory problem
investigated in this section. For the sake of comparison, we take the example
ODE defined in Eq. (237) of \cite{bremer2018},
\be\label{bremer237eq}
u'' + \lambda^2 q(t) = 0, \quad q(t) = 1 - t^2\cos(3t)
\ee
on $t \in [-1, 1]$, subject to the initial conditions
\be\label{bremer237ic}
u(-1) = 0, \quad u'(-1) = \lambda,
\ee
where we vary $\lambda$ between $10^1$ and $10^7$.

Performance statistics of the phase function method on this problem is presented in
Table 1 of \cite{bremer2018}. The maximum error quoted therein was computed
with respect to a reference solution obtained by a spectral deferred correction
(SDC) method \cite{dutt2000}. As noted in \cite{bremer2018}, the phase function
computed by the method is accurate to machine precision, and the only source of
error at large $\lambda$ stems from roundoff error during the evaluation of
trigonometric functions with large arguments. The growth of the condition number $\kappa$ with the frequency of oscillations $\lambda$ for a fixed solution interval causes the phase function method to
be accurate to $13$ digits at $\lambda = 10^1$, but only $8$  at $\lambda =
10^7$. This is important to keep in mind as our reference solution is generated
by phase function method, meaning that the numerical errors presented in Table
1 of \cite{bremer2018} serve as \emph{upper bounds} for any lower numerical
errors reported here. In other words, we can only tell if the present method is
as accurate as the phase function method, not more. We consider this acceptable
given that the numerical accuracy is limited by the high condition number of
the problem at hand, and that the present method is likewise limited by
the accuracy of evaluating trigonometric functions with large arguments.

\cref{bremer237tab} presents the runtime statistics of the present method,
\texttt{oscode}, and the WKB marching method, respectively, separated by double
lines. For \texttt{oscode}, the table contains two separate entries that have
been generated with the relative tolerance set to $10^{-6}$ and $10^{-12}$,
respectively. This is reflected in the achieved error in the low-$\lambda$
limit. For all other solvers, we set the relative tolerance to $10^{-12}$, and
in all cases we use double precision. For our solver, we set the number of
Chebyshev nodes in Riccati (oscillatory) steps (see \cref{chebysteps}) to $n = 40$, the
tolerance for stepsize selection to $\varepsilon_h = 10^{-13}$, and the number
of Chebyshev nodes in the spectral (nonoscillatory) steps to the default
$n_{\text{min}} = 16$, $n_{\text{max}} = 64$. For \texttt{oscode} and the WKB
marching method none of the parameters were modified from their default values
with the exception of the local tolerance.
% Slightly imprecise as the number of nodes is (n+1)

The quantities tabulated are described in detail below.
\begin{description}
    \item[$\bm{\mathrm{max}|\Delta u/u|}$:]{Maximum relative error over the
        integration range, evaluated at the timesteps taken internally by the
        solver.}
    \item[$\bm{t_{\mathrm{solve}}}$:]{Total runtime, in seconds, of a single
        ODE solve averaged over $10^3$--$10^5$ runs. This is equivalent to the
        ``phase function construction time'' reported in Table 1 of
        \cite{bremer2018}. Note that this alone is not a good basis for
        comparison for the methods presented, since even though the ODE solves
        were timed on the same machine, the methods have been implemented in
        different programming languages.}
%    \item[$\bm{t_{\mathrm{eval}}}$:]{Time it takes for the program to evaluate
%        the solution at an arbitrary point other than the internal timesteps.
%        This time should be compared to the ``average phase function evaluation
%        time'' of Table 1 \cite{bremer2018}, but as all runtimes, should not be
%        used for the sake of comparison on its own.}
    \item[$\bm{n_{\mathrm{s,osc}}}$:]{Number of steps taken by the solver with
        a specialised method in the oscillatory regime. This quantity is only
        applicable for the present solver, \texttt{oscode}, and the WKB
        marching method, and means a different  method in each case. For the
        present solver and \texttt{oscode}, it denotes steps taken with a
        ``direct'' asymptotic method (that described in \cref{phasefun} and WKB,
        respectively), and the WKB-inspired stepping scheme of
        \cite{korner2022wkb} in the case of the WKB marching method. When this
        quantity appears as $(n_1, n_2)$, $n_1$ denotes the number of attempted
        steps of the given type, out of which $n_2$ have been successful
        (accepted). The same convention applies to $n_{\text{s,slo}}$ and
        $n_{\text{s,tot}}$.}
    \item[$\bm{n_{\mathrm{s,slo}}}$:]{Number of steps taken by the solver with
        a ``standard'' method, in the nonoscillatory regions of the solution.
        For the present solver, this covers the adaptive-order spectral method
        based on Chebyshev nodes, but means a fixed-order Runge--Kutta method
        in all other cases.}
    \item[$\bm{n_{\mathrm{s,tot}}}$:]{Total number of steps performed by the
        solver, sum of $n_{\mathrm{s,osc}}$ and $n_{\mathrm{s,slo}}$.}
    \item[$\bm{n_f}$:]{Total number of function evaluations during a single ODE solve.}
    \item[$\bm{n_{\mathrm{LS}}}$:]{Denotes the total number of linear solve
        operations on a system of equations (\ie $A\vec{x} = \vec{b}$)
        performed by the solver.}
    \item[$\bm{n_{\mathrm{LU}}}$:]{Counts the number of LU decompositions
        performed by the solver. }
    \item[$\bm{n_{\mathrm{sub}}}$:]{Number of backsubstitutions (solving a
        system of equations in lower- or upper-triangular form) performed by
        the solver.}
\end{description}
%
\begin{table*}[tb]
    \renewcommand{\arraystretch}{1.2}
    \resizebox{\textwidth}{!}{\input{tables/bremer237_oscmethods_masked}}
    \caption{\label{bremer237tab} Accuracy, runtime and evaluation statistics of the algorithms
    considered (the present method, the phase function method, \texttt{oscode},
    and the WKB marching method) when applied to \cref{bremer237eq}. The column
    headers of this table, together with the various settings of the solvers
    are described in the text.}
\end{table*}
%
\cref{bremer237tab} shows clearly the extremely quick convergence of the asymptotic method within our solver
at sufficiently large frequencies: at $\lambda \geq 10^2$, the number of
function evaluations and steps become constant, causing the runtime to be
constant as well. It achieves the same accuracy as the phase function method in
this regime, but in $\approx 1/10$ time. Towards $\lambda = 10^7$, only
$\approx 2$ terms are required for the asymptotic expansion to achieve an
estimated (local) relative error of $10^{-12}$. 
\texttt{oscode} and the WKB
marching method also achieve fast convergence at sufficiently large
frequencies, but due to the fact that neither are adaptive in the order of the
asymptotic expansion (\ie the number of terms in the expansion) this
happens later, at around $\lambda \geq 10^4$. Potentially due to the
stepsize-update algorithm used by \texttt{oscode}, its number of steps,
function evaluations, and therefore runtime, grows slowly rather than staying
constant as the frequency increases. It is also worth noting that at
sufficiently high frequencies, $\lambda \geq 10^4$, setting a tolerance of
$\varepsilon = 10^{-6}$ in \texttt{oscode} results in the same accuracy as
setting $\varepsilon = 10^{-12}$ but at fewer function evaluations and steps,
again due to the WKB expansion being highly accurate in this regime and to
\texttt{oscode} potentially overstimating its local error. Finally, it is worth
noting that the WKB marching method needs fewer function evaluations and has a
slightly shorter runtime (although achieves fewer digits of accuracy) at high
frequencies because it asks the user to provide high-order derivatives of the
frequency $\om(t)$, therefore it need not use further evaluations of $\om$ to
compute said derivatives numerically. Three methods (the present solver, the
phase function method, and \texttt{oscode}) offer a dense output option, \ie
can perform interpolation and compute the numerical solution in-between
internal steps, and do so by interpolating a slowly-varying phase function,
therefore they all have evaluation times of $\mathcal{O}(10^{-6})$ \si{\s}.  

\begin{figure}[tb]
    \centering
    \includegraphics{plots/bremer237-timing.pdf}
    \caption{\label{bremer237-timing} Performance comparison of our solver (RDC) and
    other state-of-the-art algorithms on \cref{bremer237eq}. The left panel
    shows the total runtime of the solvers listed in \cref{solvercomp} as the
    frequency parameter $\lambda$ is varied, for two different relative
    tolerance settings: runtimes with $\varepsilon = 10^{-12}$ are plotted with
    solid, and $\varepsilon = 10^{-6}$ with dashed lines. The right panel shows
    the corresponding relative error the solvers achieved at the end of the
    integration interval. The grey shaded region in this plot serves as an
    upper bound on any errors that fall within it, since the errors plotted
    here were computed using the phase function method as a reference, which is
    only accurate up to the upper edge of the grey region.}
\end{figure}




\subsection{Evaluating special function evaluation: Legendre's equation}

Being able to solve highly oscillatory ODEs accurately means the solver can
quickly evaluate high-order special functions that obey an ODE of the required
form, such as Legendre's differential equation,
\be\label{legendreode}
(1-t^2)u'' - 2tu' + n(n+1)u = 0, \quad t \in [0.1, 0.9]. 
\ee
The problem need not be phrased as an initial value problem: due to the
linearity of the ODE, two arbitrary linearly independent solutions can be
linearly combined to satisfy initial or boundary conditions. In this example,
the results of which are shown in \cref{legendre-results}, we set tolerances of
$\varepsilon = 10^{-12}$, $\varepsilon_h = 10^{-13}$ and the number of
Chebyshev nodes in oscillatory steps to $n = 16$. 
% Imprecise again, number of nodes is (n+1)
These results again demonstrate the frequency-independence of the runtime of
our algorithm and show that the required tolerance (where the condition number
of the problem allows) is achieved. Note that after the solution of the ODE has
taken place, evaluating $u$ at intermediate points (\ie not at the internal
timesteps taken by the solver) is possible via interpolation of the phase
function $x$ in $\bigO(1)$ time, independently of the order $n$ in \cref{legendreode}.


\begin{table*}
    \renewcommand{\arraystretch}{1.2}
    \resizebox{\textwidth}{!}{\input{tables/legendre-fastest-morestuff}}
    \caption{Accuracy, runtime and evaluation statistics of the present algorithm when
    applied to Legendre's differential equation, \cref{legendreode}. The column
    headers are identical to those in \cref{bremer237tab} and are explained in
    the text. \label{legendre-results}}
\end{table*}


%\subsection{An example from cosmology \label{cosmo-example-sec}}
%
%%One might be tempted to think that considering the damping-free form ($\g(t) =
%%0$) of \cref{ode} would have been sufficient, and that allowing for the case of
%%$\g(t) \neq 0$ does not add generality, since the $\g$-term in \cref{ode} may
%%always be removed via a variable transform. Indeed, it can be shown that either
%%the independent or the dependent variable can be rescaled to yield a
%%damping-free ODE. The most general transformation that preserves the linearity
%%of the ODE is
%%\be\label{transformation}
%%t \to \tau := f(t), \quad u \to y:= g(\tau)u,
%%\ee
%%which yields the second-order linear ODE
%%\be\label{transformed-ode}
%%\ddot{y} + \left[ \frac{2\dot{g}}{g} + \frac{f''}{(f')^2} + \frac{2\g}{f'} \right]\dot{y} + 
%%\left[\ddot{g} + \left( \frac{f''}{(f')^2} + \frac{2\g}{f'} \right)\frac{\dot{g}}{g} + \frac{\om^2}{(f')^2} \right]y = 0,
%%\ee
%%where the overdot denotes differentiation with respect to $\tau$. Setting the coefficient of $\dot{y}$ to zero yields the constraint
%%\be
%%2\tilde{\g} = \frac{2\dot{g}}{g} + \frac{f''}{(f')^2} + \frac{2\g}{f'} = 0,
%%\ee
%%and simplifies the frequency of the transformed ODE to
%%\be
%%\tilde{\omega}^2 = \ddot{g} - 2\left( \frac{\dot{g}}{g}\right)^2 + \left( \frac{\om}{f'}\right)^2.
%%\ee
%%This, however, still carries a functional degree of freedom, in that one is free to choose $g$ (or $f$). Despite the fact that the ODE may always be transformed this way, in some cases it is not numerically stable to do so. We demonstrate this through an example taken from cosmology.
%
%A widely accepted theory explaining the origin of large-scale structure in the Universe is that of inflation \cite{baumann2022}. Preceding and during an early phase of exponential expansion, quantum-scale fluctuations in matter density (and hence spacetime-curvature) evolved according to the Mukhanov--Sasaki equation,
%\be\label{mseq}
%\frac{\d^2 \mathcal{R}_k}{\d N^2} + 2\g(N)\frac{\d \mathcal{R}_k }{\d N} + \om^2(N) \mathcal{R}_k = 0, 
%\ee
%where the independent variable is the number of e-folds of inflation, $N = \ln
%a$, with $a$ being the scale factor, a time-dependent characteristic
%lengthscale associated with the expansion. During inflation, $a$ grows
%exponentially, making $N$ a natural independent variable for the purposes of
%computation. $\mathcal{R}$ is a time-dependent scalar field that quantifies
%perturbations in the spatial curvature of spacetime called the gauge-invariant
%curvature perturbation. $\mathcal{R}_k$ is then the Fourier component of
%$\mathcal{R}$ associated with the wavenumber $k$. The damping term and
%frequency in \cref{mseq} are time- ($N$-) and wavenumber ($k$-) dependent, as
%well as functions of the underlying cosmological model. An example
%for their behaviour is shown in the top left panel of \cref{cosmology-example}.
%The bottom left panel then shows the evolution of a single perturbation mode
%$\mathcal{R}_k$ corresponding to a lengthscale of\footnote{When the perturbation mode is governed by
%the Mukhanov--Sasaki equation, the Universe is in its primordial phase of
%inflation, approximately $10^{-32}$ \si{\s} after the Big Bang. These primordial
%perturbations later translate into anisotropies in the temperature and
%polarisation of the Cosmic Microwave Background (CMB), which we can observe
%today. The CMB anisotropies are observed over the celestial sphere and are thus broken
%down into spherical harmonics parametrised by their multipole moment
%$\ell$. Even though a single plane wave projected onto a sphere generates
%anisotropies on multiple scales, the dominant anisotropy it generates will have
%$\ell \approx kD_{\ast}$ \cite{hu1995}, where $D_{\ast}$ is the angular
%diameter distance to the surface of last scattering, measured in
%\si{\mega\parsec}. The scale we consider in the text corresponds to the
%resolution of the current CMB experiments, \ie the largest currently observable
%multipole, $\ell \approx 2000$, or a wavenumber of \SI{0.2}{\per\mega\parsec}.
%} \SI{0.2}{\per\mega\parsec}. The perturbation mode exhibits rapid oscillations
%before it ``freezes out'' at a constant amplitude at which it will remain until
%after the end of primordial inflation. In cosmology, it is this frozen-out
%amplitude we are interested in computing, so the perturbation modes are evolved numerically
%until well into the constant-amplitude regime. Towards the end of the
%integration regime $\om \approx 0$ and $\g \approx \text{const.}$, giving the
%solution
%\be\label{infl-sol-1}
%\mathcal{R}_k \approx C_0 - \frac{C_1}{\g}e^{-\g N},
%\ee
%which quickly converges to $\mathcal{R}_k \approx C_0$. Once this phase of the
%numerical solution is reached, the condition number of the problem does not increase
%significantly -- intuitively, it is the constant part of the solution that is
%of interest, which is easy to follow numerically. Transforming the
%dependent variable to yield the damping-free equation of motion 
%\be
%\frac{\d^2 \tilde{\mathcal{R}}_k}{\d N^2} + \tilde{\om}^2(N) \tilde{\mathcal{R}}_k = 0, 
%\ee
%with $\tilde{\om}^2 = \om^2 + \g' - \g^2$, results in the behaviour plotted on
%the left-hand-side of \cref{cosmology-example}. In the region where the
%perturbation mode's amplitude was previously constant we now have $\tilde{\om}
%< 0$ and $\tilde{\om} \approx \text{const.}$, giving 
%\be
%\tilde{\mathcal{R}}_k \approx C_2 e^{+\tilde{\om}N} + C_3 e^{-\tilde{\om}N}.
%\ee
%The physically meaningful solution is the exponentially
%growing one. It is possible to follow this solution closely, but in this
%formulation the condition number of the problem grows proportionally with
%$|\tilde{\om}N|$, regardless of whether the solution
%is oscillatory or exponential, costing valuable digits.
%
%The primordial power spectrum (PPS) of curvature perturbations is defined as the function
%\be
%\mathcal{P}_{\mathcal{R}}(k) = \frac{k^3}{2\pi^2} |\mathcal{R}_k|^2,
%\ee
%where $|\mathcal{R}_k|$ is taken as the frozen-out amplitude of the given perturbation mode. 
%In cosmological inference, forward modelling often starts from the PPS which is
%computed assuming some set of cosmological parameters. For each PPS
%computation, one needs to solve \cref{mseq} for $\mathcal{O}(10^3)$ values of
%$k$, and around $\mathcal{O}(10^7 - 10^9)$ likelihood evaluations (each of
%which involves one PPS calculation) are necessary for mapping the posterior
%probability distribution. This gives a total of {$\mathcal{O}(10^{10} -
%10^{12})$} numerical solves of the highly oscillatory \cref{mseq}, which is only
%feasible if an extremely efficient, specialised numerical method is used. Moreover, the above reasoning demonstrates that the numerical method needs to be able to deal with a non-zero $\g$-term.   
%
%\begin{figure}[tbp]
%    \centering
%    \includegraphics{plots/cosmology.pdf}
%    \caption{\label{cosmology-example} Plot illustrating the change in the
%    terms in the Mukhanov--Sasaki equation \cref{mseq} and the behaviour of its
%    solution upon transforming to $\gamma$-free form. The top panels show the
%    frequency and damping term (where applicable) in the ODE as a function of
%    the e-folds of inflation $N$, while the bottom panels shows the evolution
%    of the dependent variable, the gauge-invariant curvature perturbation
%    $\mathcal{R}_k$ of comoving wavenumber $k$ corresponding to an 
%    observable lengthscale of \SI{0.2}{\per\mega\parsec}. Note the
%    symmetric-logarithmic scales on all $y$-axes, which is linear close to
%    zero but logarithmic otherwise. }
%\end{figure}


\section{Conclusions \label{conclusions}}

% Summary of paper
We presented an efficient numerical algorithm for solving linear second order
ODEs of a single variable, posed as initial value problems (see \cref{ode}), whose solution
might be highly oscillatory for some regions of the solution interval.
The
algorithm's runtime is independent of the characteristic frequency of
oscillations, like that of a few contemporary solvers that exploit asymptotics to speed up computation in the highly oscillatory region.
Our solver is an improvement on these contemporary solvers
in three ways: it is capable of solving the ODE efficiently regardless of
whether its solution oscillates or not; it is arbitrarily high-order accurate;
and it solves a more general problem by not imposing the restriction $\gamma =
0$ in \cref{ode}. 
We described the two methods making up the algorithm which works by
time-stepping, the mechanism by which the stepsize is adapted based on local
error estimates, and the procedure that decides which of the two methods to
use. One of the methods is a spectral solver based on Chebyshev nodes, while
the other is a numerical functional iteration that generates an asymptotic
solution for the Riccati form of the ODE. This amounts to constructing a
non-oscillatory phase function (logarithmic derivative of the dependent
variable). The error in this asymptotic expansion was measured in terms of the
residual of the Riccati equation. We have shown that given a sufficiently
slowly varying and large $\om$ and suficiently small $\g$, this residual
converges geometrically for the first $k$ iterations, but at a rate that
deteriorates with $k$. We formalised and proved this statement in \cref{TR},
and illustrated this temporary convergence in a numerical experiment. Further
numerical experiments confirmed the convergence of the overall algorithm and
compared its performance with that of state-of-the-art and standard solvers.

% Future work
% Allow for imaginary $\om$
Within the scope of this work, we only allowed $\om^2 \geq 0$. When $\om$ is
imaginary, any standard numerical solver will pick up the exponentially growing
solution due to rounding error, which will quickly overpower the evanescent
solution. A similar asymptotic approach, however, is possible as in the
oscillatory case, in that one can construct two series solutions to the Riccati
equation which form a complete basis for \cref{ode} and can thus be linearly
combined and the desired (evanescent or growing) solution can be selected for
each timestep. Care needs to be taken in regions where $|\om|$ is not large
enough for the asymptotic expansion to be sufficiently accurate.

% BVPs
Although we focused on initial value problems, note that due to the linearity
of the ODE any set of auxiliary conditions can be satisfied as a
post-processing step by solving the ODE with two sets of linearly indipentend
initial conditions and linearly combining the resulting solutions.

% Error analysis
It is left to a future analysis to relate the residual of the Riccati equation
\cref{ricc} or the original ODE \cref{ode} to an estimate of the solution's
local error. This would make the error control more uniform across the
different types of steps our algorithm uses and is the quantity being
controlled in typical numerical solvers.  

% Global spectral method ~ Chebop
Spectral methods are usually applied over the entire solution interval (\eg in
\cite{driscoll2008}) rather than within a time-stepping framework. When applied
locally and over a stepsize that can be changed adaptively, a degeneracy
appears between the number of nodes $n$ and the stepsize $h$, in that $n$ can
be increased and $h$ can be decreased to lower the local error. This degeneracy
could potentially be broken by determining the points at which the algorithm is
supposed to switch between Riccati and Chebyshev steps \emph{a priori}, or at
least identifying large regions of the solution interval where the Chebyshev
spectral will need to be used, and solving the ODE in these regions with a
single application of the spectral method with large $n$. Using \eg a sparse
spectral element method \cite{fortunato2021}, the solution of these regions
could be made more efficient.

% Quadrature
The piecewise-continuous numerical solution $\tilde{u}(t)$ our algorithm generates is made up
of sections represented by polynomials or exponentials of polynomials with
complex coefficients. Both of these are analytic in the complex plane, and so
it may be possible to calculate $\int_a^b \tilde{u}(t)\mathrm{d}t$ quickly,
despite $\tilde{u}$ being highly oscillatory for some or all of $[a, b]$, by
finding an appropriate contour in the complex plane. The more oscillatory
$\tilde{u}$, the faster it decays along the imaginary axis, and so the smaller
the number of quadrature points required to compute the numerical integral to a
given accuracy. This application will be explored in a future paper.
% PDEs???

\section*{Acknowledgments}
We benefitted greatly from discussions with Jim Bremer, Charlie Epstein,
and Manas Rachh.
The Flatiron Institute is a division of the Simons Foundation.

% Some things left to do:  (are this FA or AB ones? Are some obsolete?) These are for FA! Some may be obsolete.
% AB comments with "-"

% Figure for complex plane setup for proof of thm 3
% - sure, get from your talk.

% Numerically stable residual iterations!
% - not sure what you mean

% Final description of stepsize algorithm 
% Flowchart?
%  - imprt from talk

% Cosmo stuff? (Paper is too long!)
% - see once finer edit.

% Bremer's basic iteration (worked it out on paper but paper is already too long)
% - add footnote saying other iterations exist, give Bremer's. Note it
% needs sqrt which bring branch issue.

% Some comment that although theorem is pointwise convergence, uniform convergence over some interval is trivial by enclosing it in a larger ball.}
%  - ok, put in Sec 3.

% Relate the ball in the theorem to the interval size that can be well-approximated by $p$th order Cheby, using Bernstein ellipse of Tref
% - too much for here.

% From [residual of ODE and Riccati] to solution error bounds, maybe? This is similar to local discretization err in a more typical numerical solver. (check LeVeque, Tref, Hairer, etc).
% - too hard, state in Discussion.

% Numerical results to support deterioration of convergence rate r with total iteration number k in Thm (figure?)
% - no you have this in Fig 1 right?

% More references needed
% - just in intro

% BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
\Fruzsi{
\section{List of changes}

\begin{itemize}
    \item{General
        \begin{itemize}
            \item{Formatted cref so that equations appear in brackets without the ``Eq.'' in front.}
            \item{Removed $t_{\mathrm{eval}}$ from tables and text since not measured.}
            \item{Added a two-panel figure showing 1) a flowchart of the algorithm and 2) the complex plane setup for the proof of the convergence of Riccati iteration.}
            \item{Our method has a name now! \texttt{cascade}: \textbf{c}ode for \textbf{a}daptive \textbf{s}pectral Ric\textbf{ca}ti \textbf{d}efect corr\textbf{e}ction. Used this in the legend in \cref{bremer237-timing}. $\to$ The code will probably be a part of \texttt{oscode}, so I dropped this and are just referring to the method in the relevant figure as RDC -- Riccati defect correction. }
        \end{itemize}
    }
    \item{Abstract
        \begin{itemize}
            \item{Changed title to be shorter, ``arbitrarily high-order'' to spectral, added adaptivity.}
            \item{Scope of work sounded too narrow, so removed ``ODE of a single dependent variable'', and added BVPs.}
            \item{Reordered introduction to methods and added more detail for Riccati steps.}
            \item{Added comparison to other solvers.}
        \end{itemize}
    }
    \item{Introduction
        \begin{itemize}
            \item{Removed bit on oscillatory behaviour in ODEs arising in different ways (was thinking about oscillatory forcing terms, coupled first-order ODEs, etc).}
            \item{Added examples for ODEs of interest in areas of physics and engineering, with citations. Added argument that our method could replace analytic asymptotics applied by hand.}
            \item{Changed criteria on $\om$, $\gamma$ as per suggestion.}
            \item{Notation change $i \to j$ in WKB ansatz, also compressed to one line.}
            \item{Discussed complex ODE coefficients.}
            \item{Clarified wording in problem statement, \eg discretization nodes instead of time-points.}
            \item{Added QLM citations. Concerncs: Mandelzweig 2004 is a preprint -- physics journals usually don't like authors citing papers that haven't been peer-reviewed. }

        \end{itemize}
    }

    \item{Numerical results
        \begin{itemize}
            \item{Added Corollary after $\kappa$ definition to state maximum achievable accuracy, and referred to this from the Airy equation subsection.}
            \item{Airy equation: refined justification of this test case, addded new paragraph to point to the convergence plot (right panel of \cref{convergence-plot}).}
            \item{Going to refer to Bremer's work as phase function method rather than Kummer's phase function method, I think that would be too long.} 
            \item{Added DLMF citation to Airy functions.}
        \end{itemize}
    }
\end{itemize}

}
\bibliographystyle{abbrv}
\bibliography{refs}
\end{document}

