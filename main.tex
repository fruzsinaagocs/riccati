\documentclass[10pt]{article}
\oddsidemargin = 0.2in
\topmargin = -0.5in
\textwidth 6in
\textheight 8.5in

\usepackage{graphicx,bm,amssymb,amsmath,amsthm} % Figures and maths
\usepackage{xcolor}
\usepackage[colorlinks]{hyperref} % Hyperlinks
\usepackage[capitalise, nameinlink]{cleveref} % Better referencing
\usepackage{array}
\usepackage{showlabels} % Shows eq, fig, etc labels
\usepackage{siunitx} % SI units
\usepackage[caption=false]{subfig} % For subfloats
\usepackage{bold-extra} % bold for texttt
\usepackage{bm} % bold for maths 
\usepackage{tabularx} % for nice tables


% SI unit definitions
\DeclareSIUnit\parsec{pc}

% -------------------------------------- macros --------------------------
% general ...
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\bmp}[1]{\begin{minipage}{#1}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\pig}[2]{\bmp{#1}\includegraphics[width=#1]{#2}\emp} % mp-fig, nogap
\newcommand{\bp}{\begin{proof}}
\newcommand{\ep}{\end{proof}}
\newcommand{\ie}{{\it i.e.\ }}
\newcommand{\eg}{{\it e.g.\ }}
\newcommand{\etal}{{\it et al.\ }}
\newcommand{\etc}{{\it etc.\ }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdc}[3]{\left. \frac{\partial #1}{\partial #2}\right|_{#3}}
\newcommand{\infint}{\int_{-\infty}^{\infty} \!\!}      % infinite integral
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}^2}
\renewcommand{\d}{\mathrm{d}} % Upright differential
\newcommand{\ve}[4]{\left[\begin{array}{r}#1\\#2\\#3\\#4\end{array}\right]}  % 4-col-vec
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\bigO}{{\mathcal O}}
\newcommand{\qqquad}{\qquad\qquad}
\newcommand{\qqqquad}{\qqquad\qqquad}
\newcommand{\eps}{\epsilon}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\vol}{vol}
\newtheorem{thm}{Theorem}
\newtheorem{cnj}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{defn}[thm]{Definition}
% this work...
\newcommand{\om}{\omega}
\newcommand{\g}{\gamma}
\newcommand{\te}{\tilde\eta}
\crefalias{prop}{Proposition}
% Review
\newcommand{\AB}[1]{{\color{orange}#1}}
\newcommand{\Fruzsi}[1]{{\color{blue}#1}}

\begin{document}

\title{An arbitrarily high order accurate numerical method with
frequency-independent runtime for highly oscillatory second order linear ODEs}
%\title{Analysis of an iteration for non-oscillatory solutions to the Riccati form of the phase function for 2nd-order linear ODEs} % glad you killed the version starting "Analysis..." :)


\author{Fruzsina J.\ Agocs and Alex H.\ Barnett}
\date{\today
\AB{title bit long. how can make shorter \& catchier? (``high'' appears twice.)
  To me the new ideas are: phase function iteration (defect correction), spectral (= arb high ord); adaptivity; switching; damping term.
  So how about:
  An adaptive switching spectral method
  for oscillatory second-order linear ODEs with frequency-independent cost.
  Try rearranging the words a lot.
  An iterative (Riccati) phase function spectral solver
  for oscillatory second-order linear ODEs with frequency-independent cost.
  ETC
}  
}
\maketitle

\begin{abstract}
 \AB{Abs should mention apps plus that's a good excuse to motivate the switching. An idea:
   Linear second order ODEs have applications including
   special functions, quantum physics, and cosmology, and
   transitions from highly oscillatory to %evansescent
   smooth regions are common.} 
  We introduce an efficient numerical method for second order linear ODEs of a
single dependent variable posed as initial value problems, the solution of
which may vary between being highly oscillatory to slowly changing over the
course of the solution interval. \AB{this opening makes the range of application sound narrow. We want to say it is wide. Eg, no need to emphasize "single dep var": it is assumed unless you say "system of ODEs". We are selling a new idea that maybe could be generalized?}
\AB{Our contrib is mainly the Riccati method, so I think that should go first,
  before describing switching or the (standard) smooth spectral part.}
\AB{We should summarize key ideas that I think are new, specific, and will set the paper apart from previous work:
  In oscillatory regions
  a {\em phase function} representation
  for a solution pair $u(t) = e^{\pm z(t)}$ is used,
  so that $z'$ satisfies a nonlinear Riccati equation.
  %Note how we pack in teaching the reader the connections here
  We introduce % key word to say what is new
  a defect-correction iteration that gives an asymptotic series for
  a {\em non-oscillatory}
  Riccati solution on an interval;
  this iteration may thus may be spectrally approximated on a
  Chebychev grid with a small
  number of nodes.  % thus since non-osc -> small nodes
  % I'm trying to pack info about our core contrib.
  For analytic coefficients
  we prove the asymptotic convergence of the resulting residual,
  with explicit error bounds given the number of terms.
}
The algorithm automatically switches between two methods for dealing with the
two types of behaviour and has $\bigO(1)$ runtime with respect to
the frequency
%of oscillations % don't need
in the highly oscillatory regime.
% apart from switching, this repeats title -> cut.
It is %arbitrarily high order = spectral
\AB{spectrally}
accurate in both the oscillatory and the %slowly changing
smooth
regimes.
When the solution changes slowly a spectral method based on Chebyshev
nodes is used, where the number of nodes is changed adaptively to ensure the local error is
within a user specified tolerance.
During a highly oscillatory phase, the
ODE is converted to Riccati form and a non-oscillory solution is
constructed for the logarithmic derivative of the dependent variable (the
phase function) via an asymptotic series. % note inline formula conveys this more easily to the reader
\AB{This is fine but see above too.}
The number of terms included
in the series is also adaptive.
We describe in detail each method making up the algorithm and analyse their
error.
\AB{well, we only analyze the Riccati error - might be better to place that statment right after Riccati method - see above.}
We prove that the asymptotic series achieves temporary
geometric convergence in terms of the residual of the Riccati equation with a
rate that improves as the rate of change of the frequency gets smaller
relative to its magnitude.
\AB{Great. need to say: for analytic coefficients. Can merge that into the above?}
We demonstrate the \AB{its} efficiency and applications
\AB{I think better to say ``in apps'' rather than ``and apps''}
of
the algorithm through several numerical examples in which we also compare
it to other state-of-the art numerical solvers for oscillatory problems.
\AB{long sentence; split. Can you also be specific about metric, eg number of func evals, or CPU time?}
\AB{Can we say our solver has advantages over previous frequency-indep
  methods esp when demanding high accuracy? Again aim for specificity but in the minimum number of words.}
\end{abstract}

 

\section{Introduction}

The efficient numerical solution of highly oscillatory ordinary differential
equations (ODEs) has long been a challenge.
%and focus of computational mathematics.
A wide range of specialised methods exists for dealing with ODEs
with different structures,
thorough reviews of which are found in
\cite{petzold1997,engquist2009}. Due to the large number of ways oscillations
in ODEs can arise, only a handful of methods have been designed to deal with any
given type of oscillatory ODE.
\AB{I don't get this last ``ways'' - are you thinking of nonlin vs lin, or systems vs scalar, or nth-order vs 2nd-order? We know you need at least 2nd ord or 2 coupled 1st ord to get osc, but then there is only one mechanism}

In this work we focus on ODEs of a \AB{particular $\to$ commonly occurring}
form: the general, linear, second
order ODE for a single variable in one dimension
\AB{are the coeffs smooth, piecewise smooth, analytic, etc?}
This is often  generalised single harmonic oscillator.
Since approximation by harmonic oscillators is ubiquitous in physical models,
this ODE, albeit simple-looking, is pivotal in many computational physics problems.
\AB{need to back this up: can say that in early-universe sims need
  to run millions of such ODE solns at different parameters, so solution needs to be very rapid.}
\AB{Can you cite apps: your cosmo, 1D QM eg book, variable-parameter oscillators.}
\AB{other apps ideas - please check them out to see if appropriate:
https://www.sciencedirect.com/science/article/pii/S0167268196008761
https://www.sciencedirect.com/science/article/abs/pii/S0375960117311714
Refs inside here (not the thing itself):
https://apps.dtic.mil/sti/pdfs/ADA267246.pdf
}

The ODE of interest takes the form
%We are interested in efficient numerical solution of
%the oscillatory linear 2nd-order ordinary differential equation (ODE)
\be
u''(t) + 2\g(t) u'(t) + \om^2(t)u(t) = 0, \qquad t \in (t_0,t_1),
\label{ode}
\ee
\AB{where $\g$ is a given smooth ($C^\infty$) {\em damping} function,
  and $\om$ is a given smooth, and possibly large, real-valued
  {\em local frequency} function.}
\AB{discuss if $\om$ is pure real or pure im.
  Tricky if we don't allow pure im, ie $\om^2<0$ then we don't get to
  talk about evanescent regions which are ubiquitous in QM apps.
  The word ``general''
   that you use to describe the problem needs to be carefully specified instead. ``General'' could mean anything... stochastic coeffs, god knows what. Physicists tend to use ``general'' but math ppl know better since it doesn't mean a lot just sounds good :)}
  \footnote{If coefficients are merely piecewise smooth, then solution may
  be simply patched together at the breakpoints; we do not discuss this situation further.}
  
We solve the initial value problem (IVP) by specifying the initial conditions
\begin{align} % \ba, \ea doesn't work here, LaTeX complains
    u(t_0) &= u_0, \label{ic0} \\
    u'(t_0) &= u'_0, \label{ic1}
\end{align}
\AB{although we note that the adaptive frequency-independent
  methods we present could easily be applied to two-point boundary value
  problems.}
%Here the local frequency function $\om(t)$ is purely real or purely imaginary.
% AB: so is om(t) complex or real?
In regions where $\om \gg 1$, the solution $u$ is oscillatory, meaning that on some local
scale it is approximately $a e^{i\om_0t} + b e^{-i\om_0t}$, where $\om_0$
denotes a local value of $\om(t)$.
Conventional ODE integrators then require discretization with several
time-points
\AB{grid or discretization points/nodes, is more usual}
per local period $2\pi/\om$, which can be prohibitively slow.
This has led to the development of more efficient solvers for this
high-frequency case
\AB{that exploit new representations of the solution (maybe: to allow many periods per node}.
However, as we will review later, none of these methods so
far has been robust or general enough to
\AB{tackle the above problem}
solve the general ODE efficiently at an arbitrarily
\AB{sure not arb high tol? you mean close to machine accuracy?}
high
tolerance when the solution has highly oscillatory and smooth regimes. % AB

In this work we only consider regions where $\om(t)$ and $\g(t)$ are real-valued and $\om(t)$ is positive, but 
one may also consider $q(t):=\om^2(t)$ real and allow evanescent
$q<0$ regions, where the solution $u$ has rapid growth and decay.
\AB{THat should group with the statement of the problem, which is above.
  Do we believe any prob with $\om^2 <0$ ?  Or just we don't test it.
  Maybe just say $\om^2$ real, and maybe we throw in a test for evan case?}

% Asymptotic methods: WKB, oscode, QLM
% Not asymptotics: Bremer
A large class of efficient numerical solvers for highly oscillatory ODEs of the
above form makes use of asymptotic expansions. 
% WKB
Of these, the most commonly used is the Wentzel--Kramers--Brillouin (WKB), a
perturbation method applied to linear differential equations that contain some
small parameter $\varepsilon$ \cite{logan,benderorszag}. This approximation is particularly widely applied in quantum mechanics for constructing approximate analytic solutions for the Schr\"{o}dinger equation. 
Setting $\g = 0$ for
the time being, one would start from \cref{ode} with a small parameter added,
\be\label{odewkb}
\varepsilon^2 u'' + \om^2 u = 0,
\ee
and make the substitutions $u = e^{z/\varepsilon}$, $z' = x$ to get the Riccati form of the equation,
\be\label{riccwkb}
\varepsilon x' + x^2 + \om^2 = 0.
\ee
Then try the regular perturbation expansion
\be\label{wkbexpansion}
x = \sum_{i=0}^{N} x_i\varepsilon^i,
\ee
and match powers of $\varepsilon$ in \cref{odewkb} to obtain the recursion relation
\begin{align}\label{wkbrecur}
    x_0 &= \pm i\om,\\
    x_i &= -\frac{1}{2x_0}\left( \dot{x}_{i-1} + \sum_{k = 1}^{i-1}x_k x_{i-k} \right).
\end{align}
The WKB approximation forms the basis of the numerical method described in
\cite{agocs2020efficient,agocs2020dense}, and associated open-source software
package \texttt{oscode} \cite{agocs2020joss}. This uses the expansion
\cref{wkbexpansion} up to and including $x_3$ to advance the numerical solution
for $u(t)$ in a time-stepping context, but has the ability to switch to a 4,5th
order Runge--Kutta pair if the asymptotic expansion isn't accurate enough. 
In \cite{arnold2011wkb,korner2022wkb} the ``WKB marching method'' uses the same
expansion truncated after $N = 2$  to transform \cref{odewkb} into a smoother
problem.
% QLM

Instead of finding an approximation via a regular perturbative expansion, the
quasilinearisation approach (QLM) \cite{bellman1970} approximates the solution
of \cref{riccwkb} with a series of iterates $x_i(t)$ by converting it to a set
of recurrence differential equations
\begin{gather}\label{qlmode}
x_0 = \pm i\om, \\
\varepsilon x_i' - x_{i-1}^2 + 2x_i x_{i-1} + \om^2 = 0,
\end{gather}
and solving them analytically. In \cref{phasefun} we show that this is equivalent to
starting from the zeroth order solution $x_0 = \pm i\om$ and generating
successive iterates $x_i(t) := x_{i-1}(t) + \delta(t)$ by setting the residual they leave on the
right-hand-side when substituted into \cref{riccwkb} to zero, to first order in
$\delta$. While the QLM method could be used to generate an alternative
asymptotic solution to \cref{odewkb}, to the authors' knowledge it has not yet
been developed into a numerical ODE solver.

% Bremer
% Accuracy?
% arbitrarily high order
Recently Bremer \cite{bremer2018} following \cite{heitman2015,bremer2016} has proposed an efficient
method for solving \cref{ode} in the high frequency regime that runs
in $\bigO(1)$ time, independently of the magnitude of the frequency of
oscillations $\lambda$, which they define as $\om^2 = \lambda^2 q(t)$.
The method aims to find a nonoscillatory phase function $\alpha(t)$, defined
via Kummer's equation, a nonlinear and generally oscillatory ODE, and will therefore be
referred to as the phase function method hereafter.
The nonoscillatory phase function $\alpha$ can be used to compute an approximation
of the solution of \cref{ode} accurate to within
$\bigO((\mu\lambda)^{-1}\exp(-\mu\lambda))$, where $\mu$ is a constant that depends on $q(t)$ but not $\lambda$.
The proposed algorithm itself generates $\tilde{\alpha}$, an
\emph{approximation} of $\alpha$, such that the logarithm form of Kummer's
equation is satisfied to within $\bigO(-\tfrac{1}{2}\mu\lambda)$. These
generated phase functions are nonoscillatory in the sense that they can be
accurately represented using a series expansion (\eg via Chebyshev polynomials)
with $\bigO(1)$ terms, independently of $\lambda$.
The basis of the method are the observations of \cite{heitman2015,bremer2016}
that prove the existence of such nonoscillatory phase functions if $q(t)$ is
nonoscillatory, in the sense that the Fourier transform of $\log q$ is smooth
and rapidly decaying.
Finding initial conditions on the phase function leading to
a nonoscillatory solution is, however, challenging.
The authors of \cite{bremer2018} proposed a ``windowing'' method to numerically find such a global phase function
on an interval:
the idea is to first transform the interval $t \in [a,b]$ to $[0, 1]$, then use a $C^\infty$ partition of unity near $t=1$ to smoothly ``blend'' $q$ to
a constant function, integrate backwards from the constant region
down to $t = 0$, and finally integrating $t \in [0,1]$ forward using the original $q$.
This exploits the facts that a nonoscillatory phase function for
the constant case $q(t) = \om_0^2$ is simple to write down; and
that the smooth window allows
the nonlinear Kummer oscillator to make a smooth 
%(in physics terminology ``adiabatic'') 
transition which excites only an exponentially small oscillation amplitude.


Here we present an alternative approach using
an asymptotic expansion for a nonoscillatory
Riccati phase function, meaning simply that the
phase function is the logarithmic derivative $x(t) = (\log u(t))'$ from \cref{riccwkb}.
%
Our expansion is simpler than the traditional WKB expansion in that the
associated recursion relation $x_i(x_{i-1}, x_{i-2}, \ldots, x_0)$ only
involves the previous term $x_{i-1}$ as opposed to all past terms in
\cref{wkbrecur}. 
%
The expansion presented here has similar accuracy, and
exhibits geometric convergence for the first few iterations, which we show
formally in \cref{errorana}. The iterations are computed using high-order
numerical differentiation, rather than algebraically as the WKB expansion is
usually applied.
%
The simplicity of the recursion relation makes it
computationally simpler for the method to be adaptive in the number of terms it
uses in the asymptotic expansion, which allows it to outcompete the
fixed-asymptotic-order WKB-marching method \cite{korner2022wkb,arnold2011wkb}
and \texttt{oscode} \cite{agocs2020efficient,agocs2020dense} in regions of low-to-intermediate $\omega$.
%

The iteration does not introduce oscillatory contributions,
hence finds a nonoscillatory approximate phase function without
reference to initial conditions, in contrast to the phase function method.
Another crucial difference from the phase function method is that $x(t)$ is solved for
locally on an interval with an adaptively-chosen length, rather than globally.
Solutions to \cref{ode,ic0,ic1} are then constructed by time-stepping over the
integration range and matching Cauchy boundary data over adjacent intervals.
%

An important limitation of the phase function method (but not WKB-marching or
\texttt{oscode}) is that it is only applicable in the high frequency regime.
The present work, like the latter two, can deal with both regions where the
solution is oscillatory (and the asymptotic expansion used to represent it is
sufficiently accurate) and when it is not. This is achieved by switching
between the asymptotic expansion and a spectral method on the fly as the solver
steps through the solution interval $[t_0, t_1]$. The spectral method is based
on $n+1$ Chebyshev nodes, where $n$ is chosen adaptively, resulting in an
% CHECK NUMERICALLY!
$\bigO(h^{n})$ local error, where $h$ is the stepsize. This is in contrast to
\texttt{oscode} and the WKB-marching method, which both use a fixed (4,5th)
order Runge--Kutta scheme as their alternative solver and produce a
$\bigO(h^6)$ error. \texttt{oscode}'s WKB-based method produces an $\bigO(h)$
error when $h \ll 1$, although in regions of high-frequency oscillations, $h$
tends to be very large in comparison to the solution's timescale. The
WKB-marching method's oscillatory solver improves on this and has $\bigO(h^2)$ local error,
resulting in better performance at lower tolerances. The local error of the
present numerical method's asymptotic steps depends on the stepsize $h$ in two
ways: first, the asymptotic expansion's validity is determined by the magnitude
and rate-of-change of $\om$ and $\g$ over the course of the step (formalised in
\cref{errorana}); and second, the iterates $x_i$ are computed via numerical
differentiation on a Chebyshev grid with an adaptive number of nodes, which has
$\bigO(h^{n+1-k})$ accuracy if the number of iterates included is $k$. In this
sense, both of the methods making up our solver are arbitrarily high order.

%
A final advantage of the present method over \eg WKB-marching and the phase function method
is that it includes a damping term $\g(t)$, allowing direct solution of a wider
range of second order, linear ODEs.

%Structure
This paper is structured as follows. \cref{methods} describes the various
numerical methods used in the proposed algorithm including the asymptotic
expansion for regions of oscillations, the spectral method based on Chebyshev
nodes, adaptive control of stepsize, and switching between the two alternative
sub-methods. \cref{errorana} discusses the error properties of the method based
on the asymptotic expansion of the Riccati equation and its consequences for
implementation. We show results from numerical experiments in
\cref{numresults}, which involves a comparison of the contemporary ODE solver
mentioned above. \cref{conclusions} concludes this paper with a summary and
suggestions for future work.

%One advantage over Bremer is that our method generates an explicit asymptotic
%expansion. A disadvantage is that its error is limited by the number
%of iterations, and by $\om$ itself; however,
%we will show that close to machine accuracy is easily reached in
%intervals that contain many oscillations.

\section{Methods \label{methods}}

\subsection{Phase function solution on a single interval \label{phasefun}}

We will use prime to denote differentiation with respect to time $t$.
We consider an interval $t\in[a,b]$ lying in $[0,1]$.  
Writing $u = e^z$ where $z'(t) = x(t)$ is the phase function,
then any nonvanishing $u(t)$ satisfies \cref{ode} if and only if
its complex phase function $x(t)$ satisfies the nonlinear Riccati equation
\be
x' + x^2 + 2\g(t)x + \om(t)^2 = 0.
\label{ricc}
\ee
For the sake of simplicity, let us set $\g(t) = 0$ for the time being.
Almost all solutions to \cref{ricc} are oscillatory,
as the presence of $\om(t)$ would suggest.
This is illustrated by the case of constant $\om(t) = \om_0^2$ 
which has the family of analytic solutions
$x(t) = \om_0 \tan(\alpha - \om_0t)$ parameterized by $\alpha\in\C$,
where $\re \alpha$ is interpreted as a phase shift and $\im \alpha$
controls the amplitude. Their oscillation frequency is $2\om_0$.
However, the limit $\im \alpha \to \pm \infty$ produces
the only two nonoscillatory solutions $x(t) = \pm i\om_0$.
The picture is similar
for general analytic functions $\om(t)$:
it has recently been proved that there exist nonoscillatory phase functions
(in a precise sense for $\om\gg 1$ involving exponential decay of the Fourier
transform) in that case \cite{heitman2015,bremer2016}.
In fact this result was proven
for the related real-valued Kummer's equation
\be
\frac{3 \alpha'^2}{4\alpha^2} - \frac{\alpha''}{\alpha} - \alpha^2 +
\om(t)^2 = 0,
\label{kummer}
\ee
corresponding to ODE solutions $u(t) = \alpha(t)^{-1/2} e^{\pm i\int \alpha(t) \d t}$,
but the existence of a nonoscillatory Kummer's solution implies the same for
a Riccati solution\footnote{We thank Jim Bremer for explaining this argument.}.
The latter is easy to show, since if $\alpha(t)$ satisfies \cref{kummer},
then defining $\beta = -\alpha'/2\alpha$ allows \cref{kummer}
to be written
$\beta' - \alpha^2 + \beta^2 + \om(t)^2 = 0$, and these last two equations
are the real and imaginary parts of \cref{ricc}
for $x = i\alpha + \beta$.

We now turn to our numerical method to find nonoscillatory
solutions to \cref{ricc} (in its most general form, \ie without imposing $\g(t) = 0$).
When $\om\gg 1, \g$, there are two nonoscillatory solutions
which take the approximate form
$x_{\pm}(t) \approx \pm i\om(t)$, leading to a conjugate pair of basis
counter-rotating functions $u_{\pm}(t) := e^{\int x_\pm(t) \d t}$.
Given any trial solution $x$ to \cref{ricc}, its \textit{residual
  function} is defined as the left-hand side of \cref{ricc}, namely
\be
\label{R}
R[x](t) := x' + x^2 + 2\g(t)x + \om(t)^2.
\ee
Our proposal takes the following \textit{functional iteration}
to generate a sequence of functions $x_0, x_1, \dots, x_k$
on $t\in(a,b)$,
using the above positive (say) approximation as a starting point:
\begin{align}
x_0(t) &= +i\om(t)
\label{init}
\\
    x_{j+1}(t) &= x_j(t) - \frac{R[x_j](t)}{2 \left( x_j(t) + \g(t) \right)}, \qquad j=0,1,\dots,k-1.
\label{iter}
\end{align}

%with the functional $R$ defined as in \cref{R}.
This may be justified heuristically as an approximate Newton
iteration to reduce the residual function
(noting that similar exact Newton iterations have been
exploited for analysis \cite{heitman2015}).
Namely, since
\begin{align}
    R[x_j + \delta] &= x_j' + \delta' + x_j^2 + 2 x_j \delta + \delta^2 
    + 2\g x_j + 2\g\delta + \om(t)^2 \nonumber \\
    &= R[x_j] + \delta' + 2x_j\delta + 2\g\delta + \bigO(\delta^2), \nonumber
\end{align}
by linearizing we get that $\delta$ solves the linear 1st-order ODE
$\delta' + 2x_j(t) \delta + 2\g \delta  = -R[x_j](t)$,
which has an analytic solution.
Solving this exact Newton update symbolically is the basis of the quasilinearization method (QLM) \cite{bellman1970}.
However, this ODE is again generally oscillatory,
with unknown initial condition needed for a nonoscillatory solution $\delta$.
Yet if $\delta$ is nonoscillatory the first term $\delta'$ is
a factor $x_j = \bigO(\om)$ smaller than the second term, thus
one might hope that by dropping it a useful reduction in residual might
still result.
%\footnote{\Fruzsi{[Strictly speaking, we should then be dropping the $\g$ term as well, since it's also an order of magnitude smaller...]}}. 
This leads to the algebraic formula $2\left(x_j(t) + \g(t)\right) \delta(t) = -R[x_j](t)$
which is solved pointwise for each $t$, and
writing $x_{j+1} = x_j + \delta$ gives \cref{iter}.
This is a type of defect correction scheme \cite{bohmer1984}.
The result is nonoscillatory by construction, without explicit reference
to initial conditions.

The early iterates of \cref{init,iter} illustrate
algebraically
the type of residual reduction that occurs. We take $\g(t) = 0$ below for the
sake of simplicity, but the results hold for the case of $\g(t) \neq 0$. The first few iterates and residuals are
\begin{align}
    x_0 &=  i\om, &&R[x_0] = i\om' = \bigO(\om), \nonumber \\
x_1 &= i\om - \frac{\om'}{2\om}, 
    &&R[x_1] = -\frac{\om''}{2\om} + \frac{3\om'^2}{4\om^2} = \bigO(1), \nonumber \\
x_2 &= i\om - \frac{\om'}{2\om} + \delta_1, \mbox{ where }
    \delta_1 \equiv \frac{\frac{\om''}{2\om} - \frac{3\om'^2}{4\om^2}}{2i\om - \frac{\om'}{\om}}, \quad
    &&R[x_2] = \delta_1' + \delta_1^2 = \bigO(\om^{-1}). \nonumber
\end{align}
To measure the size of term we assume that $\om'$, $\om''$, \etc, are
of the same order as $\om$, because $\om(t)$ is smooth,
and also assume that $\om$ has a lower bound of the same order. If we were to include $\g$, we would assume $\g = \bigO(1) $ in terms of $\om$, and that its derivatives are of the same order as $\g$ itself by the same reasoning as with $\om$. 
Thus we see that each iteration the residual drops by a factor $\bigO(\om)$.
However, each iteration also results in terms with
one higher time-derivative of $\om$ (and $\g$).
Despite the rapid growth in complexity, one can check that this
pattern continues
by defining the $j$th correction function $\delta_j$,
so that $x_{j+1} = x_j + \delta_j$, then using \cref{R} to write
\be
R[x_{j+1}] = R[x_j] + 2\left(x_j + \g \right)\delta_j  + \delta_j^2 + \delta_j' = \delta_j^2 + \delta_j',
\label{Rdelta}
\ee
where the first two terms cancel by design due to \cref{iter}.
The resulting evolution of the residual is then
summarized as follows, which is proved simply by
substituting $\delta_j \equiv -R[x_j]/2(x_j + \g)$ into \cref{Rdelta} and using
the quotient rule.
\begin{pro}\label{PRiter}
  Let $x_j \in C^2([a,b])$ be a function on an interval $[a,b]\subset \R$
  % we may as well be pedantic about conditions
  such that $x_j(t) \neq 0 \; \forall t\in [a,b]$,
  let $\om\in C^1([a,b])$,
  $\g\in C^1([a,b])$,
  and let $x_{j+1}$ be given by the single iteration \cref{iter}.
  Then their associated residual functions \cref{R} on $[a,b]$
  are related pointwise by
  \be
  \label{Riter}
    R[x_{j+1}] = \frac{1}{2(x_j + \g)}\left( \frac{(x_j + \g)'}{x_j + \g} R[x_j] - R[x_j]' \right) 
    + \left(\frac{R[x_j]}{2(x_j + \g)}\right)^2. 
  \ee
\end{pro}
Thus, with the initialization \cref{init},
assuming that the lower bound on each $x_j$ is no smaller than $\bigO(\om)$,
and that the quadratic term in \cref{Riter} is small,
one might hope that
the residual shrinks like $R[x_j] = \bigO(\om^{1-j})$ for each $j=0,1,\dots$.
Unfortunately this will turn out not to be true, due to the increasing
order of derivatives of $\om(t)$, which will lead to an
asymptotic (but not convergent) series.
However, the iteration is numerically useful since when the right conditions
are met the residual convergences geometrically for the first few iterations with a
rate that gets better with larger $\om$. We show this formally in \cref{TR}.
%
For now, we show the temporary convergence of the residual by example: in \cref{convergence-plot}, we apply
the iteration from \cref{init,iter} to the equation 
\begin{equation}\label{bursteq}
    u'' + \frac{m^2 - 1}{(1+t^2)^2}u = 0,  
\end{equation}
over the timestep $t \in [0, 2]$ starting with initial conditions $u(0) = 1/m$ and
$u'(0) = i$. For a given $m$, we show the maximum residual over the length of
the step, $\max_{t \in [t_j, t_{j+1}]}R[x_k]$, as a function
of the iteration number $k$. We vary $m$ and observe that the maximum residual
indeed drops geometrically for the first few iterations, $R[x_k] \propto
\om_{\text{max}}^{-k}$, where $\om_{\text{max}} = \sqrt{1 + m^2}$ is the
maximum value of $\om$ over the timestep. At small $\om_{\text{max}}$, the
geometric progression stops and the series starts diverging after $5$
iterations and only about $4$ digits of accuracy can be achieved, clearly
showing the asymptotic nature of the expansion. At large $\om$, however, the
convergence is exteremely rapid and machine precision is reached within about
$5$ iterations. In \cref{pracres} we show that even though the number of iterations
required to reach minimal residual grows with $\om$ as $\bigO(\om)$, in
practice a larger $\om$ required \emph{fewer} iterations to reach a given
accuracy threshold $\varepsilon$, approximately $k_{\varepsilon} \approx
\bigO(1/\log\om)$.  

\begin{figure*}[tb]
    \flushleft
    \subfloat{\includegraphics{plots/residual-k-corr.pdf}}
    %\label{residual-reduction}
    \subfloat{\includegraphics{plots/convergence.pdf}}
    \caption{\label{convergence-plot} Error analysis and convergence of our
    method. The left panel shows the maximum residual of the Riccati equation
    over a single step of the numerical method as a function of the number of
    terms added to the asymptotic expansion, which is used to propagate the
    numerical solution in the highly oscillatory regions of the ODE. The
    differently coloured curves correspond to different peak frequencies over
    the same time interval. A geometric reduction in the residual for the first
    few iterations, forecast qualitatively in \cref{phasefun} and shown formally in
    \cref{TR}, is observed. Following this geometric reduction the
    asymptotic nature of the approximation is manifest as an increase in the
    residual, especially visible at low $\omega_{\text{max}}$. The right panel
    is a convergence plot showing the achieved relative error against the
    relative tolerance set to the solver. The coloured curves correspond to
    different upper ends of the integration region. These should be compared to
    the grey dashed line showing an achieved error matching the tolerance.
    In regions where the condition number $\kappa$ of the problem may limit the achievable
    accuracy (see \cref{conditionnodef}), we plot the maximum achievable
    accuracy as a coloured dashed line to compare the achieved error with. }
   % Need to say what ODEs were used to create these  
\end{figure*}



The numerical evaluation of the asymptotic solution $x(t)$, and the solution of
the original ODE $u(t)$ requires repeated differentiation (as part of each
correction term) and quadrature (since $u(t) = \exp \int^t x(\sigma)
\mathrm{d}\sigma$). 
Since our solver is a stepper, we are not attempting to find a global solution
$u(t)$ for the entire integration range, and only need to compute the
asymptotic solution over a timestep, $t \in [t_i, t_i+h]$ at a time. At each 
timestep, we choose to perform differentiation and quadrature by discretizing
time on a Chebyshev grid of $n + 1$ nodes.  
Over the standard interval $t \in [-1, 1]$, the $n+1$ Chebyshev nodes are given by 
\be\label{chebnodes}
\tilde{t}_n := \{\tilde{t}_j\}, \quad \tilde{t}_j = \cos\left( j\pi/n\right), \quad j = 0, 1, \ldots, n,
\ee
where the parameter $n$ can be set by the user with a default value of $n = 16$, and is fixed throughout integration. 
One may then define an $(n+1) \times (n+1)$ differentiation matrix $\tilde{D}_n$ which, given a vector of function
values $\tilde{v}_n$ defined on the Chebyshev nodes $\tilde{t}_j$, returns $\tilde{w}_n$, the function's
derivative evaluated at the nodes,
\be\label{diffmat}
\tilde{w}_n = \tilde{D}_n\tilde{v}_n, \quad \tilde{f}_n := f(\tilde{t}_n),
\ee
The timestep $t \in [t_i, t_i+h]$ can always be scaled up or down to the standard interval via
\be\label{scaledt}
t_n := \{ t_j \}, \quad t_j = t_i + \frac{h}{2} + \frac{h}{2}\tilde{t}_j, \quad j = 0, 1, \ldots, n. 
\ee
Under the scaling, the differentiation matrix transforms as
\begin{align}
    w_n &= D_nv_n, \quad f_n := f(t_n) \label{scaledDdef}\\
    D_n &= \frac{2}{h}\tilde{D}_n. \label{scaledD}
\end{align}
We then use $D_n$ to compute antiderivatives (\eg $v_n$ given $w_n$), by
solving the matrix equation \cref{scaledDdef}. 

% Initial conditions

We ensure continuity of the solution and its derivative across timestep
boundaries by treating $u(t_i)$ and $u'(t_i)$ as initial conditions in the step
from $t_i$ to  $t_i+h$. Noting that the asymptotic iteration \cref{iter} can be
initialised with $x_0(t) = +i\om(t)$ as is done in \cref{init}, or with $x_0(t)
= -i\om(t)$, the resulting asymptotic solutions, $x_{\pm}(t)$, approximate the
nonlinear Riccati equation, but the associated 
\be
u_{\pm} = e^{\int_{t_i}^{t_i+h}x_{\pm}(\sigma)\mathrm{d}\sigma}
\ee
are two, linearly independent approximate solutions of the linear ODE \cref{ode}. They may therefore be linearly combined to match any set of initial conditions,
\begin{align}
    u(t) &= A_{+}u_{+}(t) + A_{-}u_{-}(t), \qquad t \in [t_i, t_i + h],\\
    u'(t) &= A_{+}u'_{+}(t) + A_{-}u'_{-}(t), \qquad t \in [t_i, t_i + h],
\end{align}
with 
\be
\begin{bmatrix}
    1 & 1 \\
    x_{+}(t_i) & x_{-}(t_i)
\end{bmatrix}
\begin{bmatrix}
    A_{+} \\
    A_{-}
\end{bmatrix}
= 
\begin{bmatrix}
    u(t_i) \\
    u'(t_i)
\end{bmatrix}, 
\ee
where we used that $u_{\pm}(t_i) = 1$ and $u'_{\pm}(t_i) = x_{\pm}(t_i)$ by construction.


% +/- solutions complex conjugates

\subsection{Direct spectral solution on nonoscillatory intervals \label{chebysteps}}

In case the solution is nonoscillatory or the asymptotic expansion fails to
converge before the residual reaches a user-specified tolerance $\varepsilon$, we
employ a spectral method based on Chebyshev nodes instead. The motivation
behind this choice is that by adjusting the number of nodes, an arbitrarily
high order can be achieved. Although spectral methods are usually used for
solving boundary value problems whereby the nodes are laid out over the entire
integration range, there is no reason one could not apply them over one timestep at a
time in a time-stepper setting. Since the present algorithm was constructed to
switch between two methods -- one based on the asymptotic expansion of the
Riccati equation and a spectral method -- on the fly and the points of
switching are not predetermined, we opt to break the integration range up into
smaller timesteps, applying the spectral method over a single timestep at a
time.

Using the differentiation matrix from \cref{scaledD}, \cref{ode} may then be discretized over the interval $t \in [t_i, t_i+h]$ as
\begin{align}\label{discreteode}
    F_n u_n = \left(D_n^2 + 2\text{diag}(\g_n)D_n + \text{diag}(\om_n) \right)u_n = 0, 
\end{align}
where the subscript $n$ denotes a vector of function values at the $n+1$ (scaled) Chebyshev nodes $t_n$.
%\be\label{scaledt}
%t_n = \{ t_j \}, \quad t_j = t_i + \frac{h}{2} + \frac{h}{2}\tilde{t}_j, \quad j = 0, 1, \ldots, n. 
%\ee
A spectral method based on Chebyshev nodes finds $u_n$ by solving the system of
equations defined by \cref{discreteode}, subject to auxiliary conditions that
ensure the uniqueness of the solution. In our case these conditions encode
continuity across the lower timestep-boundary, \ie are the initial conditions
\be\label{discreteic}
(u_n)_n = u(t_i), \quad \left(D_nu_n\right)_0 = u'(t_i),
\ee
where a lower index outside brackets denotes a given vector element.
The initial conditions may be encoded as two rows appended to $F_n$,
\be\label{discreteodeic}
\renewcommand*{\arraystretch}{1.25}
\begin{bmatrix}
    (F_n)_{00} & \ldots & (F_n)_{0n} \\
    \vdots & \ddots & \vdots \\
    (F_n)_{n0} & \ldots & (F_n)_{nn} \\
    (D_n)_{00} & \ldots & (D_n)_{0n} \\
    1 & \ldots & 0
\end{bmatrix}
u_n =  
\begin{bmatrix}
0 \\
\vdots \\
0 \\
u'(t_i) \\
u(t_i)
\end{bmatrix},
\ee
where lower indices outside a bracket specify a given matrix element.
After solving the $(n+3) \times (n+1)$ system in \cref{discreteodeic}, the value of the solution
$u(t)$ at the end of the interval can be read off as the last element of the
solution vector $u_n$, and its derivative as the last element of $D_nu_n$.
These then serve as the initial conditions for the subsequent time-step.
Note that the system has now become rectangular but also overdetermined. The
matrix on the right-hand-side of \cref{discreteodeic} may be replaced with a
square one by removing the last two rows and incorporating the initial
conditions directly, \eg the last row can be removed by solving only for
$(u_n)_1, \ldots, (u_n)_n$ and setting $(u_n)_0 = u(t_i)$.

The error on steps taken with the spectral method above is estimated by
doubling $n$, attempting the step with the same stepsize
$h$ again, and comparing results. If the error falls below the user-specified
threshold $\varepsilon$, the spectral method is considered to have converged and the
step is accepted. If the error does not reach $\varepsilon$ by $n=n_{\text{max}}$, $h$
is halved and the iteration in $n$ begins again starting with
$n=n_{\text{min}}$. The parameters $n_{\text{min, max}}$ can be chosen by the
user, but their default values are set to $n_{\text{min}} = 16$,
$n_{\text{max}} = 64$. Computation time spent in failed steps is lost, making
it imperative that the initial stepsize estimate $h$ is chosen carefully. We
describe the procedure for choosing $h$ for both spectral and asymptotic steps
in detail in the next section.

% Error analysis? Just briefly about order of error...
% Argue why Chebyshev nodes are a good choice here, citing Tref

\subsection{Adaptive selection of interval size and type}

\subsubsection{Initial stepsize estimates}

At each timestep $t_i$, a decision needs to be made about whether to attempt a
step using the spectral method or the asymptotic exansion. The two types of
steps could be attempted simultaneously and the decision could be made based on
their estimated error (as done in \cite{agocs2020efficient}), but in order to minimise
computation time, we estimate whether the asymptotic expansion would
converge quickly enough first. Inspecting the first few iterates of the asymptotic
method from \cref{init,iter}, and noting that the convergence rate is
determined by the size of the correction term added in each iteration, a rough
upper bound on the first correction term is $\om'/\om$, giving the
approximate timescale 
\be\label{hoscini}
h_{\text{osc}} = \frac{\om(t_i)}{\om'(t_i)}.
\ee
\cref{TR} to follow justifies this initial stepsize-estimate by showing that
the ratio of bounds on $\om'$ and $\om$ appears in the rate of
convergence of successive Riccati residuals. 

A similar estimate for the spectral method should depend on two things: how
oscillatory the solution is, and what timescales the terms in \cref{ode} change
on, \ie their smoothness. If the solution is highly oscillatory, we wish to
avoid taking a step with a Chebyshev spectral method. A measure of the rate of
oscillations is simply the frequency, giving the timescale
\be\label{hsloini}
h_{\text{slo}} = \frac{1}{\om(t_i)}.
\ee

These initial stepsize estimates are too crude to attempt a step with in
practice, since over $h_{\text{osc}}$ or  $h_{\text{slo}}$, $\om$, $\om'$, and $\g$ may change significantly. 
Nonetheless they provide a useful starting point for the algorithm that computes
finer estimates.

\subsubsection{Refining the stepsize estimates}

The local error of of the asymptotic solution described in \cref{phasefun} is not
inherently dependent on the stepsize $h$ in the same way a \eg Runge--Kutta
method's is. The convergence rate of the residual of asymptotic steps depends
on the bounds on $\om$, $\om'$, and $\g$ over the course of the step as we show
in \cref{TR}, which indirectly introduces stepsize-dependence, as does the fact
that the derivatives and integral appearing in the step are computed
numerically on an $(n+1)$-point Chebyshev grid (with $n \approx 16$). Ultimately, it is how well $\om$ and $\g$ can be represented by $(n+1)$th
degree Chebyshev polynomials that determines the error in asymptotic steps, which
we therefore test directly to gain a more accurate estimate for the stepsize
starting from $h_{\text{osc}}$ as an initial estimate. Given $\om$ and $\g$ evaluated on $t_n$, 
which we denote $\om_n$ and $\g_n$, respectively, we define an error
\be
    \Delta_n[f] := \max_{j = 0, 1, \ldots, n} \left| \frac{f(t^{\star}_j) -
    f^{\star}(t^{\star}_j)}{f(t^{\star}_j)} \right|,
\ee
where $f$ is $\om$ or $\g$, 
\be
t^{\star}_n := \{t^{\star}_j\}, \quad t^{\star}_j = t_i + \frac{h}{2} + \frac{h}{2}\frac{j + \frac{1}{2}}{n\pi}, \quad j = 0, 1, \ldots, n,
\ee
and the $f^{\star}(t^{\star}_j)$ are interpolated using the $f(t_n)$ values,
\be
f^{\star}(t^{\star}_n) = L_n f(t_n).
\ee
The interpolation matrix $L_n$ is computed using a Vandermonde matrix \cite{atap}. The entries of $L_n$ are invariant under the simultaneous rescaling of
$t_n$ and $t^{\star}_n$ to lie within the standard interval $[-1, 1]$,
therefore $L_n$ need only to be computed once.
Given $\Delta_n[\om]$ and $\Delta_n[\g]$, we accept or update the stepsize as follows,
\begin{align}
    \Delta :=& \max \left(\Delta_n[\om], \; \Delta_n[\g]\right), \\
    h :=& \begin{cases}
        h &\text{if } \Delta \leq \varepsilon_h, \\
        \min \left( 0.7h,\; 0.9 h \left( \frac{\varepsilon_h}{\Delta} \right)^{\frac{1}{n+1}} \right) &\text{otherwise}.
    \end{cases}
\end{align}
In the above, the $1/(n+1)$ in the exponent is justified by noting that the
error in $n+1$-point Chebyshev interpolation is proportional to $h^(n+2)$, therefore if
$\Delta$ exceeds $\varepsilon_h$, using an exponent slightly larger than $1/(n+2)$ takes
the local error back down to a value smaller than $\varepsilon_h$, which is further
ensured by multiplying by the safety factor 0.9. We decrease the step to $0.7h$
if it yields a smaller stepsize to ensure quicker convergence if $\Delta$ is
only slightly larger than $\varepsilon_h$. The factors 0.7 and 0.9 were set based on
empirical observations. 

The local error of Chebyshev spectral steps depends on both the stepsize and
the number of nodes. Since within a step we iterate over both of these until
convergence, we only need to ensure that over our proposed stepsize the
timescale over which $u$ changes ($\approx 1/\om$) does not increase too much. Starting from $h =
h_{\text{slo}}$, we refine the stepsize proposal according to
\begin{align}
    h :=& \begin{cases}
        h/2 &\text{if } \min\limits_{j = 0, 1, \ldots, n}\frac{1}{\om(t^{\star}_j)} < \sigma h \\
        h &\text{otherwise},
    \end{cases}
\end{align}
with $\sigma = 0.8$, chosen experimentally, and $n$ being a hyperparameter with a default value of $n = 16$.

\subsubsection{Choosing the steptype}

With a refined $h_{\text{osc}}$ and $h_{\text{slo}}$ in hand, we can make an informed decision about which type of step to take.
\begin{pro}\label{steptypechoose}
    Let $h_{\text{osc}}$ and $h_{\text{slo}}$ be the refined stepsize proposals
    for a step to be taken from $t = t_i$ using the asymptotic expansion and the Chebyshev
    spectral method, respectively. The algorithm chooses to attempt an
    asymptotic step of size $h_{\text{osc}}$ if and only if
$$ h_{\text{osc}} > 5h_{\text{slo}} \quad \text{and} \quad \omega(t_i) h_{\text{osc}} > 2\pi, $$
    otherwise it will attempt a spectral step of size $h_{\text{slo}}$.
\end{pro}
The reason behind requiring the proposed stepsize for an asymptotic step not to
simply be larger than that of a Chebyshev spectral step is that in regions
where the two stepsizes compete, the asymptotic method rarely converges before
the residual reaches $\varepsilon$, either because $\om$ is not large enough, or
either of $\g$ and $\om'$ is too large. The prefactor of $5$ was set based on numerical experiments. 

If an asymptotic step was decided to be attempted, the algorithm will start
iterating over $x_j$ according to \cref{iter}, monitoring the residual
$R[x_j](t_n)$ throughout. If $\max R[x_j](t_n) < \varepsilon$ \emph{or} $\max
R[x_j](t_n) \geq \max R[x_{j-1}](t_n)$, the iteration is stopped. In the former
case, the proposed solution $u(t_i+h)$ is accepted and the independent variable
is incremented, $t := t_i + h$. Otherwise, the step is re-attempted with size
$h = h_{\text{slo}}$ and using the spectral method. 

If either of the conditions in \cref{steptypechoose} were not met, a spectral
step is attempted with $h = h_{\text{slo}}$. The local error of the step is
then estimated and the number of nodes, as well as the stepsize, is
adapted until a local error value of at most $\varepsilon$ is reached, as described in
\cref{chebysteps}.


% EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
\section{Error analysis \label{errorana}}

From now on we abbreviate the residual function by $R_j = R_j(t) := R[x_j](t)$.
\cref{PRiter} described the evolution of $R_j$
under the proposed iteration, which is local in $t$.
For smooth $x_j$ of size $\bigO(\om)$, and smooth $R_j$,
it suggests geometric reduction in $R_j$ by a factor $\bigO(\om)$ per iteration;
however, this is misleading in general because of
growth of high-order $t$-derivatives.
The series will turn out to be merely asymptotic in the large parameter $\om$.
We now make this argument rigorous, for $\om$ analytic
and sufficiently large throughout some neighborhood of $t$.

%One might hope that, given $x_j'$ and $x_j$ of order $\om$,
%and a lower bound on $x_j$ of the same order,
%then $R_j$ is reduced at each point by a factor $\bigO(\om)$.
%then the last term is negligible
%then if $R_j$ and $R_j'$ are sufficiently small


\begin{thm}\label{TR}
  Fix $t\in\R$, and let the frequency function $\om$ be analytic
  in the closed ball $B_\rho(t) := \{z\in\C : |z-t| \le \rho\}$,
  for some $\rho>0$, with bounds
    \begin{alignat}{4}
        \eta_1 &\leq |\om&&(z)| &&\leq \eta_2, \qquad &&\forall z\in B_\rho(t), \label{ommag} \\
        |\om'(z)| &\leq &&\eta_3 &&\leq \frac{\eta_1^2}{17}, &&\forall z\in B_\rho(t), \label{omder} \\
        |\g(z)| &\leq &&\eta_4 &&\leq \frac{\eta_1^2}{34\eta_2}, \qquad &&\forall z\in B_\rho(t), \label{gammaupper}
  \end{alignat}
  that is, its derivative should be sufficiently small.
  Let $k\in\{0,1,\dots\}$ be sufficiently small such that
  \be
    r := \frac{1}{2\te_1 \rho} \left(1 + \frac{\te_2}{\te_1}\right) k + \frac{\te_3}{4\te_1^2} \leq \frac{3}{4},
  \label{r}
  \ee
  where
  % *** note attempt at new consts:
  \begin{align}
    \te_3 &:= \eta_3 + 2\eta_2\eta_4, \label{eta3}
    \\
    \te_1 &:= \eta_1 - \eta_4 - \frac{17 \te_3}{4 \eta_1},  \label{eta1}
    \\ 
    \te_2 &:= \eta_2 + \eta_4 + \frac{17 \te_3}{4 \eta_1}. \label{eta2}
  \end{align}
  Then after any number $j\le k$ of iterations of \cref{init}--\cref{iter},
  the function $x_j$ has Riccati residual \cref{R} bounded at the point $t$ by
  \be
  |R_j(t)| \le \te_3 r^j.
  \label{Rjbnd}
  \ee
\end{thm}
This shows, for $\om$ of sufficiently small derivative
relative to its magnitude,
temporary geometric convergence up to $k$ iterations,
but at a rate $r$ that deteriorates with $k$.
And for any $k$ to exist satisfying the condition on $r$, $\om$ must have
a sufficiently large lower bound $\eta_1$, i.e., $t$
must be in a sufficiently oscillatory region for the original ODE.

\begin{rmk}\label{slight}
    By \cref{omder}, $\te_1 \ge 8\eta_1/17$ and $\te_2 \le \eta_2 + 9\eta_1/17$,
  thus $[\te_1,\te_2]$ is only a slight
  expansion of the interval $[\eta_1,\eta_2]$ containing the range of
    $|\om|$ in the ball.
  Applying this we see that the last term in \cref{r}
    is no more than $17/128 \leq 0.14$, thus can cause only slight deterioration in the rate.
\end{rmk}

\begin{rmk}
    In the limit when $\om$ tends to be constant in the ball ($\eta_3$ small) and the damping term is small ($\eta_4$ small),
  then $r$ tends to $k/\om \rho$, where we can interpret $\om\rho$ as $2\pi$
    time the number of oscillation periods across the ball radius.
    Then, for example, with a ball of radius of only 5 periods, $r$ satisfies \cref{r}
    for $k\le 25$.
\end{rmk}

\begin{proof}
  Define the concentric nested set of closed balls $B_j := B_{\rho_j}(t)$,
  with radii $\rho_j := (1-j/k)\rho$, $j=0,1,\dots,k$. Note that
  $B_0$ is the original ball in the statement of the theorem, while
  $B_k = \{t\}$ is the single point of interest.
  For any function $f$ analytic in $B_j$ we abbreviate the $\infty$-norm by
  $\|f\|_j := \max_{z \in B_j}|f(z)|$.
  We will need a bound for $f'$ in $B_{j+1}$ in terms of $\|f\|_j$ by
  applying Cauchy's theorem for derivatives
  \cite{steinshakarchi},
  \be
  f'(z) = \frac{1}{2\pi i} \int_{|\zeta-t|=\rho_j} \frac{f(\zeta)\, d\zeta}{(\zeta-z)^2}
  ~, \qquad z \in B_{j+1}~.
  \label{cauder}
  \ee
  Bounding the integrand, then using the cosine rule we get
  $$
  |f'(z)| \;\le \;
  \frac{\|f\|_j}{2\pi} \int_{|\zeta-t| = \rho_j} \frac{|d\zeta|}{|\zeta-z|^2}
  =
  \frac{\|f\|_j}{2\pi} \int_0^{2\pi} \frac{\rho_j\, d\theta}{|z|^2 + \rho_j^2 - 2|z|\rho_j \cos \theta}~.
  $$
  We now use $\int_{0}^{2\pi} d\theta /(a + b \cos \theta) = 2\pi/\sqrt{a^2-b^2}$
  for $b<a$ \cite[Eq.~3.613.1]{GR8}, with
  $a = |z|^2 + \rho_j^2$ and $b = -2|z|\rho_j$, so that
  $\sqrt{a^2-b^2} = \rho_j^2-|z|^2$.
  Noting that the case $|z| = \rho_{j+1}$ bounds the others,
  and using $\rho_j=(1-j/k)\rho$, we compute, for any $0\le j < k$,
  \be
  \|f'\|_{j+1} \le
  \frac{\|f\|_j}{2\pi} \frac{2\pi\rho_j}{\rho_j^2-\rho_{j+1}^2}
  =
  \frac{\|f\|_j}{2 \rho} \frac{k(k-j)}{2k-2j-1}
  =
  \frac{\|f\|_j}{2 \rho} k \left[ \frac{1}{2} + \frac{1}{2(2k-2j-1)}\right]
  \le
  \frac{k}{\rho}\|f\|_j~.
  %~, \; 0\le j< k
  \label{derbnd}
  \ee
  Note that this bound is a factor $\bigO(k)$ better than naively
  lower bounding the denominator in \cref{cauder}.
  
  %The 
  We now use induction in iteration number $j$.
%  and consider the iteration acting on the functions
    We take as the induction hypothesis that $x_\ell$ (and thus $R_\ell$)
  is analytic in $B_j$, for all $0\le \ell \le j$, with
  \begin{align}
      \te_1 \leq |x_\ell + \g| &\leq \te_2
  \qquad \mbox{ in } B_j, \quad \mbox{ for all } \ell = 0,1,\dots,j,
  \label{hypx} \\
      |R_\ell| &\leq \te_3 r^\ell \quad \mbox{ in } B_j, \quad \mbox{ for all } \ell = 0,1,\dots,j,
  \label{hypR}
  \end{align}
  where $r$ is defined by \cref{r}.
  Assuming for now the hypothesis for $j$, we apply simple bounds to the
  residual iteration \cref{Riter},
  lower-bounding denominator magnitudes and upper-bounding numerators
  via \cref{hypx},
  and applying \cref{derbnd} to the two derivative terms, to get
  $$
  \|R_{j+1}\|_{j+1}
  \leq
  \frac{1}{2\te_1}\left(
  \frac{1}{\te_1} \frac{k}{\rho}\te_2 + \frac{k}{\rho}
  \right)\|R_j\|_j
  +\biggl(\frac{\|R_j\|_j}{2\te_1}\biggr)^2
  \leq
  r \cdot \|R_j\|_j
  \leq
  \te_3 r^{j+1},
  $$
where in the middle step we used the crude bound $\|R_j\| \le \te_3$
following from \cref{hypR} and that $r\le1$, and then the definition of $r$ in \cref{r}.
%This handles the case $\ell=j+1$.
The lower cases $\ell\le j$ follow trivially from the hypothesis since the balls
are nested.
Thus \cref{hypR} is proven for $j+1$.

It remains to verify that \cref{hypx} also holds for $j+1$.
By the functional iteration \cref{init}--\cref{iter},
$$
x_{j+1}(z) + \g(z) = i\om(z) + \g(z) - \sum_{\ell=0}^{j} \frac{R_\ell(z)}{2 \left(x_\ell(z) + \g(z)\right)},
\qquad z\in B_{j+1}.
$$
Using the hypothesis, the sum is pointwise bounded in magnitude
in $B_{j+1}$ by
$$
\left|\sum_{\ell=0}^{j} \frac{R_\ell}{2\left( x_\ell + \g \right)} \right|
\leq \frac{\te_3}{2\te_1} \cdot \sum_{\ell=0}^j r^{\ell}
%\;\le\; \frac{\eta_3}{2\te_1} \frac{1}{1-r}
\leq \frac{\te_3}{2\te_1} \cdot 4,
%\leq \frac{95\eta_3}{198\eta_1}
$$

where the upper bound in \cref{r} was used to bound the geometric series.
Using \cref{eta1,eta3} for $\te_1$ and $\te_3$ we write this upper bound as
$$
\left|\sum_{\ell=0}^{j} \frac{R_\ell}{2\left( x_\ell + \g \right)} \right|
\leq
2\frac{\eta_3 + 2\eta_2\eta_4}{\eta_1 - \eta_4 - \frac{17\eta_3}{4\eta_1} - \frac{17\eta_2\eta_4}{2\eta_1}},
$$
then extend it by lower-bounding the denominator via \cref{omder} and the crude bound ${\eta_4 \leq \eta_1/34}$ from \cref{gammaupper}. This gives
$$
\left|\sum_{\ell=0}^{j} \frac{R_\ell}{2\left( x_\ell + \g \right)} \right| 
\leq
\frac{17\eta_3}{4\eta_1} + \frac{17\eta_2\eta_4}{2\eta_1},
$$
which, when combined with $|\om| \leq \eta_2$ and $|\g| \leq \eta_4$ yields
$$\|x_{j+1} + \g\|_{j+1} \le \eta_2 + \eta_4 + \frac{17\eta_3}{4\eta_1} + \frac{17\eta_2\eta_4}{2\eta_1} = \te_2,$$
verifying the upper bound \cref{hypx} for $j+1$.
Instead combining with $|\om| \ge \eta_1$ gives
$$ |x_{j+1}| \ge \eta_1 - \eta_4 - \frac{17\eta_3}{4\eta_1} - \frac{17\eta_2\eta_4}{2\eta_1} = \te_1 $$
in $B_{j+1}$, which verifies the lower bound \cref{hypx}.
Again, the hypothesis is inherited for $\ell\le j$ by the nesting of the balls.

Finally, the base case $j=0$ for induction
satisfies \cref{hypx}--\cref{hypR}
by the conditions of the theorem,
since $x_0(t) = i\om(t)$, noting $[\eta_1,\eta_2] \subset [\te_1,\te_2]$,
while $R_0(t) = i\left(\om'(t) + 2\g\om \right)$ is bounded in magnitude by $\te_3$.
\end{proof}

In the above theorem, the choice of $4/5$ in \cref{r} was merely a convenient one, chosen so that the bounds $[\te_1,\te_2]$ on $|x_j|$
involving the geometric sum were
not much wider than the bounds $[\eta_1,\eta_2]$ on $|\om|$.
%It is possible to write simpler but less tight formulae for $r$.


\subsection{Practical aspects of residual reduction \label{pracres}}

The fact that, as $k$ grows,
the rate $r$ in \cref{TR} deteriorates
we believe to be an inevitable consequence of
the series being asymptotic in $1/\om$ but not convergent.
%We support this in numerical results in ***.

However, given a function $\om(t)$ uniformly large enough
in a ball,
by stopping at a roughly optimal $k$ (an idea
called ``superasymptotics'' \cite{berrysuper,boydsuper})
one can achieve
exponential convergence with respect to the size of the frequency $\om$,
as follows.

\begin{cor}[Superasymptotic approximation]\label{super}
  Suppose $\om$ and $\g$ satisfy the conditions \cref{ommag,omder,gammaupper}
  of \cref{TR}
  about a point $t\in\R$,
  and let $\alpha := (1+\te_2/\te_1)/2\te_1\rho$ be the
  first term in the rate \cref{r} arising from these bounds.
  Then there is a $k\in\{0,1,\dots\}$ such that
  $$
  |R_k(t)| \; \le \; e \te_3 e^{-1/5\alpha}~.
  $$
\end{cor}
\begin{proof}
  Set $k$ to be the integer in the interval $[1/5\alpha-1, \, 1/5\alpha)$.
    Then since the final term in \cref{r} is bounded by $17/128 \leq 0.14$
    by \cref{slight}, and the
    first term $k\alpha \le 1/5$, we get $r\le e^{-1}$, which
    is also $\le 3/4$ so that \cref{Rjbnd} holds.
    Choosing $j=k$, and inserting a lower bound on $k$,
    $|R_k(t)| \le \te_3 r^k \le \te_3 e^{-(1/5\alpha-1)}$, which
    finishes the proof.
  \end{proof}
Recalling that $\alpha = \bigO(1/\om)$, this
shows that an $\om$-dependent
number of iterations can give, in exact arithmetic, an
exponentially convergent pointwise residual,
$$
|R(t)| \;=\; \bigO(e^{-c\om})
$$
for some constant $c>0$.
In the limit where
$\om$ tends to constant in the ball of radius $\rho$ about $t$, then
$\alpha$ tends to $1/\om\rho$, so the above
constant is roughly $c\approx \rho/3$.
Equivalently, roughly one decimal digit is achieved per
$1.1$ periods of oscillation across the ball radius.
% log(10)*3/(2*pi) = 1.099

However, this optimal number of iterations grows as $k = \bigO(\om)$,
so for large $\om$ is impractical (and unnecessary).
In practice we use an adaptive stopping criterion for $k$ based
on user-defined tolerance.
By returning to \cref{TR} and dropping the small second term
in \cref{r} we get, in the limit of constant $\om$ in the ball,
$$
|R_k(t)| \; \approx \; \te_3 \left( \frac{k}{\rho\om}\right)^k~.
$$
This shows \textit{nearly} geometric convergence in $k$,
sufficient to reach machine accuracy efficiently with a few iterations
when $\om$ is large.
For large $\rho\om$ one may approximately invert this to predict the iteration number $k$ sufficient for $|R_k| \approx \varepsilon$, to get
$$
k_\varepsilon \approx \frac{\log \varepsilon^{-1}}{\log \rho\om}~.
$$
%*** compare to numerics.

Finally, note that a stopping criterion for $k$ based on the residual $R[x_k]$ of the Riccati equation is viable because the residual is directly related to that of the original ODE.
\begin{cor}[Relationship of residuals]\label{residualu}
    Let $R[x_k](t)$ be the residual of the Riccati equation as defined in
    \cref{R} after $k$ iterations of \cref{init,iter}. Then if
    $\tilde{R}[u_k](t)$ is the associated residual of the original ODE
    \cref{ode}, defined as
    \be\label{Rode}
    \tilde{R}[u_k](t) := u''_k + 2\gamma(t)u'_k + \omega^2(t),
    \ee
    and $u_k = e^{\int^t x_k(\sigma)\mathrm{d}\sigma}$, then
    \be
    \tilde{R}[u_k] = u_k R[x_k].
    \ee
\end{cor}
\begin{proof}
    The proof follows straightforwardly from substitution of $u_k(x_k)$ into \cref{Rode}.
\end{proof}


\section{Numerical results \label{numresults}}

Before discussing the results of numerical experiments, we define a few useful quantities. 

We measure the achieved accuracy of a method by comparing the numerical
solution to the analytic (or a reliable reference) solution.
\begin{defn}[Absolute and relative error]\label{deltau}
    If an ODE of the form \cref{ode} has analytic (or otherwise reference) solution $u(t)$ and 
    numerical solution $\tilde{u}(t)$, then its absolute error at a given time $t$ is
    \be
    \Delta u := \left| \tilde{u}(t) - u(t)\right|,
    \ee
    and its relative error is defined as
    \be
    \left| \Delta u/u \right| := \left| \frac{\tilde{u}(t) - u(t)}{u(t)}\right|.
    \ee
\end{defn}

The achieved accuracy of a method is limited by the condition number of the
problem $\kappa$. The condition number measures the sensitivity of a given system to
outside perturbations, which may manifest as uncertainty in any of the inputs.
The condition number has been defined for functions of a single or multiple
variables, for matrices or linear equations written in matrix form (see, \eg
\cite{rice1966,trefethenlinalg}), and several other problems. We approximate
the condition number of an oscillatory ODE as the cumulative number of periods of
oscillation the solution of the ODE has traversed.
\begin{defn}[Condition number of an oscillatory ODE]\label{conditionnodef}
    Since the condition number $\kappa$ of an oscillatory function $f(t) =
    c_0\exp(i\int^t \om(\sigma)\mathrm{d}\sigma)$, where $c_0$ is a constant,
    is
    \begin{equation}\label{conditionno}
        \kappa_f(t) = \left| \frac{tf'(t)}{f(t)} \right| = i\om(t)t,
    \end{equation}
    let the condition number of an ODE of the form \cref{ode} be defined as the total number of oscillations the solution $u$ has passed through over the solution interval, 
    \begin{equation}
        \kappa := \frac{1}{2\pi}\Im\left(\log\frac{u(t_1)}{|u(t_1)|}  - \log\frac{u(t_0)}{|u(t_0)|}\right).
    \end{equation}
    In practice, we will often approximate $\kappa$ by using the numerical
    solution $\tilde{u}$ instead of the analytic solution $u$, since the latter may not
    exist or be available.
\end{defn}


\subsection{A toy example: the Airy equation}

A simple but effective test ODE for highly oscillatory solvers is the Airy equation,
\be
u'' + tu = 0, \quad t \in [1, 10^8], 
\ee
with initial conditions
\be
u(1) = \text{Ai}(-1) + i\text{Bi}(-1), \quad u'(1) = - \text{Ai}'(-1) -i\text{Bi}'(-1).
\ee
It serves as a good test case because it is maximally hard for standard
numerical methods due to the frequency growing with time, but should prove to
be maximally easy for specialised oscillatory methods for the rate of change of
the frequency decreases with time, making $A\cos(\om t) + B\sin(\om t)$ -- the
zeroth order solution -- an increasingly good approximation. With this test
case one can check whether the method adapts its stepsize correctly and
achieves the maximum possible accuracy, given by the condition number $\kappa$ of
the problem from \cref{conditionnodef} times machine precision $\varepsilon_{\text{mach}}$.  
\cref{airy-results} shows the
internal steps our numerical solver takes while solving the Airy equation,
colour-coded by step type, as well as the progression of stepsizes, numerical
error, and number of periods of oscillations traversed in a single step. The
stepsize $h$ is expected to reflect the timescale over which $\om$ changes,
therefore $h \propto \om/\om' \propto t$, which is observed. The numerical
error traces the theoretical minimum, $\kappa \cdot \varepsilon_{\text{mach}}$, to
within a digit. The number of oscillations traversed per step can be derived
from $h(t)$: $n_{\text{osc}} \propto \om h \propto t^{3/2}$, which again is
clearly seen in \cref{airy-results}.


\begin{figure}[tb]
    \centering
    \includegraphics{plots/airy-numsol.pdf}
    \caption{\label{airy-results} Numerical solution of the Airy equation. The
    top left panel shows the real part of the analytic solution in black, on
    top of which the individual timesteps of the solver are plotted, coloured
    by their step type. The top right panel plots the stepsize as a function of
    time, which exhibits linear growth. The bottom left shows the relative
    error achieved by the solver in black, with two dashed lines to guide the
    eye: the tolerance requirement we set to the solver in grey, and the
    maximum achievable accuracy (calculated as the product of the condition
    number~$\kappa$, defined in \cref{conditionnodef}, and machine precision~$\varepsilon_{\text{mach}}$) in blue. On the
    bottom right, we show the number of wavelengths of oscillation traversed in
    a single step, as a function of time. 
%    This is expected to be roughly proportional to the frequency times timescale over which $\om$ changes, which is $\om^2/\om' = t^{3/2}$, plotted in dashed blue. 
    %, $u'' + tu = 0$, on $t \in [1, 10^8]$, with initial conditions $u(1) = \text{Ai}(-1) + i\text{Bi}(-1)$, $u'(1) = - \text{Ai}'(-1) -i\text{Bi}'(-1)$.   
    }
\end{figure}


\subsection{Comparison with standard and state-of-the-art oscillatory solversi \label{solvercomp}}

As reviewed in the introduction, a number of numerical methods have been
created specifically for highly oscillatory ODEs. We compare the performance of
the present solver with theirs in this section. For reference, a short
description of the available numerical solvers is found below, along with the
names by which they will be referred to hereafter. 
\AB{maybe we should give ``this work'' a name: Riccati defect correciton (RDC) or something}
%
\begin{description}
\item[The \AB{Kummer?}
  phase function method]{is an arbitrarily high-order solver
        described by Bremer \cite{bremer2018}, specifically created for highly
        oscillatory problems, operating near machine precision. Its main
        drawbacks are the lack of generalisability to include a friction term
        and to operate at low frequencies or non-oscillatory regions of the
        solution. Bremer provides a \texttt{Fortran 90} implementation.
        % note the "method" is not specific to F90 !
        }
\item[oscode]   % sorry gives tex errs for me, will work on
  %\texttt{\textbf{oscode}}
{is a low-order numerical solver
        \cite{agocs2020efficient,agocs2020dense} that can accommodate a
        friction term and nonoscillatory regions in the solution. It requires
        only requires the user to specify $\om$ and $\g$, which can be given as
        either closed-form functions or as timeseries. The solver has both a
        \texttt{Python} and \texttt{C++} interface, but for the experiments below it is called in
        \texttt{Python}.}
    \item[The WKB marching method]{\cite{arnold2011wkb,korner2022wkb} uses the
        mechanism described in \cite{agocs2020efficient} to switch between an a
        WKB-inspired method in regions of rapid oscillation and Runge--Kutta
        method. Its convergence order in the oscillatory regions is one higher
        than that of \texttt{oscode}, but does not (easily) generalise to
        include a $\g$ term. It further requires the user to specify the
        derivatives of $\om$ by hand, up to $\om^{(5)}$. It is implemented in \texttt{MATLAB}.
        \AB{Emphasize it's recent?}
    }
\end{description}
%
Since solvers not created specifically for oscillatory problems typically have
a runtime proportional to $\om$\footnote{This would mean unacceptably long
runtimes for the problems presented in this section.} and the phase function
method is the only existing 
arbitrarily high-order specialised method with an open-source implementation
available\footnote{From
\url{https://github.com/JamesCBremerJr/Phase-functions}.}, we shall use its
output as the reference solution for the highly oscillatory problem
investigated in this section. For the sake of comparison, we take the example
ODE defined in Eq. (237) of \cite{bremer2018},
\be\label{bremer237eq}
u'' + \lambda^2 q(t) = 0, \quad q(t) = 1 - t^2\cos(3t)
\ee
on $t \in [-1, 1]$, subject to the initial conditions
\be\label{bremer237ic}
u(-1) = 0, \quad u'(-1) = \lambda,
\ee
where we vary $\lambda$ between $10^1$ and $10^7$.

Performance statistics of the phase function method on this problem is presented in
Table 1 of \cite{bremer2018}. The maximum error quoted therein was computed
with respect to a reference solution obtained by a spectral deferred correction
(SDC) method \cite{dutt2000}. As noted in \cite{bremer2018}, the phase function
computed by the method is accurate to machine precision, and the only source of
error at large $\lambda$ stems from roundoff error during the evaluation of
trigonometric functions with large arguments. The growth of the condition number $\kappa$ with the frequency of oscillations $\lambda$ for a fixed solution interval causes the phase function method to
be accurate to $13$ digits at $\lambda = 10^1$, but only $8$  at $\lambda =
10^7$. This is important to keep in mind as our reference solution is generated
by phase function method, meaning that the numerical errors presented in Table
1 of \cite{bremer2018} serve as \emph{upper bounds} for any lower numerical
errors reported here. In other words, we can only tell if the present method is
as accurate as the phase function method, not more. We consider this acceptable
given that the numerical accuracy is limited by the high condition number of
the problem at hand, and that the present method is likewise limited by
the accuracy of evaluating trigonometric functions with large arguments.

\cref{bremer237tab} presents the runtime statistics of the present method,
\texttt{oscode}, and the WKB marching method, respectively, separated by double
lines. For \texttt{oscode}, the table contains two separate entries that have
been generated with the relative tolerance set to $10^{-6}$ and $10^{-12}$,
respectively. This is reflected in the achieved error in the low-$\lambda$
limit. For all other solvers, we set the relative tolerance to $10^{-12}$, and
in all cases we use double precision. For our solver, we set the number of
Chebyshev nodes in Riccati (oscillatory) steps (see \cref{chebysteps}) to $n = 40$, the
tolerance for stepsize selection to $\varepsilon_h = 10^{-13}$, and the number
of Chebyshev nodes in the spectral (nonoscillatory) steps to the default
$n_{\text{min}} = 16$, $n_{\text{max}} = 64$. For \texttt{oscode} and the WKB
marching method none of the parameters were modified from their default values
with the exception of the local tolerance.
% Slightly imprecise as the number of nodes is (n+1)

The quantities tabulated are described in detail below.
\begin{description}
    \item[$\bm{\mathrm{max}|\Delta u/u|}$:]{Maximum relative error over the
        integration range, evaluated at the timesteps taken internally by the
        solver.}
    \item[$\bm{t_{\mathrm{solve}}}$:]{Total runtime, in seconds, of a single
        ODE solve averaged over $10^3$--$10^5$ runs. This is equivalent to the
        ``phase function construction time'' reported in Table 1 of
        \cite{bremer2018}. Note that this alone is not a good basis for
        comparison for the methods presented, since even though the ODE solves
        were timed on the same machine, the methods have been implemented in
        different programming languages.}
    \item[$\bm{t_{\mathrm{eval}}}$:]{Time it takes for the program to evaluate
        the solution at an arbitrary point other than the internal timesteps.
        This time should be compared to the ``average phase function evaluation
        time'' of Table 1 \cite{bremer2018}, but as all runtimes, should not be
        used for the sake of comparison on its own.}
    \item[$\bm{n_{\mathrm{s,osc}}}$:]{Number of steps taken by the solver with
        a specialised method in the oscillatory regime. This quantity is only
        applicable for the present solver, \texttt{oscode}, and the WKB
        marching method, and means a different  method in each case. For the
        present solver and \texttt{oscode}, it denotes steps taken with a
        ``direct'' asymptotic method (that described in \cref{phasefun} and WKB,
        respectively), and the WKB-inspired stepping scheme of
        \cite{korner2022wkb} in the case of the WKB marching method. When this
        quantity appears as $(n_1, n_2)$, $n_1$ denotes the number of attempted
        steps of the given type, out of which $n_2$ have been successful
        (accepted). The same convention applies to $n_{\text{s,slo}}$ and
        $n_{\text{s,tot}}$.}
    \item[$\bm{n_{\mathrm{s,slo}}}$:]{Number of steps taken by the solver with
        a ``standard'' method, in the nonoscillatory regions of the solution.
        For the present solver, this covers the adaptive-order spectral method
        based on Chebyshev nodes, but means a fixed-order Runge--Kutta method
        in all other cases.}
    \item[$\bm{n_{\mathrm{s,tot}}}$:]{Total number of steps performed by the
        solver, sum of $n_{\mathrm{s,osc}}$ and $n_{\mathrm{s,slo}}$.}
    \item[$\bm{n_f}$:]{Total number of function evaluations during a single ODE solve.}
    \item[$\bm{n_{\mathrm{LS}}}$:]{Denotes the total number of linear solve
        operations on a system of equations (\ie $A\vec{x} = \vec{b}$)
        performed by the solver.}
    \item[$\bm{n_{\mathrm{LU}}}$:]{Counts the number of LU decompositions
        performed by the solver. }
    \item[$\bm{n_{\mathrm{sub}}}$:]{Number of backsubstitutions (solving a
        system of equations in lower- or upper-triangular form) performed by
        the solver.}
\end{description}
%
\begin{table*}[tb]
    \renewcommand{\arraystretch}{1.2}
    \resizebox{\textwidth}{!}{\input{tables/bremer237_oscmethods_masked}}
    \caption{\label{bremer237tab} Accuracy, runtime and evaluation statistics of the algorithms
    considered (the present method, the phase function method, \texttt{oscode},
    and the WKB marching method) when applied to \cref{bremer237eq}. The column
    headers of this table, together with the various settings of the solvers
    are described in the text.}
\end{table*}
%
\cref{bremer237tab} shows clearly the extremely quick convergence of the asymptotic method within our solver
at sufficiently large frequencies: at $\lambda \geq 10^2$, the number of
function evaluations and steps become constant, causing the runtime to be
constant as well. It achieves the same accuracy as the phase function method in
this regime, but in $\approx 1/10$ time. Towards $\lambda = 10^7$, only
$\approx 2$ terms are required for the asymptotic expansion to achieve an
estimated (local) relative error of $10^{-12}$. 
\texttt{oscode} and the WKB
marching method also achieve fast convergence at sufficiently large
frequencies, but due to the fact that neither are adaptive in the order of the
asymptotic expansion (\ie the number of terms in the expansion) this
happens later, at around $\lambda \geq 10^4$. Potentially due to the
stepsize-update algorithm used by \texttt{oscode}, its number of steps,
function evaluations, and therefore runtime, grows slowly rather than staying
constant as the frequency increases. It is also worth noting that at
sufficiently high frequencies, $\lambda \geq 10^4$, setting a tolerance of
$\varepsilon = 10^{-6}$ in \texttt{oscode} results in the same accuracy as
setting $\varepsilon = 10^{-12}$ but at fewer function evaluations and steps,
again due to the WKB expansion being highly accurate in this regime and to
\texttt{oscode} potentially overstimating its local error. Finally, it is worth
noting that the WKB marching method needs fewer function evaluations and has a
slightly shorter runtime (although achieves fewer digits of accuracy) at high
frequencies because it asks the user to provide high-order derivatives of the
frequency $\om(t)$, therefore it need not use further evaluations of $\om$ to
compute said derivatives numerically. Three methods (the present solver, the
phase function method, and \texttt{oscode}) offer a dense output option, \ie
can perform interpolation and compute the numerical solution in-between
internal steps, and do so by interpolating a slowly-varying phase function,
therefore they all have evaluation times of $t_{\text{eval}} =
\mathcal{O}(10^{-6})$ \si{\s}.  

\begin{figure}[tb]
    \centering
    \includegraphics{plots/bremer237-timing.pdf}
    \caption{\label{bremer237-timing} Performance comparison of our solver and
    other state-of-the-art algorithms on \cref{bremer237eq}. The left panel
    shows the total runtime of the solvers listed in \cref{solvercomp} as the
    frequency parameter $\lambda$ is varied, for two different relative
    tolerance settings: runtimes with $\varepsilon = 10^{-12}$ are plotted with
    solid, and $\varepsilon = 10^{-6}$ with dashed lines. The right panel shows
    the corresponding relative error the solvers achieved at the end of the
    integration interval. The grey shaded region in this plot serves as an
    upper bound on any errors that fall within it, since the errors plotted
    here were computed using the phase function method as a reference, which is
    only accurate up to the upper edge of the grey region.}
\end{figure}




\subsection{Evaluating special functions}

Being able to solve highly oscillatory ODEs accurately means the solver can
quickly evaluate high-order special functions that obey an ODE of the required
form, such as Legendre's differential equation,
\be\label{legendreode}
(1-t^2)u'' - 2tu' + n(n+1)u = 0, \quad t \in [0.1, 0.9]. 
\ee
The problem need not be phrased as an initial value problem: due to the
linearity of the ODE, two arbitrary linearly independent solutions can be
linearly combined to satisfy initial or boundary conditions. In this example,
the results of which are shown in \cref{legendre-results}, we set tolerances of
$\varepsilon = 10^{-12}$, $\varepsilon_h = 10^{-13}$ and the number of
Chebyshev nodes in oscillatory steps to $n = 16$. 
% Imprecise again, number of nodes is (n+1)
These results again demonstrate the frequency-independence of the runtime of
our algorithm and show that the required tolerance (where the condition number
of the problem allows) is achieved. Note that after the solution of the ODE has
taken place, evaluating $u$ at intermediate points (\ie not at the internal
timesteps taken by the solver) is possible via interpolation of the phase
function $x$ in $\bigO(1)$ time, independently of the order $n$ in \cref{legendreode}.


\begin{table*}
    \renewcommand{\arraystretch}{1.2}
    \resizebox{\textwidth}{!}{\input{tables/legendre-fastest-morestuff}}
    \caption{Accuracy, runtime and evaluation statistics of the present algorithm when
    applied to Legendre's differential equation, \cref{legendreode}. The column
    headers are identical to those in \cref{bremer237tab} and are explained in
    the text. \label{legendre-results}}
\end{table*}





%\subsection{An example from cosmology \label{cosmo-example-sec}}
%
%%One might be tempted to think that considering the damping-free form ($\g(t) =
%%0$) of \cref{ode} would have been sufficient, and that allowing for the case of
%%$\g(t) \neq 0$ does not add generality, since the $\g$-term in \cref{ode} may
%%always be removed via a variable transform. Indeed, it can be shown that either
%%the independent or the dependent variable can be rescaled to yield a
%%damping-free ODE. The most general transformation that preserves the linearity
%%of the ODE is
%%\be\label{transformation}
%%t \to \tau := f(t), \quad u \to y:= g(\tau)u,
%%\ee
%%which yields the second-order linear ODE
%%\be\label{transformed-ode}
%%\ddot{y} + \left[ \frac{2\dot{g}}{g} + \frac{f''}{(f')^2} + \frac{2\g}{f'} \right]\dot{y} + 
%%\left[\ddot{g} + \left( \frac{f''}{(f')^2} + \frac{2\g}{f'} \right)\frac{\dot{g}}{g} + \frac{\om^2}{(f')^2} \right]y = 0,
%%\ee
%%where the overdot denotes differentiation with respect to $\tau$. Setting the coefficient of $\dot{y}$ to zero yields the constraint
%%\be
%%2\tilde{\g} = \frac{2\dot{g}}{g} + \frac{f''}{(f')^2} + \frac{2\g}{f'} = 0,
%%\ee
%%and simplifies the frequency of the transformed ODE to
%%\be
%%\tilde{\omega}^2 = \ddot{g} - 2\left( \frac{\dot{g}}{g}\right)^2 + \left( \frac{\om}{f'}\right)^2.
%%\ee
%%This, however, still carries a functional degree of freedom, in that one is free to choose $g$ (or $f$). Despite the fact that the ODE may always be transformed this way, in some cases it is not numerically stable to do so. We demonstrate this through an example taken from cosmology.
%
%A widely accepted theory explaining the origin of large-scale structure in the Universe is that of inflation \cite{baumann2022}. Preceding and during an early phase of exponential expansion, quantum-scale fluctuations in matter density (and hence spacetime-curvature) evolved according to the Mukhanov--Sasaki equation,
%\be\label{mseq}
%\frac{\d^2 \mathcal{R}_k}{\d N^2} + 2\g(N)\frac{\d \mathcal{R}_k }{\d N} + \om^2(N) \mathcal{R}_k = 0, 
%\ee
%where the independent variable is the number of e-folds of inflation, $N = \ln
%a$, with $a$ being the scale factor, a time-dependent characteristic
%lengthscale associated with the expansion. During inflation, $a$ grows
%exponentially, making $N$ a natural independent variable for the purposes of
%computation. $\mathcal{R}$ is a time-dependent scalar field that quantifies
%perturbations in the spatial curvature of spacetime called the gauge-invariant
%curvature perturbation. $\mathcal{R}_k$ is then the Fourier component of
%$\mathcal{R}$ associated with the wavenumber $k$. The damping term and
%frequency in \cref{mseq} are time- ($N$-) and wavenumber ($k$-) dependent, as
%well as functions of the underlying cosmological model. An example
%for their behaviour is shown in the top left panel of \cref{cosmology-example}.
%The bottom left panel then shows the evolution of a single perturbation mode
%$\mathcal{R}_k$ corresponding to a lengthscale of\footnote{When the perturbation mode is governed by
%the Mukhanov--Sasaki equation, the Universe is in its primordial phase of
%inflation, approximately $10^{-32}$ \si{\s} after the Big Bang. These primordial
%perturbations later translate into anisotropies in the temperature and
%polarisation of the Cosmic Microwave Background (CMB), which we can observe
%today. The CMB anisotropies are observed over the celestial sphere and are thus broken
%down into spherical harmonics parametrised by their multipole moment
%$\ell$. Even though a single plane wave projected onto a sphere generates
%anisotropies on multiple scales, the dominant anisotropy it generates will have
%$\ell \approx kD_{\ast}$ \cite{hu1995}, where $D_{\ast}$ is the angular
%diameter distance to the surface of last scattering, measured in
%\si{\mega\parsec}. The scale we consider in the text corresponds to the
%resolution of the current CMB experiments, \ie the largest currently observable
%multipole, $\ell \approx 2000$, or a wavenumber of \SI{0.2}{\per\mega\parsec}.
%} \SI{0.2}{\per\mega\parsec}. The perturbation mode exhibits rapid oscillations
%before it ``freezes out'' at a constant amplitude at which it will remain until
%after the end of primordial inflation. In cosmology, it is this frozen-out
%amplitude we are interested in computing, so the perturbation modes are evolved numerically
%until well into the constant-amplitude regime. Towards the end of the
%integration regime $\om \approx 0$ and $\g \approx \text{const.}$, giving the
%solution
%\be\label{infl-sol-1}
%\mathcal{R}_k \approx C_0 - \frac{C_1}{\g}e^{-\g N},
%\ee
%which quickly converges to $\mathcal{R}_k \approx C_0$. Once this phase of the
%numerical solution is reached, the condition number of the problem does not increase
%significantly -- intuitively, it is the constant part of the solution that is
%of interest, which is easy to follow numerically. Transforming the
%dependent variable to yield the damping-free equation of motion 
%\be
%\frac{\d^2 \tilde{\mathcal{R}}_k}{\d N^2} + \tilde{\om}^2(N) \tilde{\mathcal{R}}_k = 0, 
%\ee
%with $\tilde{\om}^2 = \om^2 + \g' - \g^2$, results in the behaviour plotted on
%the left-hand-side of \cref{cosmology-example}. In the region where the
%perturbation mode's amplitude was previously constant we now have $\tilde{\om}
%< 0$ and $\tilde{\om} \approx \text{const.}$, giving 
%\be
%\tilde{\mathcal{R}}_k \approx C_2 e^{+\tilde{\om}N} + C_3 e^{-\tilde{\om}N}.
%\ee
%The physically meaningful solution is the exponentially
%growing one. It is possible to follow this solution closely, but in this
%formulation the condition number of the problem grows proportionally with
%$|\tilde{\om}N|$, regardless of whether the solution
%is oscillatory or exponential, costing valuable digits.
%
%The primordial power spectrum (PPS) of curvature perturbations is defined as the function
%\be
%\mathcal{P}_{\mathcal{R}}(k) = \frac{k^3}{2\pi^2} |\mathcal{R}_k|^2,
%\ee
%where $|\mathcal{R}_k|$ is taken as the frozen-out amplitude of the given perturbation mode. 
%In cosmological inference, forward modelling often starts from the PPS which is
%computed assuming some set of cosmological parameters. For each PPS
%computation, one needs to solve \cref{mseq} for $\mathcal{O}(10^3)$ values of
%$k$, and around $\mathcal{O}(10^7 - 10^9)$ likelihood evaluations (each of
%which involves one PPS calculation) are necessary for mapping the posterior
%probability distribution. This gives a total of {$\mathcal{O}(10^{10} -
%10^{12})$} numerical solves of the highly oscillatory \cref{mseq}, which is only
%feasible if an extremely efficient, specialised numerical method is used. Moreover, the above reasoning demonstrates that the numerical method needs to be able to deal with a non-zero $\g$-term.   
%
%\begin{figure}[tbp]
%    \centering
%    \includegraphics{plots/cosmology.pdf}
%    \caption{\label{cosmology-example} Plot illustrating the change in the
%    terms in the Mukhanov--Sasaki equation \cref{mseq} and the behaviour of its
%    solution upon transforming to $\gamma$-free form. The top panels show the
%    frequency and damping term (where applicable) in the ODE as a function of
%    the e-folds of inflation $N$, while the bottom panels shows the evolution
%    of the dependent variable, the gauge-invariant curvature perturbation
%    $\mathcal{R}_k$ of comoving wavenumber $k$ corresponding to an 
%    observable lengthscale of \SI{0.2}{\per\mega\parsec}. Note the
%    symmetric-logarithmic scales on all $y$-axes, which is linear close to
%    zero but logarithmic otherwise. }
%\end{figure}


\section{Conclusions \label{conclusions}}

% Summary of paper
We presented an efficient numerical algorithm for solving linear second order
ODEs of a single variable, posed as initial value problems (see \cref{ode}), whose solution
might be highly oscillatory for some regions of the solution interval.
The
algorithm's runtime is independent of the characteristic frequency of
oscillations, like that of a few contemporary solvers that exploit asymptotics to speed up computation in the highly oscillatory region.
Our solver is an improvement on these contemporary solvers
in three ways: it is capable of solving the ODE efficiently regardless of
whether its solution oscillates or not; it is arbitrarily high-order accurate;
and it solves a more general problem by not imposing the restriction $\gamma =
0$ in \cref{ode}. 
We described the two methods making up the algorithm which works by
time-stepping, the mechanism by which the stepsize is adapted based on local
error estimates, and the procedure that decides which of the two methods to
use. One of the methods is a spectral solver based on Chebyshev nodes, while
the other is a numerical functional iteration that generates an asymptotic
solution for the Riccati form of the ODE. This amounts to constructing a
non-oscillatory phase function (logarithmic derivative of the dependent
variable). The error in this asymptotic expansion was measured in terms of the
residual of the Riccati equation. We have shown that given a sufficiently
slowly varying and large $\om$ and suficiently small $\g$, this residual
converges geometrically for the first $k$ iterations, but at a rate that
deteriorates with $k$. We formalised and proved this statement in \cref{TR},
and illustrated this temporary convergence in a numerical experiment. Further
numerical experiments confirmed the convergence of the overall algorithm and
compared its performance with that of state-of-the-art and standard solvers.

% Future work
% Allow for imaginary $\om$
Within the scope of this work, we only allowed $\om^2 \geq 0$. When $\om$ is
imaginary, any standard numerical solver will pick up the exponentially growing
solution due to rounding error, which will quickly overpower the evanescent
solution. A similar asymptotic approach, however, is possible as in the
oscillatory case, in that one can construct two series solutions to the Riccati
equation which form a complete basis for \cref{ode} and can thus be linearly
combined and the desired (evanescent or growing) solution can be selected for
each timestep. Care needs to be taken in regions where $|\om|$ is not large
enough for the asymptotic expansion to be sufficiently accurate.

% BVPs
Although we focused on initial value problems, note that due to the linearity
of the ODE any set of auxiliary conditions can be satisfied as a
post-processing step by solving the ODE with two sets of linearly indipentend
initial conditions and linearly combining the resulting solutions.

% Error analysis
It is left to a future analysis to relate the residual of the Riccati equation
\cref{ricc} or the original ODE \cref{ode} to an estimate of the solution's
local error. This would make the error control more uniform across the
different types of steps our algorithm uses and is the quantity being
controlled in typical numerical solvers.  

% Global spectral method ~ Chebop
Spectral methods are usually applied over the entire solution interval (\eg in
\cite{driscoll2008}) rather than within a time-stepping framework. When applied
locally and over a stepsize that can be changed adaptively, a degeneracy
appears between the number of nodes $n$ and the stepsize $h$, in that $n$ can
be increased and $h$ can be decreased to lower the local error. This degeneracy
could potentially be broken by determining the points at which the algorithm is
supposed to switch between Riccati and Chebyshev steps \emph{a priori}, or at
least identifying large regions of the solution interval where the Chebyshev
spectral will need to be used, and solving the ODE in these regions with a
single application of the spectral method with large $n$. Using \eg a sparse
spectral element method \cite{fortunato2021}, the solution of these regions
could be made more efficient.

% Quadrature
The piecewise-continuous numerical solution $\tilde{u}(t)$ our algorithm generates is made up
of sections represented by polynomials or exponentials of polynomials with
complex coefficients. Both of these are analytic in the complex plane, and so
it may be possible to calculate $\int_a^b \tilde{u}(t)\mathrm{d}t$ quickly,
despite $\tilde{u}$ being highly oscillatory for some or all of $[a, b]$, by
finding an appropriate contour in the complex plane. The more oscillatory
$\tilde{u}$, the faster it decays along the imaginary axis, and so the smaller
the number of quadrature points required to compute the numerical integral to a
given accuracy. This application will be explored in a future paper.
% PDEs???

\section*{Acknowledgments}
We benefitted greatly from discussions with Jim Bremer, Charlie Epstein,
and Manas Rachh.
The Flatiron Institute is a division of the Simons Foundation.

% Some things left to do:
%
% Figure for complex plane setup for proof of thm 3
% Numerically stable residual iterations!
% Final description of stepsize algorithm 
% Flowchart?
% Cosmo stuff? (Paper is too long!)
% Bremer's basic iteration (worked it out on paper but paper is already too long)
% Some comment that although theorem is pointwise convergence, uniform convergence over some interval is trivial by enclosing it in a larger ball.}
% Relate the ball in the theorem to the interval size that can be well-approximated by $p$th order Cheby, using Bernstein ellipse of Tref
% From [residual of ODE and Riccati] to solution error bounds, maybe? This is similar to local discretization err in a more typical numerical solver. (check LeVeque, Tref, Hairer, etc).
% Numerical results to support deterioration of convergence rate r with total iteration number k in Thm (figure?)
% More references needed


% BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
\bibliographystyle{abbrv}
\bibliography{refs}
\end{document}

